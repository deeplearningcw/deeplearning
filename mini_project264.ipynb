{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f56a5f4-3107-444c-8ef5-da497b39bb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyliu/miniconda3/envs/pytorch/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import time\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec72654",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(np.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9244a66-7eab-419f-a9ee-85e480dcac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "                 transforms.RandomHorizontalFlip(),\n",
    "                 transforms.RandomCrop(32, 4),\n",
    "                 transforms.ToTensor(),\n",
    "                 normalize,\n",
    "                 ]), download=True),\n",
    "                 batch_size=256, shuffle=True,\n",
    "                 num_workers=4, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "               datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               normalize,\n",
    "               ])),\n",
    "               batch_size=256, shuffle=False,\n",
    "               num_workers=4, pin_memory=True)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b5126b-64fc-4851-9363-99652bf37466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv3 = nn.Conv2d(out_channel, self.expansion *\n",
    "                               out_channel, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channel != self.expansion*out_channel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion*out_channel,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969b3dcd-949a-4e7e-a815-1e7fef511b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 128\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        # self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        # self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(256*block.expansion*4, num_classes)\n",
    "        # self.linear = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        # out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9464dcb0-a98a-4a0d-bd14-e4326c0816ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project1_model():\n",
    "    return ResNet(Bottleneck, [2, 3, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f35d814-7e04-43fa-a6a0-1cc434c03789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 32, 32]           3,456\n",
      "       BatchNorm2d-2          [-1, 128, 32, 32]             256\n",
      "            Conv2d-3          [-1, 128, 32, 32]          16,384\n",
      "       BatchNorm2d-4          [-1, 128, 32, 32]             256\n",
      "            Conv2d-5          [-1, 128, 16, 16]         147,456\n",
      "       BatchNorm2d-6          [-1, 128, 16, 16]             256\n",
      "            Conv2d-7          [-1, 512, 16, 16]          65,536\n",
      "       BatchNorm2d-8          [-1, 512, 16, 16]           1,024\n",
      "            Conv2d-9          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-10          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-11          [-1, 512, 16, 16]               0\n",
      "           Conv2d-12          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-13          [-1, 128, 16, 16]             256\n",
      "           Conv2d-14          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-15          [-1, 128, 16, 16]             256\n",
      "           Conv2d-16          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-17          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-18          [-1, 512, 16, 16]               0\n",
      "           Conv2d-19          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-20          [-1, 128, 16, 16]             256\n",
      "           Conv2d-21          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-22          [-1, 128, 16, 16]             256\n",
      "           Conv2d-23          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-24          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-25          [-1, 512, 16, 16]               0\n",
      "           Conv2d-26          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-27          [-1, 256, 16, 16]             512\n",
      "           Conv2d-28            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-29            [-1, 256, 8, 8]             512\n",
      "           Conv2d-30           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-31           [-1, 1024, 8, 8]           2,048\n",
      "           Conv2d-32           [-1, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-33           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-34           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-35            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-36            [-1, 256, 8, 8]             512\n",
      "           Conv2d-37            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-38            [-1, 256, 8, 8]             512\n",
      "           Conv2d-39           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-40           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-41           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-42            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-43            [-1, 256, 8, 8]             512\n",
      "           Conv2d-44            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 8, 8]             512\n",
      "           Conv2d-46           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-47           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-48           [-1, 1024, 8, 8]               0\n",
      "           Linear-49                   [-1, 10]          40,970\n",
      "================================================================\n",
      "Total params: 4,649,098\n",
      "Trainable params: 4,649,098\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 25.25\n",
      "Params size (MB): 17.73\n",
      "Estimated Total Size (MB): 43.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = project1_model().to(device)\n",
    "summary(model, (3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d711ca1c-34bf-4249-9293-2bca3a2ec9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetParams:\n",
    "   \"\"\"\n",
    "    A class to pass the hyperparameters to the model\n",
    "   \"\"\"\n",
    "   def __init__(self, arch='Model 1' ,epochs=100, start_epoch=0, batch_size=256, lr=0.1, momentum=0.9, weight_decay=1e-4, print_freq=50,\n",
    "                save_dir='save_temporary_checkpoints', save_every=10):\n",
    "        self.save_every = save_every #Saves checkpoints at every specified number of epochs\n",
    "        self.save_dir = save_dir #The directory used to save the trained models\n",
    "        self.print_freq = print_freq #print frequency \n",
    "        self.weight_decay = weight_decay #Weight decay for SGD\n",
    "        self.momentum = momentum #Momentum for SGD\n",
    "        self.lr = lr #Learning Rate\n",
    "        self.batch_size = batch_size #Batch Size for each epoch \n",
    "        self.start_epoch = start_epoch #Starting Epoch\n",
    "        self.epochs = epochs #Total Epochs\n",
    "        self.arch = arch #ResNet model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2d70bc-b3c1-4687-bbf4-971a161b586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs():\n",
    "    global args, best_precision\n",
    "    #Check if the save_dir exists or not\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    #Loading the model \n",
    "    model = project1_model()\n",
    "    model.cuda()\n",
    "\n",
    "    #Defining the Loss Function\n",
    "    loss_func = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    #Defining the Optimizer\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "    #                             momentum=args.momentum,\n",
    "    #                             weight_decay=args.weight_decay)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), args.lr, betas=(0.9, 0.99))\n",
    "    \n",
    "    #Defining the Learning Rate Scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                        milestones=[100, 150], last_epoch=args.start_epoch - 1)\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        #Train for one epoch\n",
    "        print('Training model: {}'.format(args.arch))\n",
    "        print('Current Learning Rate {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
    "        train(train_loader, model, loss_func, optimizer, epoch)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        #Test for one epoch\n",
    "        precision = validate(val_loader, model, loss_func)\n",
    "\n",
    "        #Save the best precision and make a checkpoint\n",
    "        is_best = precision > best_precision\n",
    "        best_precision = max(precision, best_precision)\n",
    "        if epoch > 0 and epoch % args.save_every == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(args.save_dir, 'project1_model_checkpoint.th'))\n",
    "        if is_best:\n",
    "            torch.save(model.state_dict(), os.path.join(args.save_dir, 'project1_model.th'))\n",
    "    return best_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c6ca600-7ed6-491a-bb1c-c80fc1446897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeepAverages(object):\n",
    "    #Computes and stores the average along with the current value\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19f4ab67-10a8-4f19-a7b8-b7dddeca5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    #Computes the top 1 precision\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def validate(val_loader, model, loss_func):\n",
    "    #Run an Evaluation\n",
    "    batch_time = KeepAverages()\n",
    "    losses = KeepAverages()\n",
    "    top1 = KeepAverages()\n",
    "\n",
    "    #Switch to Evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            input_var = input.cuda()\n",
    "            target_var = target.cuda()\n",
    "\n",
    "            #Compute the output of the Model and calculate the Loss\n",
    "            output = model(input_var)\n",
    "            loss = loss_func(output, target_var)\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            #Measure the Loss and Update it \n",
    "            precision = accuracy(output.data, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(precision.item(), input.size(0))\n",
    "\n",
    "            #Measure the elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "\n",
    "    print('Test Accuracy\\t  Top Precision: {top1.avg:.3f} (Error: {error:.3f} )\\n'\n",
    "          .format(top1=top1,error=100-top1.avg))\n",
    "    val_losses.append(100-top1.avg)\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "648d79b4-acfe-4461-9592-6a8126bddf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, loss_func, optimizer, epoch):\n",
    "    #Run one training epoch\n",
    "\n",
    "    batch_time = KeepAverages()\n",
    "    data_time = KeepAverages()\n",
    "    losses = KeepAverages()\n",
    "    top1 = KeepAverages()\n",
    "\n",
    "    #Switch to Train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # Measure the data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        target = target.cuda()\n",
    "        input_var = input.cuda()\n",
    "        target_var = target\n",
    "\n",
    "        #Compute the output and the Loss\n",
    "        output = model(input_var)\n",
    "        loss = loss_func(output, target_var)\n",
    "\n",
    "        #Compute the Gradient and do an SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        \n",
    "        #Measure the accuracy and record the loss\n",
    "        precision = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(precision.item(), input.size(0))\n",
    "\n",
    "        #Measure the Elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: No: [{0}] Batches: [{1}/{2}]\\t'\n",
    "                  'Loss: {loss.val:.4f} (Average: {loss.avg:.4f})\\t'\n",
    "                  'Precision: {top1.val:.3f} (Average: {top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))\n",
    "    train_losses.append(100-top1.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ebf3777-92f6-4855-baa8-0df7af075742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "  #Function to show an image\n",
    "  %matplotlib inline\n",
    "  %config InlineBackend.figure_format = 'retina'\n",
    "  img = img / 2 + 0.5     # un - Normalize\n",
    "  npimg = img.numpy()\n",
    "  plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "858ed804-c53a-478d-bb30-7bb9a7d397dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [0] Batches: [0/196]\tLoss: 2.4132 (Average: 2.4132)\tPrecision: 5.859 (Average: 5.859)\n",
      "Epoch: No: [0] Batches: [50/196]\tLoss: 2.3068 (Average: 17.5647)\tPrecision: 8.984 (Average: 10.585)\n",
      "Epoch: No: [0] Batches: [100/196]\tLoss: 2.2000 (Average: 9.9843)\tPrecision: 20.312 (Average: 12.488)\n",
      "Epoch: No: [0] Batches: [150/196]\tLoss: 2.0955 (Average: 7.3808)\tPrecision: 21.484 (Average: 14.994)\n",
      "Test Accuracy\t  Top Precision: 23.330 (Error: 76.670 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [1] Batches: [0/196]\tLoss: 2.0013 (Average: 2.0013)\tPrecision: 22.656 (Average: 22.656)\n",
      "Epoch: No: [1] Batches: [50/196]\tLoss: 1.9208 (Average: 2.0065)\tPrecision: 24.219 (Average: 24.701)\n",
      "Epoch: No: [1] Batches: [100/196]\tLoss: 1.7963 (Average: 1.9777)\tPrecision: 32.031 (Average: 25.789)\n",
      "Epoch: No: [1] Batches: [150/196]\tLoss: 1.8900 (Average: 1.9476)\tPrecision: 28.516 (Average: 26.681)\n",
      "Test Accuracy\t  Top Precision: 29.640 (Error: 70.360 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [2] Batches: [0/196]\tLoss: 1.9135 (Average: 1.9135)\tPrecision: 25.000 (Average: 25.000)\n",
      "Epoch: No: [2] Batches: [50/196]\tLoss: 1.8384 (Average: 1.8298)\tPrecision: 28.516 (Average: 30.783)\n",
      "Epoch: No: [2] Batches: [100/196]\tLoss: 1.7581 (Average: 1.8030)\tPrecision: 33.594 (Average: 31.838)\n",
      "Epoch: No: [2] Batches: [150/196]\tLoss: 1.7386 (Average: 1.7679)\tPrecision: 38.281 (Average: 33.392)\n",
      "Test Accuracy\t  Top Precision: 35.000 (Error: 65.000 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [3] Batches: [0/196]\tLoss: 1.7330 (Average: 1.7330)\tPrecision: 33.594 (Average: 33.594)\n",
      "Epoch: No: [3] Batches: [50/196]\tLoss: 1.6469 (Average: 1.6826)\tPrecision: 41.797 (Average: 37.592)\n",
      "Epoch: No: [3] Batches: [100/196]\tLoss: 1.7153 (Average: 1.6673)\tPrecision: 34.766 (Average: 37.682)\n",
      "Epoch: No: [3] Batches: [150/196]\tLoss: 1.5828 (Average: 1.6566)\tPrecision: 35.156 (Average: 37.950)\n",
      "Test Accuracy\t  Top Precision: 41.440 (Error: 58.560 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [4] Batches: [0/196]\tLoss: 1.5932 (Average: 1.5932)\tPrecision: 34.375 (Average: 34.375)\n",
      "Epoch: No: [4] Batches: [50/196]\tLoss: 1.6484 (Average: 1.6023)\tPrecision: 43.750 (Average: 39.890)\n",
      "Epoch: No: [4] Batches: [100/196]\tLoss: 1.5750 (Average: 1.6047)\tPrecision: 40.234 (Average: 40.265)\n",
      "Epoch: No: [4] Batches: [150/196]\tLoss: 1.6024 (Average: 1.5963)\tPrecision: 43.359 (Average: 40.832)\n",
      "Test Accuracy\t  Top Precision: 42.500 (Error: 57.500 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [5] Batches: [0/196]\tLoss: 1.5502 (Average: 1.5502)\tPrecision: 39.453 (Average: 39.453)\n",
      "Epoch: No: [5] Batches: [50/196]\tLoss: 1.5872 (Average: 1.5800)\tPrecision: 41.797 (Average: 42.180)\n",
      "Epoch: No: [5] Batches: [100/196]\tLoss: 1.6630 (Average: 1.5559)\tPrecision: 40.234 (Average: 42.536)\n",
      "Epoch: No: [5] Batches: [150/196]\tLoss: 1.5963 (Average: 1.5385)\tPrecision: 42.188 (Average: 42.907)\n",
      "Test Accuracy\t  Top Precision: 40.060 (Error: 59.940 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [6] Batches: [0/196]\tLoss: 1.4453 (Average: 1.4453)\tPrecision: 47.266 (Average: 47.266)\n",
      "Epoch: No: [6] Batches: [50/196]\tLoss: 1.5901 (Average: 1.4926)\tPrecision: 46.094 (Average: 45.420)\n",
      "Epoch: No: [6] Batches: [100/196]\tLoss: 1.3698 (Average: 1.4847)\tPrecision: 51.172 (Average: 45.800)\n",
      "Epoch: No: [6] Batches: [150/196]\tLoss: 1.4630 (Average: 1.4715)\tPrecision: 45.312 (Average: 46.192)\n",
      "Test Accuracy\t  Top Precision: 46.860 (Error: 53.140 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [7] Batches: [0/196]\tLoss: 1.4739 (Average: 1.4739)\tPrecision: 50.391 (Average: 50.391)\n",
      "Epoch: No: [7] Batches: [50/196]\tLoss: 1.3891 (Average: 1.4163)\tPrecision: 50.391 (Average: 47.825)\n",
      "Epoch: No: [7] Batches: [100/196]\tLoss: 1.3030 (Average: 1.3880)\tPrecision: 49.219 (Average: 48.766)\n",
      "Epoch: No: [7] Batches: [150/196]\tLoss: 1.2395 (Average: 1.3758)\tPrecision: 58.203 (Average: 49.402)\n",
      "Test Accuracy\t  Top Precision: 49.530 (Error: 50.470 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [8] Batches: [0/196]\tLoss: 1.2413 (Average: 1.2413)\tPrecision: 56.250 (Average: 56.250)\n",
      "Epoch: No: [8] Batches: [50/196]\tLoss: 1.2902 (Average: 1.2893)\tPrecision: 51.562 (Average: 52.964)\n",
      "Epoch: No: [8] Batches: [100/196]\tLoss: 1.2286 (Average: 1.2726)\tPrecision: 55.078 (Average: 53.438)\n",
      "Epoch: No: [8] Batches: [150/196]\tLoss: 1.1707 (Average: 1.2637)\tPrecision: 56.641 (Average: 54.005)\n",
      "Test Accuracy\t  Top Precision: 54.150 (Error: 45.850 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [9] Batches: [0/196]\tLoss: 1.4276 (Average: 1.4276)\tPrecision: 47.656 (Average: 47.656)\n",
      "Epoch: No: [9] Batches: [50/196]\tLoss: 1.2923 (Average: 1.2208)\tPrecision: 53.906 (Average: 55.944)\n",
      "Epoch: No: [9] Batches: [100/196]\tLoss: 1.1901 (Average: 1.1981)\tPrecision: 62.891 (Average: 56.745)\n",
      "Epoch: No: [9] Batches: [150/196]\tLoss: 1.2053 (Average: 1.1862)\tPrecision: 57.422 (Average: 57.150)\n",
      "Test Accuracy\t  Top Precision: 59.440 (Error: 40.560 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [10] Batches: [0/196]\tLoss: 1.0610 (Average: 1.0610)\tPrecision: 58.594 (Average: 58.594)\n",
      "Epoch: No: [10] Batches: [50/196]\tLoss: 1.1361 (Average: 1.1464)\tPrecision: 56.250 (Average: 58.647)\n",
      "Epoch: No: [10] Batches: [100/196]\tLoss: 1.0919 (Average: 1.1213)\tPrecision: 62.500 (Average: 59.565)\n",
      "Epoch: No: [10] Batches: [150/196]\tLoss: 1.2018 (Average: 1.1179)\tPrecision: 57.812 (Average: 59.766)\n",
      "Test Accuracy\t  Top Precision: 60.970 (Error: 39.030 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [11] Batches: [0/196]\tLoss: 1.0730 (Average: 1.0730)\tPrecision: 58.203 (Average: 58.203)\n",
      "Epoch: No: [11] Batches: [50/196]\tLoss: 0.9702 (Average: 1.0743)\tPrecision: 62.500 (Average: 61.397)\n",
      "Epoch: No: [11] Batches: [100/196]\tLoss: 1.2031 (Average: 1.0773)\tPrecision: 55.078 (Average: 61.189)\n",
      "Epoch: No: [11] Batches: [150/196]\tLoss: 0.9708 (Average: 1.0743)\tPrecision: 64.453 (Average: 61.509)\n",
      "Test Accuracy\t  Top Precision: 58.500 (Error: 41.500 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [12] Batches: [0/196]\tLoss: 1.2026 (Average: 1.2026)\tPrecision: 55.469 (Average: 55.469)\n",
      "Epoch: No: [12] Batches: [50/196]\tLoss: 1.0531 (Average: 1.0317)\tPrecision: 62.891 (Average: 62.745)\n",
      "Epoch: No: [12] Batches: [100/196]\tLoss: 1.2135 (Average: 1.0260)\tPrecision: 57.031 (Average: 63.293)\n",
      "Epoch: No: [12] Batches: [150/196]\tLoss: 0.9252 (Average: 1.0242)\tPrecision: 67.188 (Average: 63.405)\n",
      "Test Accuracy\t  Top Precision: 59.070 (Error: 40.930 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [13] Batches: [0/196]\tLoss: 0.9214 (Average: 0.9214)\tPrecision: 62.500 (Average: 62.500)\n",
      "Epoch: No: [13] Batches: [50/196]\tLoss: 0.9505 (Average: 1.0051)\tPrecision: 64.062 (Average: 63.986)\n",
      "Epoch: No: [13] Batches: [100/196]\tLoss: 0.9871 (Average: 0.9980)\tPrecision: 63.281 (Average: 64.453)\n",
      "Epoch: No: [13] Batches: [150/196]\tLoss: 0.9091 (Average: 0.9917)\tPrecision: 70.703 (Average: 64.678)\n",
      "Test Accuracy\t  Top Precision: 66.220 (Error: 33.780 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [14] Batches: [0/196]\tLoss: 0.9001 (Average: 0.9001)\tPrecision: 69.141 (Average: 69.141)\n",
      "Epoch: No: [14] Batches: [50/196]\tLoss: 1.1437 (Average: 0.9592)\tPrecision: 60.938 (Average: 65.801)\n",
      "Epoch: No: [14] Batches: [100/196]\tLoss: 0.8777 (Average: 0.9386)\tPrecision: 70.703 (Average: 66.573)\n",
      "Epoch: No: [14] Batches: [150/196]\tLoss: 0.9008 (Average: 0.9338)\tPrecision: 67.969 (Average: 66.825)\n",
      "Test Accuracy\t  Top Precision: 66.590 (Error: 33.410 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [15] Batches: [0/196]\tLoss: 0.8889 (Average: 0.8889)\tPrecision: 68.750 (Average: 68.750)\n",
      "Epoch: No: [15] Batches: [50/196]\tLoss: 0.8283 (Average: 0.9002)\tPrecision: 71.094 (Average: 67.195)\n",
      "Epoch: No: [15] Batches: [100/196]\tLoss: 0.9271 (Average: 0.9032)\tPrecision: 65.234 (Average: 67.466)\n",
      "Epoch: No: [15] Batches: [150/196]\tLoss: 0.8096 (Average: 0.9024)\tPrecision: 67.969 (Average: 67.728)\n",
      "Test Accuracy\t  Top Precision: 68.550 (Error: 31.450 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [16] Batches: [0/196]\tLoss: 0.8204 (Average: 0.8204)\tPrecision: 73.438 (Average: 73.438)\n",
      "Epoch: No: [16] Batches: [50/196]\tLoss: 0.8904 (Average: 0.8818)\tPrecision: 72.266 (Average: 68.888)\n",
      "Epoch: No: [16] Batches: [100/196]\tLoss: 0.9848 (Average: 0.8839)\tPrecision: 66.406 (Average: 68.742)\n",
      "Epoch: No: [16] Batches: [150/196]\tLoss: 0.9630 (Average: 0.8810)\tPrecision: 68.750 (Average: 68.768)\n",
      "Test Accuracy\t  Top Precision: 68.390 (Error: 31.610 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [17] Batches: [0/196]\tLoss: 0.8348 (Average: 0.8348)\tPrecision: 69.141 (Average: 69.141)\n",
      "Epoch: No: [17] Batches: [50/196]\tLoss: 0.8938 (Average: 0.8686)\tPrecision: 69.531 (Average: 69.217)\n",
      "Epoch: No: [17] Batches: [100/196]\tLoss: 0.8966 (Average: 0.8564)\tPrecision: 69.531 (Average: 69.779)\n",
      "Epoch: No: [17] Batches: [150/196]\tLoss: 0.8076 (Average: 0.8639)\tPrecision: 69.531 (Average: 69.467)\n",
      "Test Accuracy\t  Top Precision: 64.180 (Error: 35.820 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [18] Batches: [0/196]\tLoss: 1.0259 (Average: 1.0259)\tPrecision: 66.016 (Average: 66.016)\n",
      "Epoch: No: [18] Batches: [50/196]\tLoss: 0.8901 (Average: 0.8170)\tPrecision: 69.922 (Average: 71.032)\n",
      "Epoch: No: [18] Batches: [100/196]\tLoss: 0.8676 (Average: 0.8238)\tPrecision: 69.531 (Average: 70.819)\n",
      "Epoch: No: [18] Batches: [150/196]\tLoss: 0.8419 (Average: 0.8263)\tPrecision: 72.656 (Average: 70.825)\n",
      "Test Accuracy\t  Top Precision: 64.700 (Error: 35.300 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [19] Batches: [0/196]\tLoss: 0.9525 (Average: 0.9525)\tPrecision: 68.359 (Average: 68.359)\n",
      "Epoch: No: [19] Batches: [50/196]\tLoss: 0.8856 (Average: 0.8275)\tPrecision: 66.406 (Average: 71.239)\n",
      "Epoch: No: [19] Batches: [100/196]\tLoss: 0.8269 (Average: 0.8195)\tPrecision: 72.656 (Average: 71.461)\n",
      "Epoch: No: [19] Batches: [150/196]\tLoss: 0.8683 (Average: 0.8167)\tPrecision: 73.047 (Average: 71.521)\n",
      "Test Accuracy\t  Top Precision: 63.560 (Error: 36.440 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [20] Batches: [0/196]\tLoss: 0.7760 (Average: 0.7760)\tPrecision: 72.266 (Average: 72.266)\n",
      "Epoch: No: [20] Batches: [50/196]\tLoss: 0.8265 (Average: 0.8185)\tPrecision: 71.484 (Average: 71.078)\n",
      "Epoch: No: [20] Batches: [100/196]\tLoss: 0.8060 (Average: 0.8045)\tPrecision: 71.094 (Average: 71.620)\n",
      "Epoch: No: [20] Batches: [150/196]\tLoss: 0.6862 (Average: 0.8053)\tPrecision: 75.000 (Average: 71.627)\n",
      "Test Accuracy\t  Top Precision: 71.090 (Error: 28.910 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [21] Batches: [0/196]\tLoss: 0.7910 (Average: 0.7910)\tPrecision: 70.312 (Average: 70.312)\n",
      "Epoch: No: [21] Batches: [50/196]\tLoss: 0.7307 (Average: 0.7586)\tPrecision: 74.219 (Average: 73.675)\n",
      "Epoch: No: [21] Batches: [100/196]\tLoss: 0.7709 (Average: 0.7635)\tPrecision: 71.484 (Average: 73.306)\n",
      "Epoch: No: [21] Batches: [150/196]\tLoss: 0.6864 (Average: 0.7744)\tPrecision: 74.219 (Average: 72.941)\n",
      "Test Accuracy\t  Top Precision: 66.060 (Error: 33.940 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [22] Batches: [0/196]\tLoss: 0.8186 (Average: 0.8186)\tPrecision: 71.875 (Average: 71.875)\n",
      "Epoch: No: [22] Batches: [50/196]\tLoss: 0.7246 (Average: 0.7593)\tPrecision: 72.656 (Average: 73.254)\n",
      "Epoch: No: [22] Batches: [100/196]\tLoss: 0.6597 (Average: 0.7580)\tPrecision: 74.219 (Average: 73.229)\n",
      "Epoch: No: [22] Batches: [150/196]\tLoss: 0.7973 (Average: 0.7676)\tPrecision: 72.266 (Average: 72.905)\n",
      "Test Accuracy\t  Top Precision: 67.010 (Error: 32.990 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [23] Batches: [0/196]\tLoss: 0.6948 (Average: 0.6948)\tPrecision: 71.484 (Average: 71.484)\n",
      "Epoch: No: [23] Batches: [50/196]\tLoss: 0.7035 (Average: 0.7375)\tPrecision: 73.828 (Average: 73.889)\n",
      "Epoch: No: [23] Batches: [100/196]\tLoss: 0.6917 (Average: 0.7371)\tPrecision: 77.734 (Average: 74.141)\n",
      "Epoch: No: [23] Batches: [150/196]\tLoss: 0.7449 (Average: 0.7318)\tPrecision: 72.266 (Average: 74.299)\n",
      "Test Accuracy\t  Top Precision: 67.050 (Error: 32.950 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [24] Batches: [0/196]\tLoss: 0.6368 (Average: 0.6368)\tPrecision: 79.688 (Average: 79.688)\n",
      "Epoch: No: [24] Batches: [50/196]\tLoss: 0.8341 (Average: 0.7457)\tPrecision: 71.875 (Average: 74.104)\n",
      "Epoch: No: [24] Batches: [100/196]\tLoss: 0.7635 (Average: 0.7398)\tPrecision: 70.703 (Average: 74.153)\n",
      "Epoch: No: [24] Batches: [150/196]\tLoss: 0.7713 (Average: 0.7393)\tPrecision: 71.875 (Average: 74.268)\n",
      "Test Accuracy\t  Top Precision: 72.740 (Error: 27.260 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [25] Batches: [0/196]\tLoss: 0.6792 (Average: 0.6792)\tPrecision: 76.953 (Average: 76.953)\n",
      "Epoch: No: [25] Batches: [50/196]\tLoss: 0.7611 (Average: 0.7282)\tPrecision: 74.219 (Average: 74.579)\n",
      "Epoch: No: [25] Batches: [100/196]\tLoss: 0.8084 (Average: 0.7284)\tPrecision: 70.703 (Average: 74.675)\n",
      "Epoch: No: [25] Batches: [150/196]\tLoss: 0.7760 (Average: 0.7236)\tPrecision: 75.000 (Average: 74.824)\n",
      "Test Accuracy\t  Top Precision: 72.320 (Error: 27.680 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [26] Batches: [0/196]\tLoss: 0.6150 (Average: 0.6150)\tPrecision: 80.078 (Average: 80.078)\n",
      "Epoch: No: [26] Batches: [50/196]\tLoss: 0.7193 (Average: 0.7038)\tPrecision: 75.391 (Average: 75.100)\n",
      "Epoch: No: [26] Batches: [100/196]\tLoss: 0.8032 (Average: 0.7057)\tPrecision: 71.484 (Average: 75.228)\n",
      "Epoch: No: [26] Batches: [150/196]\tLoss: 0.6363 (Average: 0.7024)\tPrecision: 75.391 (Average: 75.344)\n",
      "Test Accuracy\t  Top Precision: 72.160 (Error: 27.840 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [27] Batches: [0/196]\tLoss: 0.6372 (Average: 0.6372)\tPrecision: 74.219 (Average: 74.219)\n",
      "Epoch: No: [27] Batches: [50/196]\tLoss: 0.7895 (Average: 0.6806)\tPrecision: 75.781 (Average: 75.973)\n",
      "Epoch: No: [27] Batches: [100/196]\tLoss: 0.6944 (Average: 0.6879)\tPrecision: 74.219 (Average: 76.056)\n",
      "Epoch: No: [27] Batches: [150/196]\tLoss: 0.6458 (Average: 0.6904)\tPrecision: 78.125 (Average: 76.074)\n",
      "Test Accuracy\t  Top Precision: 70.430 (Error: 29.570 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [28] Batches: [0/196]\tLoss: 0.7089 (Average: 0.7089)\tPrecision: 73.828 (Average: 73.828)\n",
      "Epoch: No: [28] Batches: [50/196]\tLoss: 0.5669 (Average: 0.6960)\tPrecision: 78.516 (Average: 75.812)\n",
      "Epoch: No: [28] Batches: [100/196]\tLoss: 0.5809 (Average: 0.6854)\tPrecision: 78.516 (Average: 76.385)\n",
      "Epoch: No: [28] Batches: [150/196]\tLoss: 0.6510 (Average: 0.6777)\tPrecision: 73.047 (Average: 76.583)\n",
      "Test Accuracy\t  Top Precision: 70.000 (Error: 30.000 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [29] Batches: [0/196]\tLoss: 0.7484 (Average: 0.7484)\tPrecision: 79.688 (Average: 79.688)\n",
      "Epoch: No: [29] Batches: [50/196]\tLoss: 0.5339 (Average: 0.6928)\tPrecision: 82.812 (Average: 76.348)\n",
      "Epoch: No: [29] Batches: [100/196]\tLoss: 0.6125 (Average: 0.6821)\tPrecision: 78.516 (Average: 76.543)\n",
      "Epoch: No: [29] Batches: [150/196]\tLoss: 0.6023 (Average: 0.6758)\tPrecision: 77.344 (Average: 76.606)\n",
      "Test Accuracy\t  Top Precision: 71.650 (Error: 28.350 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [30] Batches: [0/196]\tLoss: 0.7470 (Average: 0.7470)\tPrecision: 72.656 (Average: 72.656)\n",
      "Epoch: No: [30] Batches: [50/196]\tLoss: 0.5357 (Average: 0.6540)\tPrecision: 81.250 (Average: 77.053)\n",
      "Epoch: No: [30] Batches: [100/196]\tLoss: 0.6660 (Average: 0.6626)\tPrecision: 77.734 (Average: 76.895)\n",
      "Epoch: No: [30] Batches: [150/196]\tLoss: 0.7527 (Average: 0.6560)\tPrecision: 71.875 (Average: 77.028)\n",
      "Test Accuracy\t  Top Precision: 74.240 (Error: 25.760 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [31] Batches: [0/196]\tLoss: 0.7545 (Average: 0.7545)\tPrecision: 73.047 (Average: 73.047)\n",
      "Epoch: No: [31] Batches: [50/196]\tLoss: 0.7585 (Average: 0.6463)\tPrecision: 73.438 (Average: 77.489)\n",
      "Epoch: No: [31] Batches: [100/196]\tLoss: 0.6457 (Average: 0.6459)\tPrecision: 80.078 (Average: 77.464)\n",
      "Epoch: No: [31] Batches: [150/196]\tLoss: 0.5267 (Average: 0.6452)\tPrecision: 81.641 (Average: 77.592)\n",
      "Test Accuracy\t  Top Precision: 76.720 (Error: 23.280 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [32] Batches: [0/196]\tLoss: 0.6747 (Average: 0.6747)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [32] Batches: [50/196]\tLoss: 0.6317 (Average: 0.6281)\tPrecision: 77.344 (Average: 77.619)\n",
      "Epoch: No: [32] Batches: [100/196]\tLoss: 0.6146 (Average: 0.6273)\tPrecision: 77.344 (Average: 78.040)\n",
      "Epoch: No: [32] Batches: [150/196]\tLoss: 0.6339 (Average: 0.6328)\tPrecision: 77.734 (Average: 77.830)\n",
      "Test Accuracy\t  Top Precision: 71.520 (Error: 28.480 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [33] Batches: [0/196]\tLoss: 0.7656 (Average: 0.7656)\tPrecision: 74.219 (Average: 74.219)\n",
      "Epoch: No: [33] Batches: [50/196]\tLoss: 0.6161 (Average: 0.6326)\tPrecision: 77.344 (Average: 78.225)\n",
      "Epoch: No: [33] Batches: [100/196]\tLoss: 0.6732 (Average: 0.6243)\tPrecision: 77.344 (Average: 78.624)\n",
      "Epoch: No: [33] Batches: [150/196]\tLoss: 0.6233 (Average: 0.6290)\tPrecision: 77.734 (Average: 78.492)\n",
      "Test Accuracy\t  Top Precision: 75.190 (Error: 24.810 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [34] Batches: [0/196]\tLoss: 0.5767 (Average: 0.5767)\tPrecision: 78.906 (Average: 78.906)\n",
      "Epoch: No: [34] Batches: [50/196]\tLoss: 0.5429 (Average: 0.6142)\tPrecision: 78.906 (Average: 78.653)\n",
      "Epoch: No: [34] Batches: [100/196]\tLoss: 0.5963 (Average: 0.6274)\tPrecision: 78.516 (Average: 78.110)\n",
      "Epoch: No: [34] Batches: [150/196]\tLoss: 0.6429 (Average: 0.6224)\tPrecision: 78.516 (Average: 78.278)\n",
      "Test Accuracy\t  Top Precision: 76.670 (Error: 23.330 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [35] Batches: [0/196]\tLoss: 0.6255 (Average: 0.6255)\tPrecision: 78.125 (Average: 78.125)\n",
      "Epoch: No: [35] Batches: [50/196]\tLoss: 0.6097 (Average: 0.5946)\tPrecision: 78.125 (Average: 79.818)\n",
      "Epoch: No: [35] Batches: [100/196]\tLoss: 0.7070 (Average: 0.6053)\tPrecision: 73.828 (Average: 79.208)\n",
      "Epoch: No: [35] Batches: [150/196]\tLoss: 0.5612 (Average: 0.6043)\tPrecision: 79.297 (Average: 79.266)\n",
      "Test Accuracy\t  Top Precision: 75.420 (Error: 24.580 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [36] Batches: [0/196]\tLoss: 0.5642 (Average: 0.5642)\tPrecision: 80.078 (Average: 80.078)\n",
      "Epoch: No: [36] Batches: [50/196]\tLoss: 0.5043 (Average: 0.5851)\tPrecision: 81.250 (Average: 79.833)\n",
      "Epoch: No: [36] Batches: [100/196]\tLoss: 0.6319 (Average: 0.6011)\tPrecision: 76.953 (Average: 79.448)\n",
      "Epoch: No: [36] Batches: [150/196]\tLoss: 0.5229 (Average: 0.6039)\tPrecision: 81.250 (Average: 79.209)\n",
      "Test Accuracy\t  Top Precision: 75.130 (Error: 24.870 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [37] Batches: [0/196]\tLoss: 0.5563 (Average: 0.5563)\tPrecision: 78.516 (Average: 78.516)\n",
      "Epoch: No: [37] Batches: [50/196]\tLoss: 0.6471 (Average: 0.6052)\tPrecision: 77.734 (Average: 79.458)\n",
      "Epoch: No: [37] Batches: [100/196]\tLoss: 0.5299 (Average: 0.5930)\tPrecision: 82.422 (Average: 79.788)\n",
      "Epoch: No: [37] Batches: [150/196]\tLoss: 0.6094 (Average: 0.5955)\tPrecision: 78.906 (Average: 79.563)\n",
      "Test Accuracy\t  Top Precision: 77.710 (Error: 22.290 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [38] Batches: [0/196]\tLoss: 0.5456 (Average: 0.5456)\tPrecision: 80.078 (Average: 80.078)\n",
      "Epoch: No: [38] Batches: [50/196]\tLoss: 0.6234 (Average: 0.5917)\tPrecision: 79.297 (Average: 79.756)\n",
      "Epoch: No: [38] Batches: [100/196]\tLoss: 0.5259 (Average: 0.5934)\tPrecision: 79.297 (Average: 79.455)\n",
      "Epoch: No: [38] Batches: [150/196]\tLoss: 0.5667 (Average: 0.5850)\tPrecision: 79.688 (Average: 79.695)\n",
      "Test Accuracy\t  Top Precision: 72.560 (Error: 27.440 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [39] Batches: [0/196]\tLoss: 0.5541 (Average: 0.5541)\tPrecision: 80.469 (Average: 80.469)\n",
      "Epoch: No: [39] Batches: [50/196]\tLoss: 0.5060 (Average: 0.5866)\tPrecision: 82.812 (Average: 79.642)\n",
      "Epoch: No: [39] Batches: [100/196]\tLoss: 0.5148 (Average: 0.5887)\tPrecision: 82.812 (Average: 79.452)\n",
      "Epoch: No: [39] Batches: [150/196]\tLoss: 0.5718 (Average: 0.5781)\tPrecision: 80.469 (Average: 79.848)\n",
      "Test Accuracy\t  Top Precision: 74.970 (Error: 25.030 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [40] Batches: [0/196]\tLoss: 0.6256 (Average: 0.6256)\tPrecision: 76.953 (Average: 76.953)\n",
      "Epoch: No: [40] Batches: [50/196]\tLoss: 0.6369 (Average: 0.5547)\tPrecision: 75.000 (Average: 80.576)\n",
      "Epoch: No: [40] Batches: [100/196]\tLoss: 0.5665 (Average: 0.5547)\tPrecision: 78.516 (Average: 80.507)\n",
      "Epoch: No: [40] Batches: [150/196]\tLoss: 0.6369 (Average: 0.5663)\tPrecision: 76.953 (Average: 80.270)\n",
      "Test Accuracy\t  Top Precision: 71.250 (Error: 28.750 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [41] Batches: [0/196]\tLoss: 0.6205 (Average: 0.6205)\tPrecision: 76.953 (Average: 76.953)\n",
      "Epoch: No: [41] Batches: [50/196]\tLoss: 0.4770 (Average: 0.5568)\tPrecision: 85.156 (Average: 80.959)\n",
      "Epoch: No: [41] Batches: [100/196]\tLoss: 0.5438 (Average: 0.5623)\tPrecision: 80.078 (Average: 80.825)\n",
      "Epoch: No: [41] Batches: [150/196]\tLoss: 0.4442 (Average: 0.5572)\tPrecision: 83.984 (Average: 80.862)\n",
      "Test Accuracy\t  Top Precision: 75.500 (Error: 24.500 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [42] Batches: [0/196]\tLoss: 0.5579 (Average: 0.5579)\tPrecision: 78.125 (Average: 78.125)\n",
      "Epoch: No: [42] Batches: [50/196]\tLoss: 0.5332 (Average: 0.5580)\tPrecision: 81.250 (Average: 80.476)\n",
      "Epoch: No: [42] Batches: [100/196]\tLoss: 0.5544 (Average: 0.5528)\tPrecision: 81.641 (Average: 80.747)\n",
      "Epoch: No: [42] Batches: [150/196]\tLoss: 0.6169 (Average: 0.5616)\tPrecision: 78.125 (Average: 80.332)\n",
      "Test Accuracy\t  Top Precision: 77.500 (Error: 22.500 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [43] Batches: [0/196]\tLoss: 0.5060 (Average: 0.5060)\tPrecision: 81.250 (Average: 81.250)\n",
      "Epoch: No: [43] Batches: [50/196]\tLoss: 0.5516 (Average: 0.5319)\tPrecision: 81.641 (Average: 81.533)\n",
      "Epoch: No: [43] Batches: [100/196]\tLoss: 0.5557 (Average: 0.5438)\tPrecision: 80.078 (Average: 81.161)\n",
      "Epoch: No: [43] Batches: [150/196]\tLoss: 0.5257 (Average: 0.5496)\tPrecision: 80.469 (Average: 80.950)\n",
      "Test Accuracy\t  Top Precision: 78.250 (Error: 21.750 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [44] Batches: [0/196]\tLoss: 0.5955 (Average: 0.5955)\tPrecision: 79.688 (Average: 79.688)\n",
      "Epoch: No: [44] Batches: [50/196]\tLoss: 0.5930 (Average: 0.5488)\tPrecision: 78.906 (Average: 81.227)\n",
      "Epoch: No: [44] Batches: [100/196]\tLoss: 0.6375 (Average: 0.5493)\tPrecision: 77.344 (Average: 81.107)\n",
      "Epoch: No: [44] Batches: [150/196]\tLoss: 0.5570 (Average: 0.5465)\tPrecision: 79.688 (Average: 81.059)\n",
      "Test Accuracy\t  Top Precision: 76.080 (Error: 23.920 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [45] Batches: [0/196]\tLoss: 0.5053 (Average: 0.5053)\tPrecision: 82.031 (Average: 82.031)\n",
      "Epoch: No: [45] Batches: [50/196]\tLoss: 0.5116 (Average: 0.5387)\tPrecision: 83.594 (Average: 81.434)\n",
      "Epoch: No: [45] Batches: [100/196]\tLoss: 0.4044 (Average: 0.5353)\tPrecision: 85.547 (Average: 81.621)\n",
      "Epoch: No: [45] Batches: [150/196]\tLoss: 0.5176 (Average: 0.5361)\tPrecision: 82.812 (Average: 81.540)\n",
      "Test Accuracy\t  Top Precision: 76.000 (Error: 24.000 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [46] Batches: [0/196]\tLoss: 0.4179 (Average: 0.4179)\tPrecision: 86.719 (Average: 86.719)\n",
      "Epoch: No: [46] Batches: [50/196]\tLoss: 0.4421 (Average: 0.5106)\tPrecision: 86.328 (Average: 82.629)\n",
      "Epoch: No: [46] Batches: [100/196]\tLoss: 0.5864 (Average: 0.5142)\tPrecision: 82.812 (Average: 82.290)\n",
      "Epoch: No: [46] Batches: [150/196]\tLoss: 0.4775 (Average: 0.5191)\tPrecision: 86.328 (Average: 82.140)\n",
      "Test Accuracy\t  Top Precision: 75.710 (Error: 24.290 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [47] Batches: [0/196]\tLoss: 0.4887 (Average: 0.4887)\tPrecision: 83.984 (Average: 83.984)\n",
      "Epoch: No: [47] Batches: [50/196]\tLoss: 0.5196 (Average: 0.5111)\tPrecision: 81.641 (Average: 82.276)\n",
      "Epoch: No: [47] Batches: [100/196]\tLoss: 0.5207 (Average: 0.5134)\tPrecision: 80.469 (Average: 82.275)\n",
      "Epoch: No: [47] Batches: [150/196]\tLoss: 0.4970 (Average: 0.5191)\tPrecision: 81.641 (Average: 82.163)\n",
      "Test Accuracy\t  Top Precision: 78.810 (Error: 21.190 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [48] Batches: [0/196]\tLoss: 0.5672 (Average: 0.5672)\tPrecision: 80.469 (Average: 80.469)\n",
      "Epoch: No: [48] Batches: [50/196]\tLoss: 0.4960 (Average: 0.5313)\tPrecision: 81.641 (Average: 81.296)\n",
      "Epoch: No: [48] Batches: [100/196]\tLoss: 0.5888 (Average: 0.5284)\tPrecision: 82.031 (Average: 81.776)\n",
      "Epoch: No: [48] Batches: [150/196]\tLoss: 0.5090 (Average: 0.5255)\tPrecision: 83.203 (Average: 81.992)\n",
      "Test Accuracy\t  Top Precision: 78.770 (Error: 21.230 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [49] Batches: [0/196]\tLoss: 0.4988 (Average: 0.4988)\tPrecision: 83.984 (Average: 83.984)\n",
      "Epoch: No: [49] Batches: [50/196]\tLoss: 0.4941 (Average: 0.5010)\tPrecision: 85.938 (Average: 82.790)\n",
      "Epoch: No: [49] Batches: [100/196]\tLoss: 0.6531 (Average: 0.5103)\tPrecision: 77.734 (Average: 82.577)\n",
      "Epoch: No: [49] Batches: [150/196]\tLoss: 0.6057 (Average: 0.5115)\tPrecision: 77.344 (Average: 82.450)\n",
      "Test Accuracy\t  Top Precision: 76.620 (Error: 23.380 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [50] Batches: [0/196]\tLoss: 0.4972 (Average: 0.4972)\tPrecision: 82.422 (Average: 82.422)\n",
      "Epoch: No: [50] Batches: [50/196]\tLoss: 0.4617 (Average: 0.5233)\tPrecision: 86.328 (Average: 82.138)\n",
      "Epoch: No: [50] Batches: [100/196]\tLoss: 0.4931 (Average: 0.5183)\tPrecision: 81.250 (Average: 82.283)\n",
      "Epoch: No: [50] Batches: [150/196]\tLoss: 0.4405 (Average: 0.5113)\tPrecision: 82.812 (Average: 82.468)\n",
      "Test Accuracy\t  Top Precision: 76.200 (Error: 23.800 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [51] Batches: [0/196]\tLoss: 0.5738 (Average: 0.5738)\tPrecision: 79.297 (Average: 79.297)\n",
      "Epoch: No: [51] Batches: [50/196]\tLoss: 0.5510 (Average: 0.4979)\tPrecision: 81.641 (Average: 82.835)\n",
      "Epoch: No: [51] Batches: [100/196]\tLoss: 0.5323 (Average: 0.5069)\tPrecision: 82.812 (Average: 82.534)\n",
      "Epoch: No: [51] Batches: [150/196]\tLoss: 0.3715 (Average: 0.5039)\tPrecision: 87.109 (Average: 82.650)\n",
      "Test Accuracy\t  Top Precision: 77.690 (Error: 22.310 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [52] Batches: [0/196]\tLoss: 0.4962 (Average: 0.4962)\tPrecision: 81.250 (Average: 81.250)\n",
      "Epoch: No: [52] Batches: [50/196]\tLoss: 0.4712 (Average: 0.4901)\tPrecision: 83.984 (Average: 83.341)\n",
      "Epoch: No: [52] Batches: [100/196]\tLoss: 0.6562 (Average: 0.4971)\tPrecision: 79.297 (Average: 83.072)\n",
      "Epoch: No: [52] Batches: [150/196]\tLoss: 0.4718 (Average: 0.5024)\tPrecision: 85.156 (Average: 82.706)\n",
      "Test Accuracy\t  Top Precision: 78.120 (Error: 21.880 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [53] Batches: [0/196]\tLoss: 0.5199 (Average: 0.5199)\tPrecision: 80.469 (Average: 80.469)\n",
      "Epoch: No: [53] Batches: [50/196]\tLoss: 0.4814 (Average: 0.4892)\tPrecision: 85.938 (Average: 83.264)\n",
      "Epoch: No: [53] Batches: [100/196]\tLoss: 0.4253 (Average: 0.4890)\tPrecision: 84.766 (Average: 83.215)\n",
      "Epoch: No: [53] Batches: [150/196]\tLoss: 0.6032 (Average: 0.4963)\tPrecision: 80.469 (Average: 83.014)\n",
      "Test Accuracy\t  Top Precision: 78.530 (Error: 21.470 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [54] Batches: [0/196]\tLoss: 0.5170 (Average: 0.5170)\tPrecision: 80.859 (Average: 80.859)\n",
      "Epoch: No: [54] Batches: [50/196]\tLoss: 0.5472 (Average: 0.4970)\tPrecision: 81.641 (Average: 83.088)\n",
      "Epoch: No: [54] Batches: [100/196]\tLoss: 0.4854 (Average: 0.4913)\tPrecision: 85.547 (Average: 83.207)\n",
      "Epoch: No: [54] Batches: [150/196]\tLoss: 0.4212 (Average: 0.4883)\tPrecision: 84.766 (Average: 83.270)\n",
      "Test Accuracy\t  Top Precision: 78.180 (Error: 21.820 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [55] Batches: [0/196]\tLoss: 0.4348 (Average: 0.4348)\tPrecision: 85.156 (Average: 85.156)\n",
      "Epoch: No: [55] Batches: [50/196]\tLoss: 0.4976 (Average: 0.4767)\tPrecision: 85.156 (Average: 83.693)\n",
      "Epoch: No: [55] Batches: [100/196]\tLoss: 0.4758 (Average: 0.4831)\tPrecision: 85.938 (Average: 83.338)\n",
      "Epoch: No: [55] Batches: [150/196]\tLoss: 0.5132 (Average: 0.4863)\tPrecision: 79.688 (Average: 83.154)\n",
      "Test Accuracy\t  Top Precision: 75.800 (Error: 24.200 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [56] Batches: [0/196]\tLoss: 0.4143 (Average: 0.4143)\tPrecision: 86.328 (Average: 86.328)\n",
      "Epoch: No: [56] Batches: [50/196]\tLoss: 0.4730 (Average: 0.4888)\tPrecision: 83.203 (Average: 82.950)\n",
      "Epoch: No: [56] Batches: [100/196]\tLoss: 0.4415 (Average: 0.4873)\tPrecision: 87.109 (Average: 83.311)\n",
      "Epoch: No: [56] Batches: [150/196]\tLoss: 0.5531 (Average: 0.4814)\tPrecision: 81.641 (Average: 83.669)\n",
      "Test Accuracy\t  Top Precision: 78.440 (Error: 21.560 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [57] Batches: [0/196]\tLoss: 0.4814 (Average: 0.4814)\tPrecision: 84.766 (Average: 84.766)\n",
      "Epoch: No: [57] Batches: [50/196]\tLoss: 0.5600 (Average: 0.4727)\tPrecision: 81.250 (Average: 83.686)\n",
      "Epoch: No: [57] Batches: [100/196]\tLoss: 0.5687 (Average: 0.4682)\tPrecision: 79.688 (Average: 83.845)\n",
      "Epoch: No: [57] Batches: [150/196]\tLoss: 0.4182 (Average: 0.4686)\tPrecision: 87.500 (Average: 83.938)\n",
      "Test Accuracy\t  Top Precision: 78.340 (Error: 21.660 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [58] Batches: [0/196]\tLoss: 0.4353 (Average: 0.4353)\tPrecision: 85.938 (Average: 85.938)\n",
      "Epoch: No: [58] Batches: [50/196]\tLoss: 0.4251 (Average: 0.4652)\tPrecision: 87.500 (Average: 84.046)\n",
      "Epoch: No: [58] Batches: [100/196]\tLoss: 0.5065 (Average: 0.4726)\tPrecision: 82.812 (Average: 83.899)\n",
      "Epoch: No: [58] Batches: [150/196]\tLoss: 0.4774 (Average: 0.4719)\tPrecision: 85.547 (Average: 83.873)\n",
      "Test Accuracy\t  Top Precision: 80.950 (Error: 19.050 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [59] Batches: [0/196]\tLoss: 0.4064 (Average: 0.4064)\tPrecision: 88.672 (Average: 88.672)\n",
      "Epoch: No: [59] Batches: [50/196]\tLoss: 0.4854 (Average: 0.4574)\tPrecision: 84.375 (Average: 84.329)\n",
      "Epoch: No: [59] Batches: [100/196]\tLoss: 0.4378 (Average: 0.4703)\tPrecision: 86.719 (Average: 84.023)\n",
      "Epoch: No: [59] Batches: [150/196]\tLoss: 0.4214 (Average: 0.4658)\tPrecision: 86.328 (Average: 84.160)\n",
      "Test Accuracy\t  Top Precision: 78.540 (Error: 21.460 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [60] Batches: [0/196]\tLoss: 0.4146 (Average: 0.4146)\tPrecision: 86.719 (Average: 86.719)\n",
      "Epoch: No: [60] Batches: [50/196]\tLoss: 0.4112 (Average: 0.4500)\tPrecision: 87.500 (Average: 84.643)\n",
      "Epoch: No: [60] Batches: [100/196]\tLoss: 0.4282 (Average: 0.4566)\tPrecision: 85.156 (Average: 84.290)\n",
      "Epoch: No: [60] Batches: [150/196]\tLoss: 0.3974 (Average: 0.4555)\tPrecision: 87.109 (Average: 84.316)\n",
      "Test Accuracy\t  Top Precision: 80.420 (Error: 19.580 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [61] Batches: [0/196]\tLoss: 0.4893 (Average: 0.4893)\tPrecision: 80.859 (Average: 80.859)\n",
      "Epoch: No: [61] Batches: [50/196]\tLoss: 0.5005 (Average: 0.4644)\tPrecision: 83.984 (Average: 83.877)\n",
      "Epoch: No: [61] Batches: [100/196]\tLoss: 0.3998 (Average: 0.4609)\tPrecision: 84.375 (Average: 84.008)\n",
      "Epoch: No: [61] Batches: [150/196]\tLoss: 0.4725 (Average: 0.4637)\tPrecision: 84.766 (Average: 84.010)\n",
      "Test Accuracy\t  Top Precision: 79.360 (Error: 20.640 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [62] Batches: [0/196]\tLoss: 0.4462 (Average: 0.4462)\tPrecision: 83.594 (Average: 83.594)\n",
      "Epoch: No: [62] Batches: [50/196]\tLoss: 0.5543 (Average: 0.4572)\tPrecision: 78.516 (Average: 83.954)\n",
      "Epoch: No: [62] Batches: [100/196]\tLoss: 0.4038 (Average: 0.4548)\tPrecision: 84.766 (Average: 84.255)\n",
      "Epoch: No: [62] Batches: [150/196]\tLoss: 0.4016 (Average: 0.4565)\tPrecision: 86.328 (Average: 84.217)\n",
      "Test Accuracy\t  Top Precision: 79.650 (Error: 20.350 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [63] Batches: [0/196]\tLoss: 0.4004 (Average: 0.4004)\tPrecision: 88.281 (Average: 88.281)\n",
      "Epoch: No: [63] Batches: [50/196]\tLoss: 0.3946 (Average: 0.4553)\tPrecision: 85.156 (Average: 84.589)\n",
      "Epoch: No: [63] Batches: [100/196]\tLoss: 0.4208 (Average: 0.4640)\tPrecision: 87.500 (Average: 84.259)\n",
      "Epoch: No: [63] Batches: [150/196]\tLoss: 0.3842 (Average: 0.4643)\tPrecision: 87.109 (Average: 84.228)\n",
      "Test Accuracy\t  Top Precision: 79.750 (Error: 20.250 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [64] Batches: [0/196]\tLoss: 0.4664 (Average: 0.4664)\tPrecision: 82.031 (Average: 82.031)\n",
      "Epoch: No: [64] Batches: [50/196]\tLoss: 0.4726 (Average: 0.4345)\tPrecision: 85.156 (Average: 84.773)\n",
      "Epoch: No: [64] Batches: [100/196]\tLoss: 0.4382 (Average: 0.4272)\tPrecision: 82.422 (Average: 85.137)\n",
      "Epoch: No: [64] Batches: [150/196]\tLoss: 0.5238 (Average: 0.4334)\tPrecision: 79.297 (Average: 84.988)\n",
      "Test Accuracy\t  Top Precision: 79.770 (Error: 20.230 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [65] Batches: [0/196]\tLoss: 0.5093 (Average: 0.5093)\tPrecision: 83.984 (Average: 83.984)\n",
      "Epoch: No: [65] Batches: [50/196]\tLoss: 0.4429 (Average: 0.4461)\tPrecision: 86.719 (Average: 85.064)\n",
      "Epoch: No: [65] Batches: [100/196]\tLoss: 0.4401 (Average: 0.4416)\tPrecision: 83.594 (Average: 84.971)\n",
      "Epoch: No: [65] Batches: [150/196]\tLoss: 0.3867 (Average: 0.4429)\tPrecision: 84.375 (Average: 84.802)\n",
      "Test Accuracy\t  Top Precision: 81.650 (Error: 18.350 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [66] Batches: [0/196]\tLoss: 0.4468 (Average: 0.4468)\tPrecision: 84.375 (Average: 84.375)\n",
      "Epoch: No: [66] Batches: [50/196]\tLoss: 0.4787 (Average: 0.4471)\tPrecision: 83.203 (Average: 84.812)\n",
      "Epoch: No: [66] Batches: [100/196]\tLoss: 0.4444 (Average: 0.4347)\tPrecision: 82.812 (Average: 85.357)\n",
      "Epoch: No: [66] Batches: [150/196]\tLoss: 0.5359 (Average: 0.4367)\tPrecision: 82.422 (Average: 85.185)\n",
      "Test Accuracy\t  Top Precision: 80.490 (Error: 19.510 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [67] Batches: [0/196]\tLoss: 0.4151 (Average: 0.4151)\tPrecision: 87.109 (Average: 87.109)\n",
      "Epoch: No: [67] Batches: [50/196]\tLoss: 0.3485 (Average: 0.4186)\tPrecision: 87.500 (Average: 85.662)\n",
      "Epoch: No: [67] Batches: [100/196]\tLoss: 0.5374 (Average: 0.4220)\tPrecision: 82.031 (Average: 85.632)\n",
      "Epoch: No: [67] Batches: [150/196]\tLoss: 0.4745 (Average: 0.4328)\tPrecision: 83.594 (Average: 85.262)\n",
      "Test Accuracy\t  Top Precision: 80.170 (Error: 19.830 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [68] Batches: [0/196]\tLoss: 0.2879 (Average: 0.2879)\tPrecision: 89.062 (Average: 89.062)\n",
      "Epoch: No: [68] Batches: [50/196]\tLoss: 0.4683 (Average: 0.4191)\tPrecision: 85.156 (Average: 85.777)\n",
      "Epoch: No: [68] Batches: [100/196]\tLoss: 0.4060 (Average: 0.4253)\tPrecision: 88.672 (Average: 85.489)\n",
      "Epoch: No: [68] Batches: [150/196]\tLoss: 0.3838 (Average: 0.4285)\tPrecision: 85.938 (Average: 85.433)\n",
      "Test Accuracy\t  Top Precision: 77.430 (Error: 22.570 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [69] Batches: [0/196]\tLoss: 0.4877 (Average: 0.4877)\tPrecision: 82.422 (Average: 82.422)\n",
      "Epoch: No: [69] Batches: [50/196]\tLoss: 0.4408 (Average: 0.4306)\tPrecision: 85.547 (Average: 85.432)\n",
      "Epoch: No: [69] Batches: [100/196]\tLoss: 0.4012 (Average: 0.4233)\tPrecision: 85.938 (Average: 85.733)\n",
      "Epoch: No: [69] Batches: [150/196]\tLoss: 0.4323 (Average: 0.4261)\tPrecision: 84.766 (Average: 85.526)\n",
      "Test Accuracy\t  Top Precision: 79.150 (Error: 20.850 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [70] Batches: [0/196]\tLoss: 0.4804 (Average: 0.4804)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [70] Batches: [50/196]\tLoss: 0.3934 (Average: 0.4225)\tPrecision: 83.984 (Average: 85.440)\n",
      "Epoch: No: [70] Batches: [100/196]\tLoss: 0.3721 (Average: 0.4161)\tPrecision: 86.719 (Average: 85.744)\n",
      "Epoch: No: [70] Batches: [150/196]\tLoss: 0.3746 (Average: 0.4201)\tPrecision: 88.672 (Average: 85.596)\n",
      "Test Accuracy\t  Top Precision: 77.670 (Error: 22.330 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [71] Batches: [0/196]\tLoss: 0.4553 (Average: 0.4553)\tPrecision: 84.375 (Average: 84.375)\n",
      "Epoch: No: [71] Batches: [50/196]\tLoss: 0.5193 (Average: 0.4167)\tPrecision: 83.203 (Average: 85.723)\n",
      "Epoch: No: [71] Batches: [100/196]\tLoss: 0.5160 (Average: 0.4181)\tPrecision: 83.203 (Average: 85.640)\n",
      "Epoch: No: [71] Batches: [150/196]\tLoss: 0.5130 (Average: 0.4189)\tPrecision: 84.766 (Average: 85.697)\n",
      "Test Accuracy\t  Top Precision: 82.110 (Error: 17.890 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [72] Batches: [0/196]\tLoss: 0.3288 (Average: 0.3288)\tPrecision: 87.500 (Average: 87.500)\n",
      "Epoch: No: [72] Batches: [50/196]\tLoss: 0.4363 (Average: 0.4109)\tPrecision: 83.984 (Average: 85.685)\n",
      "Epoch: No: [72] Batches: [100/196]\tLoss: 0.3935 (Average: 0.4080)\tPrecision: 87.891 (Average: 85.903)\n",
      "Epoch: No: [72] Batches: [150/196]\tLoss: 0.4109 (Average: 0.4107)\tPrecision: 85.547 (Average: 85.904)\n",
      "Test Accuracy\t  Top Precision: 82.540 (Error: 17.460 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [73] Batches: [0/196]\tLoss: 0.3673 (Average: 0.3673)\tPrecision: 88.672 (Average: 88.672)\n",
      "Epoch: No: [73] Batches: [50/196]\tLoss: 0.4591 (Average: 0.4089)\tPrecision: 84.766 (Average: 86.045)\n",
      "Epoch: No: [73] Batches: [100/196]\tLoss: 0.3860 (Average: 0.4032)\tPrecision: 86.719 (Average: 86.259)\n",
      "Epoch: No: [73] Batches: [150/196]\tLoss: 0.3779 (Average: 0.4141)\tPrecision: 88.672 (Average: 85.904)\n",
      "Test Accuracy\t  Top Precision: 81.670 (Error: 18.330 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [74] Batches: [0/196]\tLoss: 0.3586 (Average: 0.3586)\tPrecision: 87.109 (Average: 87.109)\n",
      "Epoch: No: [74] Batches: [50/196]\tLoss: 0.5330 (Average: 0.3861)\tPrecision: 83.203 (Average: 87.209)\n",
      "Epoch: No: [74] Batches: [100/196]\tLoss: 0.3847 (Average: 0.3964)\tPrecision: 85.938 (Average: 86.638)\n",
      "Epoch: No: [74] Batches: [150/196]\tLoss: 0.5001 (Average: 0.4041)\tPrecision: 83.594 (Average: 86.457)\n",
      "Test Accuracy\t  Top Precision: 79.580 (Error: 20.420 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [75] Batches: [0/196]\tLoss: 0.3830 (Average: 0.3830)\tPrecision: 84.375 (Average: 84.375)\n",
      "Epoch: No: [75] Batches: [50/196]\tLoss: 0.3380 (Average: 0.3842)\tPrecision: 88.672 (Average: 87.025)\n",
      "Epoch: No: [75] Batches: [100/196]\tLoss: 0.4119 (Average: 0.3899)\tPrecision: 86.719 (Average: 86.746)\n",
      "Epoch: No: [75] Batches: [150/196]\tLoss: 0.4725 (Average: 0.3979)\tPrecision: 85.938 (Average: 86.463)\n",
      "Test Accuracy\t  Top Precision: 82.450 (Error: 17.550 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [76] Batches: [0/196]\tLoss: 0.3132 (Average: 0.3132)\tPrecision: 88.672 (Average: 88.672)\n",
      "Epoch: No: [76] Batches: [50/196]\tLoss: 0.5544 (Average: 0.3832)\tPrecision: 80.469 (Average: 86.987)\n",
      "Epoch: No: [76] Batches: [100/196]\tLoss: 0.2991 (Average: 0.3940)\tPrecision: 92.188 (Average: 86.510)\n",
      "Epoch: No: [76] Batches: [150/196]\tLoss: 0.3827 (Average: 0.3964)\tPrecision: 85.156 (Average: 86.527)\n",
      "Test Accuracy\t  Top Precision: 79.550 (Error: 20.450 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [77] Batches: [0/196]\tLoss: 0.4778 (Average: 0.4778)\tPrecision: 82.422 (Average: 82.422)\n",
      "Epoch: No: [77] Batches: [50/196]\tLoss: 0.3701 (Average: 0.3985)\tPrecision: 85.547 (Average: 86.029)\n",
      "Epoch: No: [77] Batches: [100/196]\tLoss: 0.3830 (Average: 0.3954)\tPrecision: 85.938 (Average: 86.274)\n",
      "Epoch: No: [77] Batches: [150/196]\tLoss: 0.3627 (Average: 0.3951)\tPrecision: 88.672 (Average: 86.289)\n",
      "Test Accuracy\t  Top Precision: 81.780 (Error: 18.220 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [78] Batches: [0/196]\tLoss: 0.2712 (Average: 0.2712)\tPrecision: 90.234 (Average: 90.234)\n",
      "Epoch: No: [78] Batches: [50/196]\tLoss: 0.4103 (Average: 0.3946)\tPrecision: 86.328 (Average: 86.428)\n",
      "Epoch: No: [78] Batches: [100/196]\tLoss: 0.3412 (Average: 0.3972)\tPrecision: 89.453 (Average: 86.417)\n",
      "Epoch: No: [78] Batches: [150/196]\tLoss: 0.4734 (Average: 0.4012)\tPrecision: 82.812 (Average: 86.351)\n",
      "Test Accuracy\t  Top Precision: 81.460 (Error: 18.540 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [79] Batches: [0/196]\tLoss: 0.2903 (Average: 0.2903)\tPrecision: 89.453 (Average: 89.453)\n",
      "Epoch: No: [79] Batches: [50/196]\tLoss: 0.3976 (Average: 0.3882)\tPrecision: 85.938 (Average: 86.543)\n",
      "Epoch: No: [79] Batches: [100/196]\tLoss: 0.3459 (Average: 0.3814)\tPrecision: 88.281 (Average: 86.823)\n",
      "Epoch: No: [79] Batches: [150/196]\tLoss: 0.3547 (Average: 0.3814)\tPrecision: 87.109 (Average: 86.892)\n",
      "Test Accuracy\t  Top Precision: 80.510 (Error: 19.490 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [80] Batches: [0/196]\tLoss: 0.3331 (Average: 0.3331)\tPrecision: 90.234 (Average: 90.234)\n",
      "Epoch: No: [80] Batches: [50/196]\tLoss: 0.3735 (Average: 0.3891)\tPrecision: 89.453 (Average: 86.903)\n",
      "Epoch: No: [80] Batches: [100/196]\tLoss: 0.3126 (Average: 0.3939)\tPrecision: 88.281 (Average: 86.707)\n",
      "Epoch: No: [80] Batches: [150/196]\tLoss: 0.4049 (Average: 0.3938)\tPrecision: 85.547 (Average: 86.618)\n",
      "Test Accuracy\t  Top Precision: 81.620 (Error: 18.380 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [81] Batches: [0/196]\tLoss: 0.3842 (Average: 0.3842)\tPrecision: 85.938 (Average: 85.938)\n",
      "Epoch: No: [81] Batches: [50/196]\tLoss: 0.3742 (Average: 0.3874)\tPrecision: 87.500 (Average: 86.558)\n",
      "Epoch: No: [81] Batches: [100/196]\tLoss: 0.3450 (Average: 0.3825)\tPrecision: 89.062 (Average: 86.784)\n",
      "Epoch: No: [81] Batches: [150/196]\tLoss: 0.3851 (Average: 0.3854)\tPrecision: 86.719 (Average: 86.750)\n",
      "Test Accuracy\t  Top Precision: 78.910 (Error: 21.090 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [82] Batches: [0/196]\tLoss: 0.4381 (Average: 0.4381)\tPrecision: 83.594 (Average: 83.594)\n",
      "Epoch: No: [82] Batches: [50/196]\tLoss: 0.3602 (Average: 0.3802)\tPrecision: 87.500 (Average: 86.987)\n",
      "Epoch: No: [82] Batches: [100/196]\tLoss: 0.4097 (Average: 0.3820)\tPrecision: 83.594 (Average: 86.970)\n",
      "Epoch: No: [82] Batches: [150/196]\tLoss: 0.4087 (Average: 0.3842)\tPrecision: 83.594 (Average: 86.908)\n",
      "Test Accuracy\t  Top Precision: 80.240 (Error: 19.760 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [83] Batches: [0/196]\tLoss: 0.4542 (Average: 0.4542)\tPrecision: 83.984 (Average: 83.984)\n",
      "Epoch: No: [83] Batches: [50/196]\tLoss: 0.3460 (Average: 0.3683)\tPrecision: 86.328 (Average: 87.102)\n",
      "Epoch: No: [83] Batches: [100/196]\tLoss: 0.3127 (Average: 0.3712)\tPrecision: 89.453 (Average: 87.264)\n",
      "Epoch: No: [83] Batches: [150/196]\tLoss: 0.4651 (Average: 0.3780)\tPrecision: 84.375 (Average: 87.084)\n",
      "Test Accuracy\t  Top Precision: 81.560 (Error: 18.440 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [84] Batches: [0/196]\tLoss: 0.3854 (Average: 0.3854)\tPrecision: 87.500 (Average: 87.500)\n",
      "Epoch: No: [84] Batches: [50/196]\tLoss: 0.4788 (Average: 0.3584)\tPrecision: 82.422 (Average: 87.561)\n",
      "Epoch: No: [84] Batches: [100/196]\tLoss: 0.3889 (Average: 0.3768)\tPrecision: 85.156 (Average: 87.094)\n",
      "Epoch: No: [84] Batches: [150/196]\tLoss: 0.3186 (Average: 0.3727)\tPrecision: 87.500 (Average: 87.298)\n",
      "Test Accuracy\t  Top Precision: 75.750 (Error: 24.250 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [85] Batches: [0/196]\tLoss: 0.5553 (Average: 0.5553)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [85] Batches: [50/196]\tLoss: 0.3947 (Average: 0.3662)\tPrecision: 85.156 (Average: 87.592)\n",
      "Epoch: No: [85] Batches: [100/196]\tLoss: 0.3287 (Average: 0.3757)\tPrecision: 89.062 (Average: 87.152)\n",
      "Epoch: No: [85] Batches: [150/196]\tLoss: 0.3631 (Average: 0.3764)\tPrecision: 85.938 (Average: 87.047)\n",
      "Test Accuracy\t  Top Precision: 81.670 (Error: 18.330 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [86] Batches: [0/196]\tLoss: 0.3331 (Average: 0.3331)\tPrecision: 90.234 (Average: 90.234)\n",
      "Epoch: No: [86] Batches: [50/196]\tLoss: 0.4473 (Average: 0.3742)\tPrecision: 85.547 (Average: 87.148)\n",
      "Epoch: No: [86] Batches: [100/196]\tLoss: 0.4133 (Average: 0.3683)\tPrecision: 88.672 (Average: 87.272)\n",
      "Epoch: No: [86] Batches: [150/196]\tLoss: 0.3943 (Average: 0.3683)\tPrecision: 87.109 (Average: 87.283)\n",
      "Test Accuracy\t  Top Precision: 81.220 (Error: 18.780 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [87] Batches: [0/196]\tLoss: 0.3555 (Average: 0.3555)\tPrecision: 86.328 (Average: 86.328)\n",
      "Epoch: No: [87] Batches: [50/196]\tLoss: 0.4145 (Average: 0.3628)\tPrecision: 86.719 (Average: 87.354)\n",
      "Epoch: No: [87] Batches: [100/196]\tLoss: 0.3323 (Average: 0.3715)\tPrecision: 87.891 (Average: 87.164)\n",
      "Epoch: No: [87] Batches: [150/196]\tLoss: 0.3759 (Average: 0.3748)\tPrecision: 87.500 (Average: 87.120)\n",
      "Test Accuracy\t  Top Precision: 81.010 (Error: 18.990 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [88] Batches: [0/196]\tLoss: 0.4066 (Average: 0.4066)\tPrecision: 85.156 (Average: 85.156)\n",
      "Epoch: No: [88] Batches: [50/196]\tLoss: 0.3693 (Average: 0.3718)\tPrecision: 87.109 (Average: 87.362)\n",
      "Epoch: No: [88] Batches: [100/196]\tLoss: 0.3819 (Average: 0.3670)\tPrecision: 85.938 (Average: 87.365)\n",
      "Epoch: No: [88] Batches: [150/196]\tLoss: 0.4040 (Average: 0.3666)\tPrecision: 85.547 (Average: 87.482)\n",
      "Test Accuracy\t  Top Precision: 83.640 (Error: 16.360 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [89] Batches: [0/196]\tLoss: 0.2820 (Average: 0.2820)\tPrecision: 91.797 (Average: 91.797)\n",
      "Epoch: No: [89] Batches: [50/196]\tLoss: 0.3254 (Average: 0.3551)\tPrecision: 87.891 (Average: 87.837)\n",
      "Epoch: No: [89] Batches: [100/196]\tLoss: 0.2893 (Average: 0.3598)\tPrecision: 87.500 (Average: 87.639)\n",
      "Epoch: No: [89] Batches: [150/196]\tLoss: 0.3529 (Average: 0.3637)\tPrecision: 87.891 (Average: 87.448)\n",
      "Test Accuracy\t  Top Precision: 82.310 (Error: 17.690 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [90] Batches: [0/196]\tLoss: 0.3724 (Average: 0.3724)\tPrecision: 85.938 (Average: 85.938)\n",
      "Epoch: No: [90] Batches: [50/196]\tLoss: 0.3753 (Average: 0.3513)\tPrecision: 86.328 (Average: 87.921)\n",
      "Epoch: No: [90] Batches: [100/196]\tLoss: 0.3191 (Average: 0.3578)\tPrecision: 90.625 (Average: 87.655)\n",
      "Epoch: No: [90] Batches: [150/196]\tLoss: 0.3959 (Average: 0.3584)\tPrecision: 85.156 (Average: 87.678)\n",
      "Test Accuracy\t  Top Precision: 80.520 (Error: 19.480 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [91] Batches: [0/196]\tLoss: 0.2916 (Average: 0.2916)\tPrecision: 91.016 (Average: 91.016)\n",
      "Epoch: No: [91] Batches: [50/196]\tLoss: 0.3451 (Average: 0.3623)\tPrecision: 88.672 (Average: 87.623)\n",
      "Epoch: No: [91] Batches: [100/196]\tLoss: 0.3116 (Average: 0.3752)\tPrecision: 88.672 (Average: 87.229)\n",
      "Epoch: No: [91] Batches: [150/196]\tLoss: 0.3194 (Average: 0.3692)\tPrecision: 88.281 (Average: 87.448)\n",
      "Test Accuracy\t  Top Precision: 81.410 (Error: 18.590 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [92] Batches: [0/196]\tLoss: 0.3561 (Average: 0.3561)\tPrecision: 88.281 (Average: 88.281)\n",
      "Epoch: No: [92] Batches: [50/196]\tLoss: 0.3466 (Average: 0.3503)\tPrecision: 87.891 (Average: 87.898)\n",
      "Epoch: No: [92] Batches: [100/196]\tLoss: 0.4407 (Average: 0.3506)\tPrecision: 86.328 (Average: 87.883)\n",
      "Epoch: No: [92] Batches: [150/196]\tLoss: 0.3608 (Average: 0.3577)\tPrecision: 86.328 (Average: 87.694)\n",
      "Test Accuracy\t  Top Precision: 79.180 (Error: 20.820 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [93] Batches: [0/196]\tLoss: 0.4147 (Average: 0.4147)\tPrecision: 85.156 (Average: 85.156)\n",
      "Epoch: No: [93] Batches: [50/196]\tLoss: 0.2887 (Average: 0.3477)\tPrecision: 87.891 (Average: 88.044)\n",
      "Epoch: No: [93] Batches: [100/196]\tLoss: 0.2602 (Average: 0.3504)\tPrecision: 91.406 (Average: 87.836)\n",
      "Epoch: No: [93] Batches: [150/196]\tLoss: 0.3919 (Average: 0.3517)\tPrecision: 87.109 (Average: 87.867)\n",
      "Test Accuracy\t  Top Precision: 78.740 (Error: 21.260 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [94] Batches: [0/196]\tLoss: 0.3695 (Average: 0.3695)\tPrecision: 88.281 (Average: 88.281)\n",
      "Epoch: No: [94] Batches: [50/196]\tLoss: 0.3655 (Average: 0.3550)\tPrecision: 88.281 (Average: 87.829)\n",
      "Epoch: No: [94] Batches: [100/196]\tLoss: 0.4480 (Average: 0.3472)\tPrecision: 83.594 (Average: 88.049)\n",
      "Epoch: No: [94] Batches: [150/196]\tLoss: 0.3033 (Average: 0.3524)\tPrecision: 89.062 (Average: 87.906)\n",
      "Test Accuracy\t  Top Precision: 80.800 (Error: 19.200 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [95] Batches: [0/196]\tLoss: 0.3188 (Average: 0.3188)\tPrecision: 90.234 (Average: 90.234)\n",
      "Epoch: No: [95] Batches: [50/196]\tLoss: 0.2509 (Average: 0.3491)\tPrecision: 91.016 (Average: 87.684)\n",
      "Epoch: No: [95] Batches: [100/196]\tLoss: 0.3287 (Average: 0.3506)\tPrecision: 87.891 (Average: 87.821)\n",
      "Epoch: No: [95] Batches: [150/196]\tLoss: 0.4179 (Average: 0.3540)\tPrecision: 84.375 (Average: 87.836)\n",
      "Test Accuracy\t  Top Precision: 80.650 (Error: 19.350 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [96] Batches: [0/196]\tLoss: 0.3600 (Average: 0.3600)\tPrecision: 86.328 (Average: 86.328)\n",
      "Epoch: No: [96] Batches: [50/196]\tLoss: 0.3371 (Average: 0.3455)\tPrecision: 88.672 (Average: 88.266)\n",
      "Epoch: No: [96] Batches: [100/196]\tLoss: 0.3541 (Average: 0.3498)\tPrecision: 87.891 (Average: 88.069)\n",
      "Epoch: No: [96] Batches: [150/196]\tLoss: 0.2886 (Average: 0.3481)\tPrecision: 89.844 (Average: 88.183)\n",
      "Test Accuracy\t  Top Precision: 83.630 (Error: 16.370 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [97] Batches: [0/196]\tLoss: 0.3092 (Average: 0.3092)\tPrecision: 91.406 (Average: 91.406)\n",
      "Epoch: No: [97] Batches: [50/196]\tLoss: 0.3100 (Average: 0.3553)\tPrecision: 87.500 (Average: 87.646)\n",
      "Epoch: No: [97] Batches: [100/196]\tLoss: 0.4656 (Average: 0.3468)\tPrecision: 85.156 (Average: 87.937)\n",
      "Epoch: No: [97] Batches: [150/196]\tLoss: 0.3293 (Average: 0.3511)\tPrecision: 88.281 (Average: 87.896)\n",
      "Test Accuracy\t  Top Precision: 83.030 (Error: 16.970 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [98] Batches: [0/196]\tLoss: 0.3317 (Average: 0.3317)\tPrecision: 89.453 (Average: 89.453)\n",
      "Epoch: No: [98] Batches: [50/196]\tLoss: 0.4226 (Average: 0.3418)\tPrecision: 85.156 (Average: 88.381)\n",
      "Epoch: No: [98] Batches: [100/196]\tLoss: 0.3859 (Average: 0.3396)\tPrecision: 88.281 (Average: 88.455)\n",
      "Epoch: No: [98] Batches: [150/196]\tLoss: 0.3195 (Average: 0.3395)\tPrecision: 89.453 (Average: 88.421)\n",
      "Test Accuracy\t  Top Precision: 82.770 (Error: 17.230 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [99] Batches: [0/196]\tLoss: 0.2866 (Average: 0.2866)\tPrecision: 87.109 (Average: 87.109)\n",
      "Epoch: No: [99] Batches: [50/196]\tLoss: 0.4466 (Average: 0.3496)\tPrecision: 85.547 (Average: 88.082)\n",
      "Epoch: No: [99] Batches: [100/196]\tLoss: 0.2919 (Average: 0.3437)\tPrecision: 89.844 (Average: 88.293)\n",
      "Epoch: No: [99] Batches: [150/196]\tLoss: 0.3111 (Average: 0.3483)\tPrecision: 88.281 (Average: 88.123)\n",
      "Test Accuracy\t  Top Precision: 82.030 (Error: 17.970 )\n",
      "\n",
      "The lowest error from model: Model 1 after 100 epochs is 16.360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args=ResNetParams()\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "best_precision = 0\n",
    "best_precision = run_epochs()\n",
    "print('The lowest error from model: {} after {} epochs is {error:.3f}'.format(args.arch,args.epochs, error=100-best_precision))\n",
    "model_save_name = 'project1_model.pt'\n",
    "#path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
    "\n",
    "#Saving the generated model and testing its loading\n",
    "path = model_save_name\n",
    "torch.save(model.state_dict(), path) \n",
    "model_path = path\n",
    "model.load_state_dict(torch.load(model_path, map_location=device), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e00a0c9c-eb50-475f-9682-4cd323a5ae59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAFNCAYAAADRi2EuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd3zeZb3/8deVvUez2iZt071nSmmBskrZUxFFliDi0eOeHH8eRdwej4ge9QgioIKIIIctlLZSRmmb7pHupk3SrKbZe1y/P677zmjvJHfa3Emavp+PRx7f3N/5uVMfDz5+ruv7uYy1FhEREREJvKDBDkBERETkbKHES0RERGSAKPESERERGSBKvEREREQGiBIvERERkQGixEtERERkgCjxEjkLGWOCjTE1xpix/XnuYDLGTDLGBKQ/zon3Nsa8aYy5LRBxGGP+0xjzv6d6vYgMbUq8RM4AnsTH+9NmjKnv9NlnAtATa22rtTbGWnukP88dqowxbxljvuNj/4eNMQXGmOC+3M9ae7m19ql+iOsyY0zuCff+vrX230733j6eda8x5l/9fV8R6RslXiJnAE/iE2OtjQGOANd12ndSAmCMCRn4KIe0J4E7fOy/A/iLtbZ1gOMRkbOUEi+RYcAY8wNjzN+MMX81xlQDtxtjlhhjPjDGVBhjCo0xvzLGhHrODzHGWGNMpufzXzzHXzfGVBtj1hpjxvf1XM/xq4wxe40xlcaYXxtj3jPGfKKbuP2J8dPGmP3GmHJjzK86XRtsjHnIGFNmjDkIXNnDn+gfwEhjzHmdrk8Crgb+5Pl8vTFmizGmyhhzxBjznz38vd/1fqfe4vBUmnI8f6sDxph7PfvjgZeBsZ2ql6mef8snOl1/kzFmp+dvtMoYM7XTsXxjzFeMMds9f++/GmPCe/g7dPd9Mowxrxhjjhtj9hlj7ul0bLExZpPn71JsjPkvz/4oY8zTnu9dYYxZb4xJ7uuzRc42SrxEho+bgKeBeOBvQAvwRSAZOB+XEHy6h+s/DvwnMAJXVft+X881xqQCzwJf9zz3ELCoh/v4E+PVQBYwH5dQXubZ/xngcmAucA5wS3cPsdbWAs8Bd3ba/TFgm7V2p+dzDXAbkABcB3zRGHNtD7F79RZHMXANEAd8Cvi1MWaOtbbS85wjnaqXJZ0vNMZMB/4MfB5IAd4CXvImpx63AMuBCbi/k6/KXm/+hvu3Gg18FPiZMeYiz7FfA/9lrY0DJuH+jgB3A1FABpAEfBZoOIVni5xVlHiJDB/vWmtftta2WWvrrbUbrLXrrLUt1tqDwCPART1c/5y1Ntta2ww8Bcw7hXOvBbZYa1/0HHsIONbdTfyM8cfW2kprbS7wr07PugV4yFqbb60tA37SQ7zghhtv6VQRutOzzxvLKmvtTs/fbyvwjI9YfOkxDs+/yUHrrAJWAkv9uC+45PAlT2zNnnvHA+d2OueX1toiz7Nfoed/t5N4qpWLgPuttQ3W2k3A43QkcM3AZGNMkrW22lq7rtP+ZGCSZx5gtrW2pi/PFjkbKfESGT7yOn8wxkwzxrxqjCkyxlQBD+L+Q9mdok6/1wExp3Du6M5xWGstkN/dTfyM0a9nAYd7iBfgbaAKuM4YMwVXQftrp1iWGGP+ZYwpNcZUAvf6iMWXHuMwxlxrjFnnGcarwFXH/B2SG935ftbaNtzfM73TOX35d+vuGcc8VUGvw52ecTcwA9jjGU682rP/CVwF7lnjXlD4idHcQpFeKfESGT5ObGHwe2AHriIRB3wHMAGOoRA39ASAMcbQNUk40enEWAiM6fS5x3YXniTwT7hK1x3Aa9baztW4Z4DngTHW2njgD37G0m0cxphI3NDcj4E0a20C8Gan+/bWduIoMK7T/YJwf98CP+Ly11Eg2RgT3WnfWO8zrLV7rLUfA1KB/waeN8ZEWGubrLUPWGunAxfghrr7/IatyNlGiZfI8BULVAK1nrlCPc3v6i+vAAuMMdd5qh9fxM1NCkSMzwJfMsakeybKf9OPa/6Em0d2D52GGTvFctxa22CMWYwb5jvdOMKBMKAUaPXMGVvW6XgxLumJ7eHe1xtjLvbM6/o6UA2s6+b83gQZYyI6/1hrDwHZwI+MMeHGmHm4KtdfAIwxdxhjkj3VtkpcsthmjLnUGDPLkwxW4YYe204xLpGzhhIvkeHrq8BduP9Q/x43gTqgrLXFuMnZvwDKgInAZqAxADH+DjdfajuwgY5J3z3Ftx9Yj0uIXj3h8GeAHxv3Vui3cEnPacVhra0Avgy8ABwHbsYlp97jO3BVtlzPm4GpJ8S7E/f3+R0uebsSuN4z3+tULAXqT/gB9282GTds+RzwLWvtvzzHrgZyPH+XnwMftdY24YYo/4FLunbihh2fPsW4RM4axlXfRUT6n3GNSY8CN1tr3xnseEREBpsqXiLSr4wxVxpjEjxvD/4nbghq/SCHJSIyJCjxEpH+dgFwEDc0dgVwk7W2u6FGEZGzioYaRURERAaIKl4iIiIiA0SJl4iIiMgAOSO6DCcnJ9vMzMzBDkNERESkVxs3bjxmrfXZw/CMSLwyMzPJzs4e7DBEREREemWM6XYJMw01ioiIiAwQJV4iIiIiA0SJl4iIiMgAOSPmeImIiMjQ1tzcTH5+Pg0NDYMdyoCJiIggIyOD0NBQv69R4iUiIiKnLT8/n9jYWDIzMzHGDHY4AWetpaysjPz8fMaPH+/3dRpqFBERkdPW0NBAUlLSWZF0ARhjSEpK6nOFT4mXiIiI9IuzJenyOpXvq8RLREREzniXXHIJb7zxRpd9v/zlL/nMZz7T7TUxMTGBDuskSrxERETkjHfrrbfyzDPPdNn3zDPPcOuttw5SRL4p8QLIz4bNfxnsKEREROQU3Xzzzbz66qs0NTUBkJuby9GjR5k/fz7Lli1jwYIFzJ49mxdffHFQ41TiBbDzBXj1q9DWNtiRiIiIyCkYMWIEixYt4vXXXwdcteuWW24hMjKSF154gU2bNrF69Wq++tWvYq0dtDgD2k7CGPNl4F7AAtuBu4FRwDNAErARuMNa2xTIOHqVPAVaGqDyCCRmDmooIiIiZ7rvvbyTXUer+vWeM0bH8d3rZvZ4jne48YYbbuCZZ57hsccew1rLt771LdasWUNQUBAFBQUUFxczcuTIfo3PXwGreBlj0oEvAAuttbOAYOBjwE+Bh6y1k4By4JOBisFvyVPc9ti+wY1DRERETtkNN9zAypUr2bRpE3V1dWRlZfHUU09RWlrKxo0b2bJlC2lpaYPa5DXQDVRDgEhjTDMQBRQClwIf9xx/EngA+F2A4+hZylS3Ld0Dk5cPaigiIiJnut4qU4ESExPDJZdcwj333NM+qb6yspLU1FRCQ0NZvXo1hw8fHpTYvAJW8bLWFgA/B47gEq5K3NBihbW2xXNaPpDu63pjzH3GmGxjTHZpaWmgwnSiRkBUEhzbG9jniIiISEDdeuutbN26tT3xuu2228jOzmb27Nn86U9/Ytq0aYMaX8AqXsaYROAGYDxQAfwduNLf6621jwCPACxcuDDws+CSpyrxEhEROcPdeOONXSbPJycns3btWp/n1tTUDFRY7QL5VuNlwCFrbam1thn4B3A+kGCM8SZ8GUBBAGPwX/JkJV4iIiISUIFMvI4Ai40xUcb11F8G7AJWAzd7zrkLGNyGGl4pU6GuDGrLBjsSERERGaYCOcdrHfAcsAnXSiIIN3T4TeArxpj9uJYSjwUqhj5pf7NRVS8REREJjIC+1Wit/S7w3RN2HwQWBfK5p6Q98doD45YMbiwiIiIyLKlzvVf8GAiJUC8vERERCRglXl5BQZA02fXyEhEREQkAJV6dpUzRHC8REZEzUFlZGfPmzWPevHmMHDmS9PT09s/ehbN7c/fdd7NnT2ALMIHuXH9mSZ4CO/4BzfUQGjnY0YiIiIifkpKS2LJlCwAPPPAAMTExfO1rX+tyjrUWay1BQb7rTo8//njA41TFC1ixq5if/XO3Z4K9hbL9gx2SiIiI9IP9+/czY8YMbrvtNmbOnElhYSH33XcfCxcuZObMmTz44IPt515wwQVs2bKFlpYWEhISuP/++5k7dy5LliyhpKSkX+JR4gVsy6/g92sOUhs3we3QPC8REZFhY/fu3Xz5y19m165dpKen85Of/ITs7Gy2bt3KihUr2LVr10nXVFZWctFFF7F161aWLFnCH//4x36JRUONwOIJSfx61X6yq5O4CKM3G0VERE7H6/dD0fb+vefI2XDVT07p0okTJ7Jw4cL2z3/961957LHHaGlp4ejRo+zatYsZM2Z0uSYyMpKrrroKgKysLN55551Tj70TJV7AgrGJhAYb3j9cw0WJ41wvLxERERkWoqOj23/ft28fDz/8MOvXrychIYHbb7+dhoaGk64JCwtr/z04OJiWlpZ+iUWJFxAZFsy8MQl8cLDMs1i2Kl4iIiKn7BQrUwOhqqqK2NhY4uLiKCws5I033uDKK68csOdrjpfHkglJbC+opClxoku82loHOyQRERHpZwsWLGDGjBlMmzaNO++8k/PPP39An2+stQP6wFOxcOFCm52dHdBnvL//GB//wzr+uTSXaRu+BV/YAiPGB/SZIiIiw0VOTg7Tp08f7DAGnK/vbYzZaK1d6Ot8Vbw85o9NJCw4iPU1SW6HGqmKiIhIP1Pi5eGd5/VmcZzbocRLRERE+pkSr04WTxjB+4WWtqhk9fISERGRfqfEq5PFE5Nos1AVnak3G0VERProTJg33p9O5fsq8epkgWee1yEyXC+vs+x/QCIiIqcqIiKCsrKysyb5stZSVlZGREREn65TH69OIkKDmTc2gY1VycyvL4e6MohOHuywREREhryMjAzy8/MpLS0d7FAGTEREBBkZGX26RonXCRZPSOLd1UncG4ab56XES0REpFehoaGMH682TL3RUOMJFk8YwX472n3Qm40iIiLSj5R4nWDB2ERKg1JoDgrXBHsRERHpV0q8TuDmeY0gz6RrsWwRERHpV0q8fFg8IYkdTSNpK9VQo4iIiPQfJV4+LJ6QxP620ZjKPGiuH+xwREREZJgIWOJljJlqjNnS6afKGPMlY8wIY8wKY8w+zzYxUDGcqvljEygJSsZgobposMMRERGRYSJgiZe1do+1dp61dh6QBdQBLwD3AyuttZOBlZ7PQ0pEaDDxyZ43G2vPnn4kIiIiElgDNdS4DDhgrT0M3AA86dn/JHDjAMXQJ+PGZQJQe7xwcAMRERGRYWOgEq+PAX/1/J5mrfVmM0VA2gDF0Cezp04BYP+hA4MciYiIiAwXAU+8jDFhwPXA3088Zt2CTj4XdTLG3GeMyTbGZA/G8gMzJk0E4Gj+4QF/toiIiAxPA1HxugrYZK0t9nwuNsaMAvBsS3xdZK19xFq70Fq7MCUlZQDC7Co4NIza4Diqjx2lte3sWPBTREREAmsgEq9b6RhmBHgJuMvz+13AiwMQwylpi0ohtrWczUfKBzsUERERGQYCmngZY6KB5cA/Ou3+CbDcGLMPuMzzeUiKTBxJiqli5W6fRTkRERGRPglo4mWtrbXWJllrKzvtK7PWLrPWTrbWXmatPR7IGE5HSGwa6aHVrFbiJSIiIv1Anet7EpNKEpXsLqqmoEId7EVEROT0KPHqSUwqYa21RNDIKlW9RERE5DQp8epJdCoAcxObWJVT3MvJIiIiIj1T4tWTGJd4LR9reP9AGfVNrYMckIiIiJzJlHj1JNr1D1uS1kpjSxvvHzg2yAGJiIjImUyJV088Fa8pMfVEhQVrnpeIiIicFiVePfFUvELry1g6OZlVu0twqxx10lAFf7sdjh8chABFRETkTKLEqych4RARDzUlXDotlcLKBnYXVXc9p2Aj5LwM7z08ODGKiIjIGUOJV29i0qC2hEumumHHk4YbKzyLaG97Fuq1tJCIiIh0T4lXb6JToaaE1LgIZqfH+0i8jrhtcx1seXrg4xMREZEzhhKv3sSkQI1Lti6dlsqmI+Ucr23qOF5xBBLGwZjFsP5RaGsbpEBFRERkqFPi1ZvoVKgtBeDCKSlYCx8cLOs4Xn4YEsbCok9B+SE4sHKQAhUREZGhTolXb2JSoLEKmhuYkxFPZGgw6zonXhVHIHEcTL/ezQdb/8jgxSoiIiJDmhKv3niWDaK2hNDgILLGJbLu0HG3r7kBaorcUGNIGGTdDftWQNmBwYtXREREhiwlXr3xNFGlxg03njt+BLuLqqmoa4LKPHcsYZzbZn0CgoIh+48DH6eIiIgMeUq8ehPTUfECOHdCEgDrDx3vaCWRMNZt40a5IcfNf4am2oGOVERERIY4JV698Q41et5snDsmnvCQIDfc6G0l4U28ABbdBw2VsP3vAxyoiIiIDHVKvHrjWTbIm3iFhwQzf2wC6w6VuTcag0IhdlTH+WMXQ9ps11rixOWFRERE5KymxKs3oREQHt8+1Ahw7vgkdh2tovl4LiSMgaBOf0ZjXGuJ4h1wZO3AxysiIiJDlhIvf3Rqogpugn2bhbrigx0T6zubfbOrhO19YwCDFBERkaFOiZc/OjVRBZg/NpHQYENIVV7X+V1eYdEwcpZbQFtERETEQ4mXP06oeEWGBXNOegTRLeW+Ey+A9Cw4ugXaWgcoSBERERnqlHj5IyatyxwvgMtGNQDQEDPG9zXpWdBUDcf2BTo6EREROUMENPEyxiQYY54zxuw2xuQYY5YYY0YYY1YYY/Z5tomBjKFfRKe6FhEtje27zklwfbpy6hN8X5Oe5bYabhQRERGPQFe8Hgb+aa2dBswFcoD7gZXW2snASs/noS2ma0sJgCnhbtmgtcejfV+TNBnCYuHopkBHJyIiImeIgCVexph44ELgMQBrbZO1tgK4AXjSc9qTwI2BiqHfRHftXg8QXpNHI2GsyjO+rwkKgvT5qniJiIhIu0BWvMYDpcDjxpjNxpg/GGOigTRrbaHnnCIgLYAx9I8T1msEoOIINRGj2FpQSX1TNxPo07OgaIdbTBvIKaxyazyKiIjIWSmQiVcIsAD4nbV2PlDLCcOK1loL+Gzvboy5zxiTbYzJLi0t9XXKwPF2r+88wb78MCSOpbnVsjmv3Pd16VnQ1gzFO9hTVM31//MuP39zT+DjFRERkSEpkIlXPpBvrV3n+fwcLhErNsaMAvBsS3xdbK19xFq70Fq7MCUlJYBh+iGm63qNAFQcIXbkRIIMrDt43Pd1ngn2bfnZfOP5bTS32u7PFRERkWEvYImXtbYIyDPGTPXsWgbsAl4C7vLsuwt4MVAx9JvQSAiP62ii2lgN9ccJS8pkxug4t26jL3GjIXYUB7asYWteBXMz4tlXUqPhRhERkbNUoN9q/DzwlDFmGzAP+BHwE2C5MWYfcJnn89AX3amJasURt00Yx7njk9h8pILGFt/zvOpS5hJSuIll01K5/6rpAGw83M3QpFfxTsjb0F+Ri4iIyBAR0MTLWrvFM1w4x1p7o7W23FpbZq1dZq2dbK29zFp7Zoy9xXRaNqhL4jWCxpY2/rz2MG7KWgdrLS8dG8V4U8gPr8pg3pgEQoIM2b0lXq99HV75UgC+hIiIiAwmda73V3QK1BS7372JV+I4LpySwgWTkvnBqznc9fgGCivr2y95NjuPl46NAmBkTQ6RYcHMSo8nO7eHXLO1BY5u7jqfTERERIYFJV7+ikntSIbKD0NoFEQlEREazJ/uWcT3b5jJhkPHufwXa3h2Qx7FVQ384NUcwsZ07WC/cFwiW/Mrux2apHQ3NNdBXRlYny98ioiIyBlKiZe/olOhoQJamqDisFsc27jmqUFBhjuWZPLGly5k+ug4vvH8Nq745RqaWtr47keWQPIUKHAd7BdmjqCppY0dBZW+n+NtuGpb3TJFIiIiMmwo8fKXt6VEbakbakwYd9IpY5OieOZTi/nudTNobmnjm1dOY3xytGsrkZ8N1pI1zi1NmZ3bzTyvzp3u67p5W1JERETOSEq8/BXTadkgb8XLh6Agw93nj2fbA1dwzwXj3c70LHddVQEpseGMT45mQ7eJ1yYIDne/K/ESEREZVpR4+cu7XmPZATcE2E3i5RUc1GkNx/QFbttpntfGw8dPeguSploo2QXjl7rPSrxERESGFSVe/orxdM/P9/TXSjx5qLFbabMgOKwj8cpMpLyumQOltV3PK9zm5nZNvsJ9VuIlIiIyrCjx8pe34uVNvHqpeHUREg4jZ3eZYA+w8fAJbSUKst128mVuq8RLRERkWFHi5a+wKAiLgaLt7rOPyfU9Ss9y/bnaWpmQHM2I6LCT53kVbIT4sZA43s3zqj3WP7GLiIjIkKDEqy+iU6C1CcJiITKxb9emZ0FTDRzbizGGrHGJJy8dVLDRzQczBqKSoO7MaOovIiIi/lHi1RcxaW7bqYeX39JPbqR66FgtpdWNbn+Np02F97zoJA01ioiIDDNKvPrCO8G+LxPrvUZMdFWy/W8BPuZ5HXXzv9oTryglXiIiIsONEq++8E6w78vEeq+gIJh3G+S8DNVFzEqPIzwkqKORasFGMEEwaq77HJUEdZrjJSIiMpwo8eoLbxPVvk6s91p4D7S1wsYnCA8JZm5GAhsOd0q8UqZDeIz7rIqXiIjIsKPEqy+iPUONp1LxAkiaCJOXQ/YfoaWJhZmJ7CyopL6xpWNivVdUsmvU2tp8+nGLiIjIkKDEqy+SJ7vhwNTpp36PRfdBTTHsfpmFmYm0tFlycrZBfXnH/C6AKDcHjPpulhYSERGRM44Sr77IXApf3ukqV6dq4jLXp2v9o2SNdclV6e733LEuiVeS22q4UUREZNhQ4tUXxkDc6NO7R1AQnHMvHFlLfGUO00fFUXtwPTYksmslzZt4qYmqiIjIsKHEazDMvw1CImHDo3xx2WTGNuymJHoKBId2nBOd7LaqeImIiAwbSrwGQ2QizLkFtv2dKzKDmBOcy5uV6Ryraew4R0ONIiIiw44Sr8Gy6FPQUo954/8RZpvY2DKBn/1zd8fxSM/kei0bJCIiMmwo8RosI2fD2PNg+7MATF1wMc9m57Mlr8IdDwmD8Dg1URURERlGlHgNpkWfctvIRO64+mJSY8P5zos7aGuzbn9vTVTbWlURExEROYMo8RpM06+D2NEw5lxiIkL51tXT2ZZfybPZee54b4nXlqfgoVl681FEROQMEdDEyxiTa4zZbozZYozJ9uwbYYxZYYzZ59kmBjKGIS04FO75J1z/awBumDeaczIT+dkbe6isa+498SraDs21sOe1AQpYRERETsdAVLwusdbOs9Yu9Hy+H1hprZ0MrPR8PnsljmtfA9IYwwPXz6SiromH3trrEq/aHhKviiNuu/vVAQhURERETtdgDDXeADzp+f1J4MZBiGHImjk6npvmZ/D37DyaI0b0XPEqP+y2B1ZDY/XABCgiIiKnLNCJlwXeNMZsNMbc59mXZq0t9PxeBKQFOIYzzocWpFPb1MqB2nBoqYemupNPstZVvEbOgdZG2P/WwAcqIiIifRLoxOsCa+0C4Crg340xF3Y+aK21uOTsJMaY+4wx2caY7NLS0gCHObQsnpBEckw42SXG7fBV9ao77uZ3zbkFopIh55WBDVJERET6LKCJl7W2wLMtAV4AFgHFxphRAJ5tSTfXPmKtXWitXZiSkhLIMIec4CDDNbNH8r63Luirl1dFrtsmjoepV8G+N6GlaaBCFBERkVMQsMTLGBNtjIn1/g5cDuwAXgLu8px2F/BioGI4k107dzRFLTHug6+Kl3difeI415aisQoOrRm4AEVERKTPQgJ47zTgBWOM9zlPW2v/aYzZADxrjPkkcBi4JYAxnLGyxiYSEpMMzfhukupNvBLGQtJkCIuB3S/D5MsGNE4RERHxX8ASL2vtQWCuj/1lwLJAPXe4CAoynDtrEmyG+ooSIk88ofwwRCRARLz7PHk57H4NrvkFBAUPdLgiIiLiB3WuH8Iumz+VVms4dOTIyQcrjrhql9e0a6G2BPI3DFyAIiIi0idKvIawOWMSqTJxFBbmn3yw4nDXxGvy5RAUCjkvD1yAIiIi0idKvIYwYwytkSNoqi6lrKax44C3h1diZse+iDiYcBHsfsUdFxERkSFHidcQF5mQSiLVvL6jqGNnbSm0NHSteIEbbizPheKdAxqjiIiI+EeJ1xAXlZBKWkgtL2892rHTu1RQwriuJ0+7BjCu6iUiIiJDjhKvIc5EJZEaXMP63OMUVzW4nRXexOuEildMKow5V13sRUREhiglXkNdVDJRLZVg23h1m6eVfXeJF8D0a6F4e0dVTERERIYMJV5DXVQSxrayMC2El7d5hhsrjkBUEoTHnHz+pOVuqy72IiIiQ45fiZcxZqIxJtzz+8XGmC8YYxICG5oALsECrp8SxuYjFZRUNZzcw6uzlKkQnQK57wxgkCIiIuIPfytezwOtxphJwCPAGODpgEUlHTyJ19LRBoBVu0vcMGJ3iZcxkHkBHHpHbSVERESGGH8TrzZrbQtwE/Bra+3XgVGBC0vaRbvEa1xkPekJkazKKYLKvJPfaOwscylUH4XjBwcoSBEREfGHv4lXszHmVuAuwPvKXGhgQpIuPBUvU3ecS6elsnv/fmht6r7iBTD+QrfVcKOIiMiQ4m/idTewBPihtfaQMWY88OfAhSXtPIkXdWUsm55KSounkWrnrvUnSpoEMSPdcKOIiIgMGSH+nGSt3QV8AcAYkwjEWmt/GsjAxCM0CkIioK6MxROSeC20zO3vqeLlneeV65nnZczAxCoiIiI98vetxn8ZY+KMMSOATcCjxphfBDY0AVzSFJUMdWVEhAZzXlItADY+o+frxi+FmmI4ts//Z1kLz90Du148jYBFRESkO/4ONcZba6uADwF/staeC1wWuLCki6gRUOcqXXNiKim18eQca+35msylbpvbh35e5bmw43nY+X+nFqeIiIj0yN/EK8QYMwq4hY7J9TJQopLaE68MU0q+TWHV7uKerxkxAeLSIfdd/5/jnYzflyqZiIiI+M3fxOtB4A3ggLV2gzFmAqD/Og+UTolXWHUe1ZHpvJVT0vM17fO83vW/n5d3Mn7ZPmjrpaImIiIifeZX4mWt/bu1do619jOez//LsJEAACAASURBVAettR8ObGjSLjoZastcMlSZT1TKeLbmV1Ba3djzdZlLobYUSnf3/gxrXZIWHA4tDa5XmIiIiPQrfyfXZxhjXjDGlHh+njfG9DK7W/pNVBI0VrqlgtpaGJ05BWth9Z5eql7jPfO8/Gkrcfyga7o68yb3uXTv6cUsIiIiJ/F3qPFx4CVgtOfnZc8+GQhRI9z26GYARmVOY1R8BKt6G25MzIT4sf5NsPcuqn3OJ932mBIvERGR/uZv4pVirX3cWtvi+XkCSAlgXNKZt4mqJ/EyCeO4dFoq7+wrpbGlt7cbL8DmvsdDb+5m9e4SmlvbfJ+X+w7EjoKMc1z7imN7+vELiIiICPifeJUZY243xgR7fm4HygIZmHQSley2nsSL+AyWTU+ltqmVdQeP93zt+KWY+uO8sXo1dz+xgcU/WskDL+1ka14F1jvp3lo3HJm51E3KT56iNxtFREQCwN/E6x5cK4kioBC4GfiEPxd6ErXNxphXPJ/HG2PWGWP2G2P+ZowJO4W4zy7tFa8trioVGsF5E5OJCA1iZU7PbSV2hs8F4N/HH+XROxeyeEIST68/wg2/eY9lv3ib/SXVLsmqLXFvQQKkTIFSVbxERET6m79vNR621l5vrU2x1qZaa28E/H2r8YtATqfPPwUestZOAsqBT/Yp4rORN/Fqqm5fKigiNJgLJiXzVk4JLd0MHza3tvHVN8rIJ40ro/exfEYav7ltAdnfvoyffng2+eX1PLM+r2MOmHcyfvIUqD/u3qQUERGRfuNvxcuXr/R2gufNx2uAP3g+G+BS4DnPKU8CN55GDGcH7+R6gIRx7b/eND+Dgop6vvjMFp9ztx5Zc5DdRdWY8UsJzVsLbe6cuIhQPnrOWGanx7PpSLkbZozLgMTx7sLkqW6reV4iIiL96nQSL39WXv4l8A3AmxUkARXW2hbP53wg/TRiODsEh0JEvPu90+LY18wZxf+7ejqvbi/ks09t6jLR/kBpDQ+v3Mc1s0eRPu9yaKiAws1dbrtgbAI7Ciqxue+6apd3Me3kyW6rNxtFRET61ekkXj22QzfGXAuUWGs3nsrNjTH3GWOyjTHZpaWlpxTgsOIdbuyUeAF86sIJPHjDTFbsKua+P22kobmVtjbLf/xjOxEhQXz3+hkw6TIIj4O3HujSxX7B2ETGteVh6o51zO8CiB8DIZHq5SUiItLPQno6aIypxneCZYDIXu59PnC9MeZqIAKIAx4GEowxIZ6qVwZQ4Otia+0jwCMACxcu9HPNm2EsKsk1OU0cd9KhO5dkEh4SxP3/2M49T2zg0mmprD90nJ99eA6psRFABFz2ALz6Fdj6V5j3cQDmj01kSdBOdxPvotoAQUGQPEkVLxERkX7WY8XLWhtrrY3z8RNrre0xabPW/oe1NsNamwl8DFhlrb0NWI17KxLgLuDFfvgew183FS+vj54zll/cMpcPDpbxg1dzWDIhiY8s7LS4QNbdMOZceONbUHsMgJHxEVwSvodjISNPTuiSpyrxEhER6WenM9R4qr4JfMUYsx835+uxQYjhzBOVBCbITYLvxk3zM/ifjy9gdno8P/7QbIzpNA0vKAiuexgaa1zyBdDWxjns4gM74+SbJU9xSxQ11/fzFxERETl79Vi16i/W2n8B//L8fhBYNBDPHVamXQth0RDSc9uzq2eP4urZo3wfTJ0OF3wZ1vwM5n4MolOJaatiVdMUzqlqIC0uouPclCmAhbL9MHJ2/30PERGRs9hgVLzkVEy7Gq7+r9O/z9KvQtIkeOXLsO9NANa2zWTzkfKu5yVPcVs1UhUREek3SrzONqERbsixPBf+9WNs4njKglPYdKSi63kjJrqhTS0dJCIi0m+UeJ2NMi+A+XdAaxNm/FJmpsedXPEKjXDNWtVEVUREpN8o8TpbLX8Qxl0Acz7KgrGJbMuvpKnlhO73KVNV8RIREelHSrzOVlEj4O5XIfMC5o9NoLGljZzCqq7nJE92iVdbq+97iIiISJ8o8RIWjE0E8DHBfiq0Nrq2EgPgeG3TgDxHRERksCjxEkbFR5AWF37yBHvvm40BbqRa29jCN57byoLvr+CDg2UBfZaIiMhgUuIlGGNYMDaRzXknVrxOf7Hs1jZLSVVDt8d3FFRy3a/f5e8b8zEG3j+gxEtERIYvJV4CuOHGvOP1lFY3duyMGgHRKafVy+sP7xxk0Y9WcsVDa/jlW3vZV1wNQFub5dE1B7npt+9R19TK0/cuZmpaLFvzKnq5o4iIyJlrQDrXy9A3f2wCAJuOlHPFzJEdB5JP/c1Gay1/XX+EiSnRxEeF8vDKffzyrX1MSo0hITKU7MPlXD4jjZ9+eA6J0WHMzUjgjV1FWGu7LnckIiIyTKjiJQDMSo8nNNiw+aR5XpNdLy9r+3zPjYfLyS2r4zMXT+LZTy9h3beW8f0bZpISE05uWS0/vGkWv78ji8RotwzS3DEJVNQ1c+R4XX98JRERkSFHFS8BICI0mBmj49l04puNKVOhvhzqyiA6uU/3fG5jPlFhwVw1y1XQUmMjuGNJJncsyfR5/twx8QBsza9kXFJ0n7+DiIjIUKeKl7SbPyaBbfkVtLR2aqTqnWDfx3ledU0tvLKtkKtnjyI63L/8fkpaLOEhQZrnJSIiw5YSL2m3YFwiDc1t7C6q7tiZPNVt+/hm4xs7i6hpbOHmrAy/rwkNDmJWenyviVdrW9+HPUVERIYCJV7Sbv4YN8H+z2sPs6Og0iU4cekQGtXnxOu5jfmMGRHJolQLOS/7fd3cjAR2HK3sqLrtegkaKtuPN7e2cfHPV3Pjb95je35lN3cREREZmpR4SbuMxEiyxiXyt+w8rv31u8z93pvc8fgGisMzacjf5vd98svreP9AGTcvGEPQ2l/D3273+83IuWPiaWhuY29xDVQXwbN3wPpH249vOHScvOP17C6q4vrfvMu3/287FXXqeC8iImcGJV7SzhjD8585j/fuv5SHPzaPG+ePprS6kTcrR9OSv4mSKv/eNnxhUwHWwocWpMOhNW6nn1WveZ6q29b8CqgudDsLNrYff2NnERGhQaz5+iV84rxMnl53hEv/+22e3ZBHm4YgRURkiNNbjXKS9IRI0uelc8O8dAAKVu8l5u23+NaTL/Pzz3yEsJDu83VrLc9tymfJhCTGRDVD4RZ3YPcrsPQrvT577IgoEqJC2ZpXwa0JpW5nfjZYiwXe3FXM0skppMZF8N3rZvKRrDF858UdfOP5bXzzH9sIDQ4iLDiIsJAgQoMN18wezXeum3G6fxIREZF+oYqX9Cp95gUAhBRu4oGXd/Z47obccg6X1blJ9Uc+ANsGEy5xVauqo70+yxjDnIwEtuRVQG2J21lbAlUFbC+opLCyoUuD1xmj4/j7vy3hNx9fwOcvmcQ954/nloVjuGb2KFJjI3h6/WEamltP/cuLiIj0IyVe0rvkyRAWy21jjvH0uiM8te5wt6c+tzGP6LBgrpo90g0zBofD8u+5g7tf9etx8zLi2VdSQ1NlUcfOgo28ubOY4CDDsmmpXc43xnDNnFF85fKp3H/VNL5z3Qy+f+Msvrx8Mg3NbWzIPd7nrywiIhIISrykd0HBMHoeC4IPcvHUFB54aafPZKauqYVXPb27osJCIPcdGLMIRs2FpMl+z/OaOyaB1jbL8ZICCImEoFAo2MgbO4tYlDmivdN9bxZPSCIsOIg1e0v79HVFREQCRYmX+Cc9C1O0nYdvnkFGYhSf+csm9hVXs6eomnf2lfLcxnwefHkXtU2tfGThGNftvnAbZLphSqZfC7nvQl3v1ac5GW6CfU3ZUYgdCSNnU5+7gX0lNVw+M83vkKPCQjhnfCJr9h47pa8sIiLS3zS5XvyTngVtzcRX7uHRO7O48Tfvs/yhNSedNn9sAudkJsKe1wELmUvdgWnXwbsPwd43YN6tPT4qJTac9IRIWquKITkVRs4heNPTBNHG5Z0X8PbDhZNT+PHruymqbGBkfESfru13RTugaPvJ+0fPh9RpAx+PiIgMOCVe4p/0BW5bsJFJ52bxt08vZu2BMtLiIjw/4aTGRhAZFuzOy30HQiIgY6H7PHo+xI52bzf2kniB6+cVtv8YxMyD9CzCNjzKFWlVpCdE9insC6e4xGvN3lJuOWdMn67td3//BJT56Gc2YiJ8fiMYM+AhiYjIwApY4mWMiQDWAOGe5zxnrf2uMWY88AyQBGwE7rDWqgPmUBeXDjFp7T21Zo6OZ+bo+O7PP/QOjDkXQsLd56AgmHYNbP4LNNVBWFSPj5ubkUD83nIawpOpTZhFEvChtOI+hz1tZCypseG8vW+QE6+2Nqg4DFl3w/lf7Ni/6//grQegJAfS1PZCRGS4C+Qcr0bgUmvtXGAecKUxZjHwU+Aha+0koBz4ZABjkP5ijBtuPLqp93PrjkPx9o5hRq/p10JLPRxY2est5o6OZoSpobAlln8WxVBlI1kYcugUwjYsnZzCu/uODe4aj7Wl0NoEaTNhxPiOn7m3AsZVAkVEZNgLWOJlnRrPx1DPjwUuBZ7z7H8SuDFQMUg/S1/g1mxs6GWNxNx33Xb8CYnXuPMhIgFyek8yZic2A3CwPoo3dpWyL3gyCeX+L1vU2UVTU6isb2Zbfs+Lb5+K6oZmNh4+TmNLL73CqvLdNi696/7YkZBxTp/WsxQRkTNXQN9qNMYEG2O2ACXACuAAUGGtbfGckg+kd3PtfcaYbGNMdmmp2gEMCelZbnt0c8/n5b7rFtYevaDr/uBQmHoV7H0dWpt7vEV0s3v7cVNZCGsPHKMhbT6meAc0N/Q57KWTkjGGfnm7saW1jU1HyvnVyn185H/fZ/6DK/jw79ZyzxMbem7UWulJvOJ9/M99+rVQtA3Ku++PJiIiw0NAEy9rbau1dh6QASwC/H51y1r7iLV2obV2YUpKSsBilD4YPd9tO62d6FPuOzB2MYT46Lc17RpXMfNWxbpT45LttcUhNLda0qafB20tvt8K7EVidBhz0uNZs+/0EviNh49z7o9W8qHfvs9Db+2lobmN+y6cwNevmMp7+8v47FObaGpp831xZYHbxvuYZzbtWrf1s8GsiIicuQbkrUZrbYUxZjWwBEgwxoR4ql4ZQMFAxCD9IDIRkiZBQQ/zvGqPQckumH2z7+MTl7mmqLtfgYmXdH+fGjeR/hjxJMeEM2HufFiFS/rGnNPn0C+cksJvVu+nsq6Z+KjQPl+/p6iaux/fwIjoMB64fibnT0pmRKdGrvGRoXz7/3bwpb9t5lcfm09I8An/n6bK0ww2MvHkmydNhNQZ7m+y5LN9jk1ERM4cAat4GWNSjDEJnt8jgeVADrAa8P5X+S7gxUDFIAGQntVzxSv3HbfNvND38bAomLTMVXfauqkOQfs6jcdsPMtnpBEUP9q1o+it2tZZ3fH2ytmFU1Jos/Degb4PN+aX13HnH9cRERrMnz95LtfNHd0l6QK4ffE4vn3NdF7bXsQ3nttG24kT+SvzID6j+5YR066FI2td4jqYqot6n8OHm9tWUFF/6s8p3QN2EF92EBEZJIEcahwFrDbGbAM2ACusta8A3wS+YozZj2sp8VgAY5D+lp4F1YXdL3h96B0Ii4HR87q/x9Sr3D2O7en+nJpSbGg0t5w3jU9ekOl59oK+JV4vfwGe/ggA88YkEBse0uflg8pqGrnzsfXUN7Xyp08uYsyI7ttg3Lt0Al9dPoV/bC7gP1/cge2cWFQW+J7f5TX9Wreg+J7XKK5qCMiLAH75803w+v29nvbN57dx/a/f7f2lAl8Ovg2/WQQHVp1CgCIiZ7aADTVaa7cB833sP4ib7yVnIu8E+4KNEDf65OO578LYJW4ifXdSprvt8UOQOt33ObUlmJgUHrh+Ztdn737FVbKiRvQe67H9UJoDtWWERidx/qRk1uwtxVqL8aNZaU1jC3c/sYGCinqeuvdcpo2M6/Waz106ifrmVn77rwOMTojk3y+Z5A5U5sPky7q/cOQciB8LOa/wvZyZrNpdwtr7l/m9LmW/aGmC0t1Az3+b/PI6/rmjiDYLK3NKuHr2qL49Z+cLbntglat+ioicRbRWo/RN2qz2RatPUl3sqlje9Rm7kzDWbSuOdH9OTQlEp3bd5+9ble3xeKpyh91E/gunpHC0soEDpTXdXtLQ3MqRsjqyc4/zb3/eyM6jVfz2tgUszPQj0cP1Dfv6FVO5ZvYoHl65j8NltS6hqSmGuIyeLoTp12IPrmb9niM0NLfxt+w8/75nf6nMc1W3sv3Q1n0l66l17t9tRHQYz2/M79sz2lxVD4BDJy85JSIy3GnJIOmb0AgYOct34uWd33Vi/64TRSe7dhM9JV61pTBiQtd93uHLgk29V0qa6jrmKuW+CzNu4MIpyQC8vfcYk1JjKaluYO2BMt7dd4yt+RUUVzVSWe9tc2EZxXF++pFLWTa9l4W5qwpdV/8g9/9jjDF857oZvL23lO+8uJMnbkzBYN0cr55MuxbzwW85t2UT70ct5U/v53LvBeNPnqgfKMcPum1ro+uyf+LfH5eYPrP+CMtnpDE+OYZH3zlIaXUjKbHh/j0jf4NLQlOmuTdU68t9v3AgIjJMqeIlfZeeBQWbu06OL90D//qx+4/oyLk9X2+Mq3pV9NC3qqYEok9oIxIRD8lT/JvnVV3ofZibdwZkJEYxISWax987xBUPrWHRD1fyxWe28OauYsYkRnH93NF87fIp/OzmOfzz4gLej/wiN2c29vycmlJ4eC5s+1uX3WlxEXxl+RTe3lvK+q2eFhg9zfECGLuYmpAErg7N5sEbZnG0soEVu/q+TNIp8yZeAMd8rCkJvLT1KOV1zdx1XiY3Z6XT2mZ5cUsfXkze/bKrmF72AGDh8PunE7GIyBlHiZf0XXoWNFV3LPic8zI8eqmrMH30KQj2o5CaMK77xKu1BerKICb15GPetyp7eyPOO/l/wkVunpfn7cbr5oxur9B888ppvPy5C9j0n8t57BPn8P0bZ/G5SydzS1YG0w79GWPbeh/WLNzqKkQ+llK6c8k4po+K47X3st0OXz28OrEmiFVtWVwatIWrp48gIzGSx9/P7fn5/aj66B4arfu327vj5OTWWsuT7+cyJS2GJROSmJQay9yMeJ7f5GfiZa1btWD8hTDxUreIuicpFhE5Wyjxkr7zzrXKWw9vfQ/+drsbOrrvbcg83797JIyF8m6GGuuOAfbkipf32bUlHZ3gu+OteM35qNt6hkG/vHwKOQ9eyV/uPZfPXDyR2RnxBAedMJn8yAdurUmA4h09P8d7/Njekw6FBAfxgxtnEV1f5HacuFzQCXYereKFhgVE2jqCD7/DXUsyWX/oODuP9t7eoT9UFOxhv02nnDhytmd3GnZ1Nh4uZ+fRKu5cktn+csKHszLIKazyL8aSHCg/5N7gDAmHMYs6hqdFRM4SSryk75ImQ1gsvP4NePcXkPUJuPu13ofSOksYC42VUO+jbUKN6+FFjI+5VemeZYh6G270VrymXOnaW3TqlB90YqJ1ovWPuGHNEROheGfP53qPdzM0lzUukaVpjZTbGPYc77n1wopdxbxvZ2JDoyHnZW45ZwyRocE88V5uzzGcqO543873CK7IpSwsndCR08lozeOHr+7qcvzJtYeJjQjhpvkd/87XzRlNaLDh+Y1+VL12vwIYmHqN+5x5oUtcTzFeEZEzkRIv6bugIFetaGuB634F1z3sKhh9kTjObX1NsK/1Jl4+hhq9b1UWbun5/tVFLuGKGuHaW/hbWakqhJyXYP4dkLHQ/8SrqgAaq32ekpVQS7FJ5tv/t/3kxqqdrNxdzOyxaZgpl8PefxIfEcKHs9J5cetRymp6mWvmVZIDP5sA+1b4d75HZU09yc2FhKVOIiZ9OjNCi3g2O6+971lxVQOvby/kloVjiA7vGEpOjA5j2bQ0XtxSQHNrDw1xwQ1Jj1kEsZ6E2vsSRm/LR4mIDCNKvOTU3Pg7+NwGyLrr1K5vbynhY56XZz6Wz6HGkHDPMGVuz/evPgqxnv5S45e6ocDqot7j2viEa6Ww8B5Im+kSqu4qMi2Nrn1G8hT3uZuqV1htIbGpmWzILee5Tb6HSAsr69lRUMVlM9JgwiXuzb9je7lrSSZNLW38dX0Pb4B2udE2wMLa3/h3vsf6rdsIM62MmjATkqcS2VpFVlIL//GP7dQ0tvDUuiO0Wssdi8eddO3NWRmU1Tbx9p4emtOWH3YLgXvXpQS3iHpolIYbReSsosRLTk1sGiRmnvr1CadY8QJP4tXDG5HgKldxnsQr08/KSksTbHwcJi936yemeZq3dlf1OrbXVf1mfqjjsy+VeYweN5mscYn86LUcjvmoXr2V477zZdPTOipBh9YwOS2WpZOT+fMHh3uvKEHHm4kHV0NpN/H4sHvXVgDGTJjZnkj+6MIIjlbW84NXdvH0uiNcPCWFzOTok669aGoKSdFhPNdTTy/vAuDTOyVeIWFuMXVNsBeRs4gSLxkckYlunpivxKumxC0oHRbj+9rEcT33AAM3uT7W01l/1FwIj+u9spLzkqs0LbrPfU6b7bbdJV7e/dOvg6AQ34lXYzU0VGLi0/nJh2ZT29jC91/ZddJpb+0qJjMpiokp0ZA43jVb9cR79/mZFFc18s8dflTsjh+EyBEQHAYb/tD7+UBTSxsV+W75pqDkiZDiEq+pwUe5+7zxPLMhj2M1jdx1XqbP60ODg7hhXjordxdRXlnl+yG7X4HUmSf3Bstc2uWtUxGR4U6JlwwOby8vX5WrmhKISel+QemEse7Nx8ZuOtC3tbnEy1vxCgqGcef1XllZ/6hLeiZ6mrPGpEJUcvdvNhbvgOBw90Zn4njXy+xElZ5J5/FjmJwWy2cunsSLW46yek9Jx9dtbGHtgTIum57m3hY0xnX/z30X2tq4eEoq45KieOzdQz3OEQNc4jVqDnbGjbRtfooVm/fz0Iq9fPGZzewp8j0H7YODZYxsPUprcATEjHRJX2gUHNvH166YwjhPQnjhZB9Dvx4fzkrn8+ZZYn41Gbb9vevB2mNuAfDO1S4vbzXysOZ5icjZQYmXDJ7uKle1PpYL6sw7TFnZzZI6dcfcEGBsp7UkM5fC8QPdL+5duA3yPoBz7m3vQI8xbrixu8SraAekTnN9y1Km+p7jVeUZfvO0kvj3SyYyMSWab7+wg9rGFgDe2VtKU2ubm9/lNX6p62VWupugIMOnL5zIlrwKvvb3rbT0MOTYcuwAK4qiuW3bHIKaa3j7uf/hV6v28fr2Ir7+3FZafSRuK3YVMyG4BDNivPvuQUGQNAlK9xAVFsJLn7uAZz+9pMe3QWcGHeHfQ16isS0Y/nGvW2i71dOOYs9rbimiaT4Sr9HzXGVTw40icpZQ4iWDx9u9/sRmqDWlvltJtF/Xw/ww6Eiu4jot3tw+b6qb/8BveNQNb86/rev+kbPdm4K+1i4s3tkxHJk82SV2rV17X3VUvNxyQeEhwfzkw3MoqKjnv990Q5Nv5ZQQHxnKwnGdls5pn5fm4r110Ri+unwK/9hcwGef2kRD88nxvPzBTkIay9nVmMyk+RdTFjeDb6e8w64HruC/PjKHbfmVPL2ua4XRWstbOcVMDyslKGlix4HkKe2JZHxkKEkxPby12tYKL32BprAELqr/ObvG3g7rfgd/usFVL3Necf/WI2effG1waK9vnTY0t7I1r4Jn1h/huy/u4GOPrOWpdb3M8RuCsnOPsyFXrTOkfzQ0t/LASzsprKwf7FCkj7RWowyehHHQVOPW64vqtAh1bQmMOaeH6zxvRHY3wd7bPLVzxSttNkQkuP/Az/1o1/PrjrvhsTm3nLxuYNpMaGlwQ3jJkzv215S4OL0T8JOnuCpbeW7X8yrzwQR1vGEJnJM5gtvOHcsT7x/i2rmjWLW7mEunpXZdkzFxnPueh9bAuZ/GGMPnl00mLjKU7760k3ue2MCjdy4kOjyE5tY2fvhqDhvXruK6cLj7ukuJmzcbNn8OXvwsFLzH9XMv5NnsPH72xh6umDWS1NgIwDVtLaqsIy2qEEZc3/H8lKmw4zm35mVYVDf/EB7rH4Wjmwj/0GPM3zyO6/ck8MayLCau/Rb8/kL39z3n3u6HjscvhRUr+O3L77KzKoqaxhZqG1uo8fwcrajHW6iLDgsmKjyEH72aw9WzRpEYHdZzbH6obWyhor6Z9ITI075Xd/KO13HnH9cTZGDVVy8mNS4iYM+Ss8N7+4/xxPu5JESF8qXLpgx2ONIHqnjJ4PHVUqKt1Q2x9TTUGJPqlpvpbskhXxWvoCDPvKkTKitNtfDyF6GlHhZ96uR7eROrou1d93uHH9sTr6lue+I8r6oCl3SdsIzSN6+aRkpsOJ96Mpvyumb3NuOJMi+Ew+91WRPzrvMy+e+PzGXdoePc9od17C+p5uOPfsAT7+dy93R3XtxoTyyzPuQm2q9/BGMM379hFo3Nbfzo1Zz2+63YVcxoU05wW1PXie/e5LHMd4uMdhV5sPJBmLScoNkf5hcfnceYEVF87IOxlH3sFTfJv7XR9/yu9u/pqns5a18np7CK8romQoODyEiMYuG4RD536WT+9/YFvP31i9n+wBU8de+51DW38vs1B7u/Zw9aWtvYeLicX63cxy2/X8u8B9/k/J+s4nf/OnBK9+tNW5vlm89v48fmf/iZfYgfvZbT+0UivXhn3zHAJWByZlHFSwZP58rV6Pnu97oyNx+ou1YS0GmR7W6GGqsLXZXpxOQt8wL3dl1FHiSMgbIDbrmjkhy47Hu+h8KSp4IJdsOKsz7Usd/7RmPaLM95k9z2xDcbK/N8LhUUFxHK966fxb/9ZSOhwYYLpySf/OzMC2DLX6BkZ5fYPpyVQUxECJ9/ejOX/WINEaFBPPyxedxQuQsO0tHmIzQSFtwJ7/8KKvKYkDKGf7toAr9atZ9bFo7hvEnJrNhVzPKRlmxggAAAIABJREFUtVDOCYmXJ3k7ts+9FeqLtfDa1wAL1/w3GENcRCj/e3sWN/32Pe5b0cRfP7masMJsN5zYjdWVaWTZSO4clcc5n7+42/O8pqTFcv3c0Tz5fi73Lh1Pck/DoCd4cUsB335hB9WNLRgDs0bH88kLJnC4rJaf/nM3NY3/v73zDovqTPvw/dJBFEQQFAsIKCIWRNTYS9SY6Kb33kw2m8TsJtlks6nfbno2yabH9L4pJtbEmKixxtg7KhZEESkiKKLSzvfHMweG4QwMHfW9r4sL5sxpUw7zm6f8nmIeGN+jfCRSQ/DlqjRW7D7Me4E78SrK474N+7hqYBcGd2vXYMfQnH0sTZFO4PVpeRScKsHfW3+cny7oiJem+SiPeNkJKHNckJV5quO2TiNeGVIj5jis275uaud8eG+0iLTrpsOw+6z35ekjaURHS4nMrRLJamX78PQJkNtVhFd6eX2XI+fFh3HFgE5cktCJ1j6eVVeopi5tQq8wPr45ibGx7fnhrqFc2C9c0qFtwkVwmQy4RX6v/QiAu0ZH0yXIj0dnbmFvznG2ZRxlTHtbd6i98GoXJeLVqlPTZNtM2DkPRv+zYhIB0COsNS9c1oe1+47wrwUZ0H2C0zRj5tGT3D99G9s840k0apiLace9Y2M4VVLKO7WIUqVkHuOh6ZuIau/Pm9f0Z+2j45h9zzAenhjLG9f056qkzry5aDdPztpac/eoi+zPLeTZH5M5t5sfrU5m4ll2inPbHOCJmVtd82XTaCw4mHeC3dnHGRPbnpIygz/2HG7uU9LUAi28NM2Hb6AIlkrCK1N+VxfxAqkPcxrxOlippqqc9nGSevvtWfjyChFvU36D6LHVHyu0l4Xw2lKRZjQJjqksvAxDUo3VzLB84bK+PH9ZH+s7AzqJTYWTwvMh0cF8cFMSPTu0kQW5e6r6ZLXtCt0niiN/8Ul8PN35vwt7sSf7OFM+XQNAH79cSQnaR+Y8vCVy5swU9kSezOrs0BcG3Vnl7kl9OnLHiG58tnIf366x7j4tLTP469cbKCwqIXrgRNyq6zp1ICrEn4sTOvHZyn1kHj1Z4/oni0u5+8v1+Ht7MO2GRC7o04Egu/owdzfFs5f05vbhkXzy+z4e+K767lFXMAyDh7/fBMDTIyqicn+LOcSOzGN8+vvp1yDQUBzMO9Fg4vZsZJktzfjXc7vj7eHGMp1uPK3QwkvTvAR2rRy5Om6OC6pJeHWRovyTFoadRzOgTceqy93cJIqUlyaF9LfMd819P7QX5KdVDPQuLZZIUBXh1UPc4s0uzcLDUpjfxjri5RKRw211XtUP2AZswiuy6vKBt8u5JM8GYFSP9pzfO4yUrAKiQloReHK/PA9u7g6Pp7tz4bX4BXmtJr9WNbJo48EJPRgS1Y5HftjMEzO3cOBIYaX7312ymxW7D/Pk5F4E9z5XFtbCVmLq2BhKygyXarP+b842dmQe4z9X9CtvLCgn5Vf4Tyyq8DCPnN+Tv43rzvfrpHt0V9YxDMeuW3uKT8AbA2H951Xu+mrVfpbvOswjF/Qk9JRNfPoEEHV8PSO7h/DKLzvJshKN6z+HNwdBaUmNj6sKxw/Dy72kKaOFsnLPYYY+v5AHvttYo/haviuH/bmF1a5zNrIkJZvQNt7Eh7dhYGSQrvM6zdDCS9O8ONZqFdQwLsikuiHbziJeAOP+D676Ci5+t+ZuPROzjivL5jifkwKlRRVWEibB3aHoWMVMyHybh5eTVKNLRAyHk/lVi/sdOXVMuiwdI14AkSPl+dg2o3zR45N60drHgwv6dITcvdbbBXeHw7uqir5Tx2D9ZxB/mfhwOcHD3Y23ru3PRf3C+eKPNEa9+BsPfLuRXVkFrEs7wn/m7+SCPh24Mqlz5a5TF+nSzo/LEzvx5R9pHMxz3lI/d1MGX/6Rxh0juzGyu0UKe9krknJOW4lSinvHxvD4pDh+Sc7k3JeXMOS5hTz47UZmbTxI7vGiyttumS7zOnctqLT4wJFCnp67jSFR7bhmYBcRsMod+lyJ2r+ap86PoqikjGd/2l71fNZ+DNnb5ae27Fkk3nE7f679tk1AaZnBU7O34e3hxvfr0nnmx2Snwvat33Zx7ft/8ND0TU18li2bsjKD5btyGBYdglKKodHB7MwscCnyq2kZ6Go8TfMS2BV2L5QokVIiHjx8wLt1DdvZ1YeFxVcsLyoUodLGifBqG1H7GZPm/jO3igN+eWG9Q8QrxByWvUOOXy68nKcaa8S+Lq0akUPuXvltJaDc3CD2Alj/Rbk9RFiAD8seGkMrTzdYtQciR1TdLri7CMwjqVLzZbLpazh1FAbdUePpB/p58eLlffnruO68t3QPX61KY/q6A/h7eRDWxodnLu5d4dbfdWitB2bfPSaa6esO8OaiXTx9cdXmiP25hTw8fRP9OgfywPgeVXeQubXCNT99bXn35S3DIhnfK5SlKTksTcnm562H+NY2izKuQxuGRLVjaFQ7Rqx8F3fbfnIKTrE+LY/1aUf4eeshDOD5S/vI48vZIa9N1BhYNY2IE9uYMqIbbyzaxZVJnYnr2IajJ4opzDlA9wOrK87H/r3tCubzl762dts1EV+v3k9yxlHeuCaB1XtzeX/ZXoL8vbhrVHT5OoZh8Py8HbyzeDftW3vz+57DHMo/SViAtuAAsYA5UljM8BhpyBkWLb+X78rhkv71+JKnaTJ0xEvTvLTtCsWFMlYGxDy1VXvnnk8m5SaqDnUyVh5e9aV1B/H3Mi0kMreAm2dlvy4oHy5d7mB/tGJcUJ1p00Fc5GtKwZnDsa2EF4hrfMkJEbk2Anw98TiRLc9/W4sUpePjARHIq96TLtTwRJcfRsdAX56Y3IvlD43h7tHRhAX48Po1CQT42jUVRA4XkZfnZCKBBZ3a+nFlUme+WbO/SkqquLSMu79aDwpevzoBT3eLf3er3hOhHxRVRax0auvH1QO78Na1iax/fDwz/jKU+8d1J8DXk09/38frn36Je+YmMt3aU5qTwtB//8jtn65h2pI9tPL24BWbtQYgz2FwdxHuyg1Sl/KX0dGEB/py1bSV9HlyPsOeX8SnH78FQJHhTt6uldU+9iPHi3jx5+1kH7Mbum6+Tw5uqFuqshHJP1HMS/N3MDAiiAt6d+CJyb34U9+OvDBvB/9bJZHr0jKDR2ds4Z3Fu7l2UBe+vH0whgGzNqY389m3HJbYuhmH2gRXXIc2BLXy0nVepxE64qVpXuwjV/4hEvHyr6GjEcCvHXi2qppqtPLwqi9KSbrxkJ3wCokV13V7WneQwd9mJ2D+fvlQ96unbUDEcElplZY4racqF15WAgrEmsInUOw07D21qhNsprDM2QE9zpO/U5dKCuyit2sWxxa08/fm/vE9uN8q+mQf3et3jcv7vHt0DN+sOcBV01bSxteT4tIyikrKKCwqIaegiLeu7V8hgOw5kSfRu96XyczNzd+KZ5pbVYHm7qbo1zmQfp0DuWdsDCeLSzn2xZec2t+Kb/2v4+78l3l2mAed48+hd3gAPp529XKlJWJd0mOiNJN06At7l+I7+hHevq4/v27LpI2vJ218PBmz+g0KT0Sw4VgAHVNWEmAYTq0tHpu5hTmbMli+6zD/mzIYnxOZMj2hYwIcXC/Dx60sUpqJ1xakcKSwiMcnx6GUQil46fK+5J8o5pEfNuPv48H8rZnM2niQu0ZF8eAEsfXo2ymAGesPMmVEVM0HOc3IOnqSTQfyGduzvcsWJktTsonr0IaQ1tKw4eamGBLVjmUpORjVvF+am7TDhSxOyWbxjmxWp+Zyz5hobhvu5IviGU6jRbyUUp2VUouUUtuUUluVUlNty4OUUr8opVJsv9vWtC/NGUx55CpVfpsRr5pwNmS7MSJeIGnFrG3ywZy51ToFpFTlzsb8dOkUrO8/wsjhkto7tNH5Orl7xELD29/6fndP6H4e7Pip8lijcuFlIdj8gsTWw77AftU06QztdUnV9euL2XWaWruB2WEBPjw2KY6o9v6EB/rSPdSf/l0CGd2jPc9d0pvzezsR4Ru+lGhf0u0SvTt1VGraXMDnZA4haT/hPeAG7r7hOgAuCc8nKSKosugCieKVFVdEECOGwYHVUFRIn06B/G18D24b3o0r4v0JzlmFX58LaRM9mE7FqcxcZW1gO2/LIeZsymBMbHs27M/jwe82YZjRrqFT5XcLSjfuzi7gkxWpXJXUmfjwgPLlXh5uvH1df/p1DuTuL9cza+NBHjovlr+fF1suIC7sF862jKOkZFoPeT9dMQyDu79cz22fruHh6ZspKqm5i7awqIS1+46UpxlNhkUHk3XsFLuyChrrdOvEyeJSnvkxmVEvLmLEi4t4bMYWth86Sqe2vjzzYzK/7z47bTAaM9VYAtxvGEYcMBj4i1IqDngYWGAYRgywwHZbc7YSaEvDmZGrgkzXIl5gPWS7MSJeIBGv4kL5MDuWUbW+yySkh53wOlC/+i6TrsPkd3XpRmcF8vb0nAQn86RL0n475V4ReXTE7NQESQFunwuJN4q/WUPj5gYRQ+s0MPv6wV359JaBvH/jAN66NpFXr0rgxcv7ctVAJ4+rrEzmc3YeJLVzZtrUVbGy7hMRU0m3Sc2gp19VyxGTHFsE1DSljRgh2x5YVXm9nfNl7FTsZOISR+Ghypg9fx75JyrP/8wrLOKxmVuI69CGd69P5KHzYpm98SBbV8yViFrPP0l00/ZY5m89xPAXFjLihUXc8dkaXv11J/O2HCLtcGG9LTNc5d9ztuHr6W4Z6fTz8uDDm5IYFxfK85f25s+jKke2JvXtgJuCGRvqn24sLTNIO1xzl6RhGKzdl8upEhe6ievInE0ZrErNZUhUO75es58bPvyDI47NGw78sSeX4lKD4TGV/0eaaUfTzb6l8MK8HUxbsoeI4FY8MTmOhfePZOnfR/P1HecQEdyKe75ab93Ze4bTaKlGwzAygAzb38eUUslAOHAhMMq22ifAb8BDjXUemhaOd2uJcuSl2cYF5bgW8QIRC/t+r7zsWIak+2oqzq8tptDa/E3l244Ex8DGr8Tm4mg6dBtV/2O3DpUP7dSlzo1ec/dA1Ojq9xM1VgaBJ8+pOK/cPfI8OqZNTYJjYOsPUttlM2EtN2VtDCJGiO3FkdTaN0HUht0L5bGP/qfcDo6R9036Wuh3dfXblhbDmg/l+TQnFrTv6bzz1BTi5rpdzxGxu3dp5ffH9tngHwbhibjZbFW6ndrOf+bv4P8urIiw/mtOMrnHi/jopiQ83d24c2Q3dmUV0HrL72SED6CDmzuEJ1K6fy0PfL2BH9anExvWmqgQf5IzjjJ/W2alufStvNwJ8PUsT3f6eLnjriS9av4kdG7LbcMj65TGWrQji0U7svnn+T2dThkI9PPivRsGWN7XvrUPw2JCmLnhYL2nCjw6Ywv/W53GN3ecQ1JEkNP15m/L5I7P1nLFgE68cJmTyQ314ERRKc/+mExchzZ8dusgZm1M56HvNnPxW8v54KYkokKsI9dLUrLx9nBjQETlRFHnID8i2vmxfFcOtwxzUm7QxCzZmc2Hy/dywzldK71/Afy9PXjnukQufGM5d3+5ni9vH1R5Vu0ZTpM8UqVUBJAA/AGE2kQZwCHAYkid5qyibVdJGRbm1jwuyJ7ALnAqX/y8TI4ebPhoF8gHq3KTWiuosJhwxIxqZCWLCLQYF1QnIoeLyCyx+EZcVCgWGlbpQnu8/MQsdvvcivmPVqar9oT0kChZ/gGxOeg+0Xl0rCGoxq2/QVk1TQR+T9tgcDd3iXy5EvHaPkde24FTKpaFxkvEy8oaIXunCCofW4rNu7XUYdl3cBafEEuK2Ask8tc6FAI6c2HIIT5fuY8t6fmAiJjp6w7w55FR5Sk7pRTPjA2kq1sWHxwIZ13aEfZ6x0J2Mr9u3MPUsTHMunsYb17bn4UPjGLrUxP44a4hPHtJb/56bneuTOrCkOhgYgNKuC/vWS7I+ZCcgiIO5p1k3+FCth48ytM/JvPBsr21eopLSstIzjjKv+ZsIzK4FTcOiajV9vZc1K8jB46cYO2+I/Drk/JTS2asT+erVWko4Om5zm0sSkrLeH7edjzdFd+sOcBPmzMs16sPby/ezcH8kzz5p164uykuTujEl7cP4tjJEi5+c3m5Qaojy1JyGBhpkc5Gol4r9xxuERMRco8X8cC3G4lu788j5/e0XKd7aGuevaQ3q1JzeXF+NRMyzkAaXXgppfyB6cB9hmFUcrs05J1v+e5XSk1RSq1RSq3Jzs5u7NPUNCeml9dxFz28yrez8PI6luHcw6s+ePpKd6E5wNvZOZp1PHuXiIisj4eXPdHnQvFxa7uFI6nyu6ZUI0h347GDkLFeREJNKUqzwH7x8/LYrQaJNyQhseAXXOs6r1qRuwdS5sOAm8Gjwr2e8ESJWpWccr4twKr35b0XM65iWWg8nMit8HCzJ2dnhdWISeRwEXmnbDU5uxdKKtu+8SG8P3Fluwhq5c2jM7ZIEfr3m4lu7889Y6Mr7c57/woAdrVK4Nr3/uBfG3xxp4yZl/jz13Hd8fKo+Ffv5+VBQpe2XD2wC1PPjeHxyXG8NNyNV/PvY8jJxVypFjD77qH8OHU48+4bwaL7R3F+7zCe/jGZBcmZTp+WE0WlfL/uAE/O2solby0n/smfmfjfpaTmHOfxyXGVzqG2jO8Vho+nGz+sT4fN0+GPd0WsusiurGM88sNmBkYE8e+LerNhfx5znQiqr9fsZ0/2cf57VQJ9OwXw8Pebych3/VgAh/JPknXMOoW2P7eQdxfvZnLfjgyMrIi6DYgIYsZfhhIW4MONH63i85WV61cz8k+QklXAiBjrUoxh0cEcLyplw/68Wp1rXSgtM0jNOW55n2EYPDx9E3mFxfz3qn6WItHkooRwrh3UhXcX72H+Votr5wylUYWXUsoTEV1fGIbxvW1xplKqg+3+DkCW1baGYUwzDGOAYRgDQkJcrPnRnJ4EdpEOQHNcUG1SjVC5wN6Za31DYKYXnaUZQaJObh4Vtg0NUeMF0G20dHFun1P1vpqsJOzpPkHSXMlzJMJ4Kr8G4WWL4K3/HNrFNEzqtDqUkuLz1KXW0aOGYPUHEuFKvLny8vBEqb06VM3MSNP3K+m2yk7/5nvCsc7LMER4BTsIr4hhUs+1/w+5nTxHImJmZ6ftfNzy9/HUuVJAf+nbK8g8epIXL+uDt4fDh1nqUvAN4p83XUrHQB8GnCNjsLoVuRBJ2PQNvD9OUqgJ18sXoKMV9VRubor/XN6P+I4B3PvVepIzqk6L2H7oKH96Yxl/+2Yj36zZj4ebG9cM7MqrV/Zj0QOjGN3DxWvaCf7eHoyLC2PBpr0yRaK4EHYvcmnbwqIS7vpiHb6e7rx+TQJXJnUmNqw1L8zbUaWG6/ipEl75JYWkiLZMjA/j1asSKCop44Fva3bZBykmf/XXnYx4cRFjXlrM9+sOVImsPfNjMm5K8Y+JsVW27xzkx/Q/D2FETDCPztjCEzO3lNfhmfVbwxwK602GRAWjFE6jZQ2FYRg8+O1GRr30G7d9soY92ZUL+r9evZ/52zJ5cEIPenUMcLKXCh6fHEefTgHc/+1GPlu5j9cXpPD4zC3c+dlaLn17Be8udn0e6+lCY3Y1KuADINkwjJft7poF3Gj7+0ZgZmOdg+Y0IbCrjNYxP7RcjXg5uteXlUHBocaJeIFrwsvdU4SMWThdHw8vezx9IOZc2P5jRZrQpCYrCXv8gqSAffsc1wRbm3ApHMeQ1FpTtKpHDpcPfvP8GpKiQhGRPSdXTUmH95ff1aUbTd+vhOsqLw+Nk9+ZDnVeBZnSLRnsUFTeebAI9NSlYjex8yfpOrWvtbMV/J8flMGgyCB2ZRVw67BIErpYNILvXQoRQ4kJC2DB/aO4a/IQCOhS/WMpLYafHobvb5fHfsdiiQJaPAe+Xu68d8MA/H08uO2TNeXeYYZh8MUf+7jwjeUcKSzmo5uS2PzkBL658xwenxzHRQnhdG3Xyvk51IKL+nWk3Uk7jzerLyEWPDZjKylZBbx6VT9C2/jg7qZ4eGIsabmFfL6ycnPO+0v3klNwiocn9kQpRaStKHz5rsM1ploXJGcy7pXFvPprCuPjQunZoTV/+2Yjd3+1nrxCKRFYsSuHn7Yc4q5RUXQM9LXcT2sfT96/MYnbhsnc0Js/Xk3+iWKWpeQQ7O9NbJh1/WqAnyd9wgOcjg86drKYrQfzmbclg2lLdvPojM3M2eTabFR7Xpq/g+/XpzMuLpSVew4z/pUlPDlrK0eOF7Enu4CnZm9jaHQ7bnWx1szbw503r+mPp7sbj83Ywn9+2cnMDQfZlV3A4YJTvPjzDpcaIk4nGtPHayhwPbBZKbXBtuwR4DngG6XUrcA+4IpGPAfN6YCZMjQdu1u5GOH0CQTvNhXC63i2RBEaLeIVX/m3M+xnHDZUjRdA7GTYNhPS10DngRXLc/dIg4JvoOv7+elB2DlPblcnvNzcJN14eDf0varu514bImwu+qlLKzvmg0SQZvxZaqRccM6vwsYvpWbNvj7LpE24WHI4Eyvlvl+Xi4C1x7etiGzHiJfp6eZotuvtL8Jq71KIWiF1irGTKq/ToR8oN1T6Ol66fCr/W53G3aMd9gMS8c1PgyH3VF4e3r964fX19SL4Bt8lo7TcPSXq5u4l28VdWGn1sAAfPrgxicvf+Z0pn63h3esTeWLmVn7acojhMcG8fEW/cm+pWlNWKueTdIuk1S0Y0T2EX30OSXFKWG+bNUo13nbAN2v2M33dAaaOjanUCTiyewjDooN5fWEKlyV2IsDXk+xjp3h3yW4mxoeR2LVC3F6Z1JlFO7J44eftDIluVymKYxgGu7MLeO6n7fyanEV0e3++vH0QQ6KCKS0zeHfJbl6ev5O1qUd47tLePPvjdjoH+XL7iOqj0+5uikcnxRET6s8/f9jCxW8tJ/d4EWN6VO/3NSwmmHcW7+HYyWIMYHlKDr/tyGZJSjYZ+ZVTnw94fse2db707fSStc+dBZ+v3Mebi3Zz9cAuPHNxPDkFRbzy604+/T2VH9anE9TKC29PN/5zeT/c3Fz/ktY5yI8lfx9N/oligv29yiO6mUdPMvLFRbz8yw5evSrB5f21dBqzq3EZ4OyZH9tYx9WchpiRqwNr5J++T83haaDCy8t0rz9m+/bWWBGvbqNg2N8g9vzq1zPTSj4Bzn216kL38eKYnzy7qvByJc1oEnuBCK/V7wOq4vl3xqhHJLXj06ZOp11rgmNEAKUug8SbKt+3/nPpGt3xk6TFXJ23CeIRt+Bf0GUIdDmn6v1KiRhyJlZM3y9ndW6hvaoKL1OAhzgxjF32Cmz8n0TRoh3+LXr7S81b+lo6j/bjwQlVU1NARd1f5PDKy8MTZT5nQXZVi5b0dSK6Rj8KIx+sWO7hLaImfZ3loeLDA3jlyr7c+fk6hj23iDLD4OGJsUwZ3q1WH7RVSFsJO+aCX1unwsvT3Y3x7Y9SekhRPPAefGZNgbQV1uOugNWpuTw2YwtDo9tx79jKglUpxT/Oj2XS68t4a9Eu/nF+T15bkEJRSRkPTuhRZd3nLunDhFeXMPV/G3hgfHc2p+ez6UA+m9PzySssxs/LnUfOj+WmIZHltWzuboq7RkUzIiaEqf9bz00fyRfLd65LrLbuyZ4rk7rQtV0r/vz5WvIKi52mGU2GRgfz5qLdXPr2CvZkH6ekzKC1jwfDY4K5ITyQru386BLkR+dAL1r/dwq7TwXyrznbmOako9SeX7Zl8vjMLYyJbc+/LuyFUoqQ1t48c3Fvbjwngn/P3cayXTm8fW3/Oo138vf2wN+7siQJbePDzUMjeWfxbu4YGUXPDk30P6iR0c71mubHTMcdTYc2nWqXzgrsCkdsKYCjtmLZxuhqBCmwP/eJmtczP2QbKs1o4hMgHzLb50iEwnyecvdCl8Gu7ycgHDr2h4PrJB3lUUOUwnStbyrMOq+9SytmeIIMUJ//aEUzxpbp0P961/f78z9EOE1+1fl7LLw/7PhRolv2EcRy36/B4jxvRWgv2PWrFOebz2nOTrGpsPoyEDEMlr4kgq7HRPCySMmF95f0sv3z4MjepdKQEOIgzExvsoPrpLbPntXvS83gIIvIX3iinFNZaeU6NhvnxXfgsUlxTF97gKcvjrdOfdYWM23ozAvNRj/fTPYZoWwqTuAiDx+pjXMQXmmHC3nh5+3M2ZRBxwAfXr0yAXcLUdirYwAXJ4Tz0YpUhsUE8+WqNK4Z2IVuFlYObVt58Z8r+nL9B6u48/N1eLgpuoe2ZkJcGL07BTAuLpTQNtZiIz48gDn3DOc/83dworiUCb1q18g/uFs7ZvxlKN+uOcB58WHVrpvYtS3dQlrh6e7GlBHdGNWjPQldAquOyzqwFooK6OZ2kgXbDrJoexajY52XeKxPO8I9X60jPjyAN65JqGL90COsNZ/dOoi8wiIC/byc7KVu3Dkiii9W7uOln3fwwU1J9dpXYVEJMzcc5IoBnS3fE02FFl6a5sfLT9KLxy2+mddEYBfYu1g+mMojXo2UanQVM63UkGlGk56TYM5fxa4iNE4+5PP3Q5DrI3bK93NwXc0WFM2FOSbp8K6K53OeTTjd+gt8eyOseldqrVwR6im/ykigUf+wjj6ZmGIlY0PlRgJH3y8rQntJqjt7B3ToI8tydsr5W51j50ESwSwrrppmtD+f9Z9L56rVa2UYEhmMGFb1GB36igVK+trKwuv4Ydj8nTx3VtHl8ESx28jZKTYqFtw6LNLlGp4aMQwRUCDv62rSh4GF+9jm2ZlH5u4hwj+B6E0zyRrwGJEh/uSfKOb1hbv49PdU3N0U94yJ5o6RUVWiKPY8ML4HczdlcMvHq/HxcKsSGbNneEwI3955Dh5uip4d2rgctQKpkXt0UpzL6zvStV0rHphQzfvWhreHOwvvH1XzDlMePKQtAAAgAElEQVSXAOBulDCkXQFPzt7KOVHtLB/TnuwCbv1kDe1bS6rZz8v589nQogukdu3OUVG8MG8Hq1Nzq/Vfc8aR40V88nsqH69IJa+wmM5t/WqMHjYmZ49jmaZlY9Z5+dfS1i2wCxQVSIfe0Qzp2HO1OL+xMFONDWUlYU+PCwBVESE4sg8wapdqBKnzgtpv11SYUYy98gFByi+w5TsY/oBYMwy8Xawf9q9yvg+TouMw96/yugz7a/XrdrTVkTimG1dNk/em6ftlRahtLqJ91CbboqPRxMsPOiXJe7bHROt1anLUP7IXjh6ommYEW6qyZ9Vt138Kpaecp0xr6+JvsvUHmDYa3h5W+ef9ceID54xDm6RGretQabJx1lRRWoI6vIvY3gO4OCGcn0oG4H/yEFNf+Zhhzy9ixAuL+Gj5Xi5J6MRvD4zm/vE9qhVdIMPbbxkWSXGpwZQRUTXWqCVFBJHQpW2tRFe17PkNvr1Zxos1BGVlMP22mn3w9i6Vsg7gHwPc2He4kGlLqj7vq/bmcunbYlXy8c1Jda/hqyc3D4mkfWtvnv9pu1P/NSvS807w1OytDHluIa/+msKArkFM//M5zSq6QEe8NC2FwC5SNO5qYb1JeWfjPvHw8g+1TI80Kd6tYewT0G1kw++7dajUdyXPhpF/r52VhD0h3aV2K8a6nqbZCeom6bnUZVLUP+dv0hloOvf3vgJ+eVIEUZdB1e/rt2clNXnTjzWnVX3bil+bfY2T6fs18qHKvl9W5+zhI0PUAU4dkyiso4eXPSMekAiZY7G+Sfs42Wf6Ohnm7Yj5ARthIbzAlqqcU5GqLCuF1R/K+k6iWQRFSdNK+tqq3ZtWlJbAgidhxetyvpW6aw3Y+TP88Q6M/7f19slzJDI34gH4bLltCL3Fc5a3D0qLaNc1nqcTesP4jhgvvckzPVN5Q52Dh5sb94yNJjasdnVA946JITK4FX/q2wyR8rWfwNbvpU7v8k+k47g+ZG6WyG7RcWsxDtLNmrYS4i6Czd8Q53mIC3p3581Fu7g4Iby80P7bNft55IfNdG7rxwc3JREZ3DDdqXXB18ude8fG8OiMLSzakcWY2Mpf0MvKDPYePk5KZgG7so6xM7OAlKyC8vmef+rXkTtHRtE9tIEnmtQRLbw0LQNTQNU2WmV6eeXtazzX+row/G+Nt+/YC+CXxyXaVVfhBTCqBU/qUkrEwZ7fYNEzEhG5eV6FcPL2h4RrRXgdexpaO6l9ObgBfn8T+t/o+odaeGJFpA3sfL9uqn47dw+pszIjXuWjgqoRXtFjqxbVV9qnp6QMnUWfUpfKlw1nxwhPhPWfSWQsqJuIoPw0mPC082O6uUnkz5WI1/Ec+O5meb6SbocJz1QVp9/cCOs+E6Fv1Qyxfa40O3QdKtG/zK0QbzGEvfz5tKXcWrVDdR1K72PLePcv/6n5XJ3g6+XOFQMauB7TVdLXSsq5MBc+/ROMf1q6detq22IK8d0LRXxZ1Q0eXC9mzLEXSJlGzk4enXQHi3Zk8dTsbUy7PpEXft7BO4t3MzS6HW9dk0iAn5ORYk3IlUmdeW/pHl6Yt4NR3dvj5qbYn1vId2sP8N3aA6TnVZjchgf6EhPqz5jYEK4Z1JVwJ9YdzYVONWpaBqaActU81XG7vLTGc61vaZj1QNvnivDyDnAeMTmdiRwuZp6/vyGip6tDJ2LSbVJTtfYT6+1LS2D2VCk8H/eU68cNT5T30tGDNt+vz6x9v6wIi6+IeOWkyG9HD6/aEp4IGRslUmFPdfVd9ttCRQRv1TSpPexRQ2dueKIIoOrc4dPXwbRRku696G244CXriODAKWLhseW7qvfl7oGsrfKe9vAWAZnpxMC2XHjZ1WHFToLs7ZCzq/rH0xI5niNfGGMvgNsXQMwEmPcQfD9F3nd1IdWWQiw5KY0eVphfKiKGl1vfdAjw5d6xMfyanMnFb6/gncW7uWZQFz6+eWCLEF0gXa1/G9ed7YeO8dTsrVw9bSXDX1jEawtT6BbSiucv7c2su4ey9akJLH94DB/fPJAHJ8S2ONEFWnhpWgqmgKptcb1PgPh55aU1rmt9S6JdlKR0TBPUoMimMTZtasz0mX8onGshnNpFifXAmg+tRcmip6VIfuLzkkJ0Ffsap83fwsl8a98vK0LjpUmkIEtSiG4e9W9gCE+EkhNSeG7PvuUiEJ2lGUHSiR6+8liyd8KeRTLkvBrvq/JjlpU4H/y9awF8aOt2veVn6FdNc0fXIdC+l4g+x/ocs6g+9gL5bWXJYZK9U94L9t2m5nbbZ1f/eBoCw5DGlrQ/GmZ/phgOT5T/Y1d+DmMelffch+MlVV0bykph3wroc4W835OdGMymLpXXo1W7Cs9Bw+CWoZFEhbRi84E8Hp8Ux9MXxVfthqwtpwpg+u3Sed0ATO7TkalBK/Ff9V/S805w/7juLHtoDJ/dOogrk7rQp1MgrWqo62sJaOGlaRl0Hgz9roPIOtRFBXaRD6VT+WdHxAvkm37a7xIJaakF8vWlbQQM+jNc/K5zc9iBU2RaQbLdB++pY/DNDbDsZeh7DfS6uHbHDY2XbsMDa8SpPjTe2vfLcltzdNAW+UBrG1nZjb4uWDnqr/sUPrtY7ECcdURC5VTl6vclGtL/Rufrlx+zhgL7RU/Ll5wpi2W4eHUoZdcM4SBats+BsD4VpQZh8dKle8Ji3qDV6KXAzmI060xkNCRZySLyV7zWMPtLXyu1bR1sz5+bG4x4EK7+SsZWLawmHWxFxkaZktBttAyz3/kzlBRVXqekSIRjxDC5HdJDvlgUZOHl4cZntw5i1t3DuGVYZLVGrS6zfS5s/kaMhxsAt9xd3HfqHe73mclv953DPWNjWmREqya08NK0DLz94aI3oVUduk3adq34gDgbIl4gdhBGGRTmnLnCSymY+BxEjXa+TvS5ItBWvSe3c1LgvbHygT7+33DRW7WPBnr6iADY8IUUKw+83fV9mFMNMrfahmPXM80IIt5828p7vOQUzL4PZt0jNVF3LK45SmymKjd8Cb0ucS2q3KaD2LJYCa8Da2X54LskauIKfa6QlPiqaRXLjmVKmrLn5Ipl5vOXta3y9oYBOTusa9l6TpLGnKO1H39TK0yj2l0L6p4KtCd9rdQEOpos95gISbdKQ8KBWnSWpto1WvScJF9E9zkMm09fK9FTs/DeTNva0rgdA32JD3fRwNoVzEhkTV2WrlBWBrOnokqLcCs9hdvBNfXfZzOhhZfm9Mec9QhnT8QrrI9EO+DMFV6u4OYutV5pK2Dpy/DeGBGj18+QETp1/dYenigpQ58AGRHkKn5BIlgOrpc0sOOooLpgOuqnLoOPL4C1H4ktxnXTXavtC+8v10fRMddTpuZ2VsJr9Xvg5V+7EVJeraRDcttMOHZIlu2YCxiVI3ZmxNBxUPnxbInMWAkv0xpl+1zXz6cupC6V1HHJCdi9oH77Mgx5bs1opiNjH5eGkdlTq6bRnbF3qdQTtg6FqDEyY9UxEpi6FFAi2qGi/jDHhWHqtaX4hIhUNw8ZB1d8suZtqmPD55JeH/cvQMn1cJqihZfm9Mf0AIOzJ+KllHyrhbNbeAH0u1bqmBY8JXVfUxbX38rDTLUlXG/dGVYdob3Ed6yspP6F9fbnc2SvpLuu+BTOfdJ12xTzsXTsD50Sa3fM3D3ScWdyPEeMbfteXfsRUkm32pohPpbbyXPkvWtva9G6g8wddSywN2deWtlMhPQQC5ClL8Pnl1X+mf9o1aHydaGsTD7o4y+TmtL6pjaPpMKJ3IrXxhGfADj/RYm4/v5mzfsrLZHSAzOF6Okr3bLb51Z+/HuXSDTXFOxtOoqINhtBGpLdC8XwOOl28Y074ILnnjPMqRVdh8kXqg59GiaK1kxo4aU5/TEL8+HsiXgBDLhVBhk7G2FztuAXJELknLvFciKwAawBYsZLJGbwXbXfNrSX1NpA9VYStaH3FfJa37agyvDqGmkbIdEmV8Zd2WM/cshk3SdQWuTcfLU67Jshjh8WERA7qXJUUqnqZ15aPZ9KibFu6zAoPFzxc+yQeIut+aD25+pI1lYZZB41GrqfJ3MuXY1EWWFGEp0JL5AUbOwk+O0556ayJhkbxEja3rsrdrLUP5rHKj4pkSf7ZgylJCqb3QgRr+Q5IiBHPCi1bPURSvMelgiaOe4rYrgIueq6blswWnhpTn9M4eXdpmGHUrd0gqMl+lGbQdFnKoPvFG8qz9oP57WkVTBc9YXMtawtZp0SNEyqESpe6/ZOBmVXh1Jw4ZuVRyC5Qsd+gKrovistgTUfSQNMXWvXBk6BgkypUSsrrlzfZRIaLzVe9pGanJ0yW9LZGK5+V8OURZV/7lwqhea/PlX/+q9yo9phEmk+mV+/VFf6OjHGbV/DGKGJL0iqbs7fqnaEVjo/O4sIk+7jZVuzzip9jaScHbtgg7s3fMSrtETEaffzpA6wQ7+KGrTasnO+RFlHPFhxPUWOkC8ArkyuaIFo4aU5/TGF19kU7dK0XMJswqt1h9qn41oSPgHyoWxGTHbOk47D2tSJOWI2Q+yYC/5hED6g6jqhvSRFdcTOgqC6mZfOUAomvSwC78cH637OICKrbaSMAYsaK6nt7fVIN6avlUh1TR2vAeESqdyzCDZ9U835LRURZ9+c5NtWRFaybXLB3qUSeeo6pPK2wd1l7NSpgro/Hkf2LZcIoVm/FzFMuoRr25RwqgDm3i8p+6H3VSzvMlgey2la56WFl+b0x9sf/Nq1HNd6zdlNu2ixbWioNGNzEp4oIsEwpCMxoLNEMeqK2QwBEHu+WCg4YgpX+zqv6mZeVkdQNxj1sIikutZllZVKd6CZxvPys9VP/Vi3+rHSYukyrS7NaM+AW2Sm58//kBSt1f7SVlbUd9nTcxLk7haT2dSl0pTjaM1iPq+HnUS9MjbK9IjaiKbtcyWiZ05liBwhAtjRTqQmFj8vkxYm/7eyOa9PQM1RtMO7JdrZEDV+DYwWXpozg8SbIf7S5j4LjUaiGAOnSAH66U54f+ko3LVAxsu4Yr5aEwnXS9dd4s3W94fESjTDrPM6VSARmepmXlbHOXdL+vLHB+Hk0dpvf2izpBYjRlQsi50kczgPrq/9/rKSpTPSVeHl5i7C42Q+/PJY1fvT10mE0MpIt4fNYHbzt7b6LgtxZqaNs3daH3/RMyKAPhjnmhGqYYjwihpb0ZjSZbCMg6pNutEwxAIl7sKqUytAhHB1UbRFT4uXX/Z214/ZRGjhpTkzGPsY9L+huc9CoxEmPC11R6c7pjiY+1dw926Ya8w3EK7/QTrTrPD0laihKbzMSExdI4junjD5NXH5X/iv2m+falffZdJ9ggiJujjmlxfWO7GSsCK0FwydKt5yexY7nN8SQFmLqjYdJFq28m2piYocUXWdtpHyWHIshNepY7B7kYi6/AMwbaR07FbHwfUilHva2YR4t5bHW5sC++ztYg0TPc76/ggziray6n3HDol1CTifhNCMaOGl0Wg0GmtC4yVtmpcmEeW6GBzX6bi9KsYVNcTMy06JMnx61Xuwf3Xttk1dJkLQvpTBL0giLnVJX6avlfqrtrUcJTXiQUmdzrmvcjff3qXyOjnzdIudJBEx5W49gcHDS8ZaWXl57fpVrCBGPQxTfhPvwC8uh8UvOk/hbZ8jx3JMSUcMlw5ZV2vJTJEWaRHJA4miuXlY13mt/VisS5S789mfzYgWXhqNRqOxxsMLwnrL33WxkKgrofEyQPrkUbE6UO7196sb86j4Vs2+13okkRWlJTL/0CqNFztJonGOVgyGAeu/kBowK9LXSSSx1hMVfGHSq2ItseRFWVZySuqmrKJdJmbnaMd+zps9gntYdzYmz5H62S7niDi7db4YCi/6N3x9naQ/rbaJGFpVCEYOFzFkFaGyInWJ1BTa+zTa4+0v3nSOUbSSIrEsiR4nDQdaeGk0Go3mtKLfNdDnqtqlxupL+eigZEmBBUVWLq6uC96tJeWYs1MmHGRuq3mbQ7b5h1ZRF3NAt/2c0KLj8P3tMPMu+O6Wqv5bRcchO9n1+i5Huo0Uw+Dl/5UUWvpasYhwFhUC8U/rfTkk3uR8neAYKUYvLalYVlIEKfNlhJFp1uvlB5dMg/Oeky7X98ZAll0NVU6KRM5iLWxCOg+SGaiupBvLyiB1uQje6gRqxLCqUbTts8WyZOCU6oeuNyNaeGk0Go3GOUm3wSXvNu0xKw0bT2m4DtGYc+GmuWI2+v65sOX76tcv9++yEDZtOoodhmkrkbsHPhgPm78T6wMr/62MjTJjta7CC2QGqU8AzLrXVu+lqlpEOHLp+9XX54X0kHqpI6kVy/YuEdHpKKKUgsF/hhtnS8TrvTGwdYbcZ4pQU5Ta49XKNvrKBeGVtU2c/asTlFARRUuzi6Ktek8sS6LPlQ7ZYxnW3aDNiBZeGo1Go2lZBHQScZGxAQ7valhrji6DZaxUWDx8d7OMorGP9NiTapt/6N/e+v6ek6SYfO3HMG2UFKBf9x2Me8raf8ssrO9Yj+ihXxBMeFYMUVe8Jk0Kvm3rvj+oeH7tC+y3z5ZxQt1GWW8TMRTuWAKhcfDtjfDLE5A8Sx6bM+PhyOFwcEPN3aWp1QheezoPliiauX7GJhmdlHS7WJWYAj6rZUW9tPDSaDQaTctCKUk37pgnkZiG9kRr0wFunCMf0Cteh88vljmU9pj+WNVFXcxo0OypUng+5TeJtICM9HL030pfK4bP/iH1O/8+V4grvzMbidpiOsKbBfZlpVKjFn1u9dMg2nSUCGLizbD8VRGh9t2MjkQMB6O0coTKir1LJWpV0/gvLz/oNKBCeK1+T8xtE66V22bK2nHoejPTaMJLKfWhUipLKbXFblmQUuoXpVSK7Xc9ZbpGo9FozkhCe8HxLPm7riOKqsPDCy54CS56G9L+gHdHVoxHAonMFBVUL2yCo6V4PeF6KTwPsutUdHOr8N+a/6gsS19bvzSjiVIw6RXxPOt1cf335xMgkwTMAvsDq+W5txrp5IiHt8xQ/NPr0L6X1JM5o/NA6ZJNXeJ8nbIycb6vrmHAnghbFC0vDTZ9C30ur4gA+reHViEtrs6rMSNeHwOOFscPAwsMw4gBFthuazQajUZTGTNNBA0389KKftfArT+LmPnwPFj3mSw3xUFNAuDKz+HCN6xnppr+Wxu/lNqvvLSGEV4gIu8vf0jEpyEI6V7RoZk8W1J4MU48tKzofwPctaJihJsVnr4SBayuwD5zM5zMq2xYWx0RwySKNvNuMaZNcui+DY1vcZ2NjSa8DMNYAuQ6LL4Q+MT29yfARY11fI1Go9GcxoTabCz8wyQi05h0TJC6ry6DYdbdMPs+MQ51nH9YF0z/rRl3ye2GEl4NjTks2zCkYaDbyMZ53iOGw6FNzi09avLvcsSMou1dLLYXjsa8ob3EjNVZHV8z0NQ1XqGGYWTY/j4EhDbx8TUajUZzOtA+FlCNG+2yp1U7uO576Uhc+5HUDTVE/ZTpv1V6SkYhdehb/302BsE94FQ+7F4o3Y2x1dRq1YfI4dLZuW+F9f2pSyEoSurHXMHTFzoNlL+tvOZC48VyI3d33c63EWi24nrDMAzAcHa/UmqKUmqNUmpNdnZ2E56ZRqPRaJodr1YymidmfNMd091DOhIv/0SK5Rtq/mu3kTDwDoiZUDG/sKVhCtylLwPK2hKiIeiUJDVYK16r6n5fVmozrHWxvsuk92UiaK38w+ytSVoI9Zx2WmsylVIdDMPIUEp1ALKcrWgYxjRgGsCAAQOcCjSNRqPRnKFc83XzHLfXRfLTkJz/QsPur6ExGxj2LRObBmcWGvXFw1u8yGb+BdZ9AgPshqVnmIa1LtZ3mQy4ufJ+7AnpIZ5qmVsbTkjXk6aOeM0CbrT9fSMws4mPr9FoNBqNxpHWHcS3C6q3hGgI+l0radxfnpCB1iZWA8nri4e31K+1oM7GxrST+Ar4HeihlDqglLoVeA4Yp5RKAc613dZoNBqNRtOcKLt6usZKM9ofa9KrUnv100MVy/cuFZHUOqxhjxfaq0V5eTVaqtEwjKud3DW2sY6p0Wg0Go2mjkSOlKhXfQeSu0JwNIx8EBb+G3b8JEOt034Xc9iGJrQXbP4WThypv8t/A6Cd6zUajUaj0UhjwU1zmu54Q6ZCSE+Y+4DUltVkWFtXTGsSVwajNwFaeGk0Go1Go2l6PLzgT6/B0XSYbrOCaBTh1bI6G7Xw0mg0Go1G0zx0HghJt8qIopCe9Z9jaUXrMPAN0sJLo9FoNBqNhrGPy6ihHhMbZ/9KQVh8i+lsbGofL41Go9FoNJoKfALgnnXit9VYhMbD2o/FpNXNvfGO4wI64qXRaDQajaZ5cfeUyFRjEdoLigshd2/jHcNFtPDSaDQajUZzZtOCCuy18NJoNBqNRnNmE9JThpS3gDovLbw0Go1Go9Gc2Xj6QLsYLbw0Go1Go9FomoTQXpC5ubnPQgsvjUaj0Wg0ZwGhvSAvDU7mN+tpaOGl0Wg0Go3mzCfMNjooK7lZT0P7eGk0Go1Goznz6XIOTPkN2sc162lo4aXRaDQajebMx6cNdExo7rPQqUaNRqPRaDSapkILL41Go9FoNJomQgsvjUaj0Wg0miZCCy+NRqPRaDSaJkILL41Go9FoNJomQgsvjUaj0Wg0miZCCy+NRqPRaDSaJkILL41Go9FoNJomQgsvjUaj0Wg0miZCCy+NRqPRaDSaJkIZhtHc51AjSqlsYF8jHyYYyGnkY2jqhn5tWib6dWm56NemZaJfl5ZLQ782XQ3DCLG647QQXk2BUmqNYRgDmvs8NFXRr03LRL8uLRf92rRM9OvScmnK10anGjUajUaj0WiaCC28NBqNRqPRaJoILbwqmNbcJ6Bxin5tWib6dWm56NemZaJfl5ZLk702usZLo9FoNBqNponQES+NRqPRaDSaJkILL0ApdZ5SaodSapdS6uHmPp+zFaVUZ6XUIqXUNqXUVqXUVNvyIKXUL0qpFNvvts19rmcjSil3pdR6pdQc2+1IpdQftuvma6WUV3Of49mIUipQKfWdUmq7UipZKXWOvmaaH6XUX23/x7Yopb5SSvnoa6Z5UEp9qJTKUkptsVtmeY0o4TXba7RJKdW/oc/nrBdeSil34E1gIhAHXK2UimveszprKQHuNwwjDhgM/MX2WjwMLDAMIwZYYLutaXqmAsl2t58HXjEMIxo4AtzaLGel+S8wzzCMWKAv8hrpa6YZUUqFA/cCAwzDiAfcgavQ10xz8TFwnsMyZ9fIRCDG9jMFeLuhT+asF17AQGCXYRh7DMMoAv4HXNjM53RWYhhGhmEY62x/H0M+QMKR1+MT22qfABc1zxmevSilOgEXAO/bbitgDPCdbRX9ujQDSqkAYATwAYBhGEWGYeShr5mWgAfgq5TyAPyADPQ10ywYhrEEyHVY7OwauRD41BBWAoFKqQ4NeT5aeMkH+3672wdsyzTNiFIqAkgA/gBCDcPIsN11CAhtptM6m3kV+DtQZrvdDsgzDKPEdltfN81DJJANfGRLA7+vlGqFvmaaFcMw0oGXgDREcOUDa9HXTEvC2TXS6JpACy9Ni0Mp5Q9MB+4zDOOo/X2GtOHqVtwmRCk1CcgyDGNtc5+LpgoeQH/gbcMwEoDjOKQV9TXT9NjqhS5EhHFHoBVVU12aFkJTXyNaeEE60NnudifbMk0zoJTyRETXF4ZhfG9bnGmGem2/s5rr/M5ShgJ/UkqlIqn4MUhdUaAtjQL6umkuDgAHDMP4w3b7O0SI6WumeTkX2GsYRrZhGMXA98h1pK+ZloOza6TRNYEWXrAaiLF1m3ghBZCzmvmczkpsdUMfAMmGYbxsd9cs4Ebb3zcCM5v63M5mDMP4h2EYnQzDiECuj4WGYVwLLAIus62mX5dmwDCMQ8B+pVQP26KxwDb0NdPcpAGDlVJ+tv9r5uuir5mWg7NrZBZwg627cTCQb5eSbBC0gSqglDofqWFxBz40DOPpZj6lsxKl1DBgKbCZilqiR5A6r2+ALsA+4ArDMBwLJTVNgFJqFPCAYRiTlFLdkAhYELAeuM4wjFPNeX5nI0qpfkjTgxewB7gZ+VKtr5lmRCn1FHAl0q29HrgNqRXS10wTo5T6ChgFBAOZwBPADCyuEZtQfgNJDRcCNxuGsaZBz0cLL41Go9FoNJqmQacaNRqNRqPRaJoILbw0Go1Go9FomggtvDQajUaj0WiaCC28NBqNRqPRaJoILbw0Go1Go9FomggtvDQazWmJUqpUKbXB7qfBBkErpSKUUlsaan8ajUZj4lHzKhqNRtMiOWEYRr/mPgmNRqOpDTripdFoziiUUqlKqReUUpuVUquUUtG25RFKqYVKqU1KqQVKqS625aFKqR+UUhttP0Nsu3JXSr2nlNqqlJqvlPK1rX+vUmqbbT//a6aHqdFoTlO08NJoNKcrvg6pxivt7ss3DKM34kD9qm3Z68AnhmH0Ab4AXrMtfw1YbBhGX2TO4Vbb8hjgTcMwegF5wKW25Q8DCbb93NlYD06j0ZyZaOd6jUZzWqKUKjAMw99ieSowxjCMPbah64cMw2inlMoBOhiGUWxbnmEYRrBSKhvoZD+6RSkVAfxiGEaM7fZDgKdhGP9WSs0DCpCRIzMMwyho5Ieq0WjOIHTES6PRnIkYTv6uDfYz9EqpqIm9AHgTiY6tVkrpWlmNRuMyWnhpNJozkSvtfv9u+3sFcJXt72uRgewAC4A/Ayil3JVSAc52qpRyAzobhrEIeAgIAKpE3TQajcYZ+puaRqM5XfFVSm2wuz3PMAzTUqKtUmoTErW62rbsHuAjpdSDQDZws235VGCaUupWJLL1ZyDDyTHdgc9t4kwBrxmGkddgj0ij0Zzx6BovjUZzRmGr8RpgGEZOc5+LRqPROKJTjRqNRqPRaDRNhI54aTQajUaj0TQROuKl0Wg0GkIljxkAAABBSURBVI1G00Ro4aXRaDQajUbTRGjhpdFoNBqNRtNEaOGl0Wg0Go1G00Ro4aXRaDQajUbTRGjhpdFoNBqNRtNE/D/Ib4QYexYaZwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the model\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.plot(val_losses,label=\"Val\")\n",
    "plt.plot(train_losses,label=\"Train\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('plot_graph.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "22cf10741174a73c16493d470940eeed718720095727e52348075ea0a9a6dd40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
