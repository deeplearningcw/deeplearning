{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f56a5f4-3107-444c-8ef5-da497b39bb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyliu/miniconda3/envs/pytorch/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import time\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9244a66-7eab-419f-a9ee-85e480dcac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "                 transforms.RandomHorizontalFlip(),\n",
    "                 transforms.RandomCrop(32, 4),\n",
    "                 transforms.ToTensor(),\n",
    "                 normalize,\n",
    "                 ]), download=True),\n",
    "                 batch_size=64, shuffle=True,\n",
    "                 num_workers=4, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "               datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               normalize,\n",
    "               ])),\n",
    "               batch_size=64, shuffle=False,\n",
    "               num_workers=4, pin_memory=True)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b5126b-64fc-4851-9363-99652bf37466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv3 = nn.Conv2d(out_channel, self.expansion *\n",
    "                               out_channel, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channel != self.expansion*out_channel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion*out_channel,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969b3dcd-949a-4e7e-a815-1e7fef511b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 128\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        # self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        # self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(256*block.expansion*4, num_classes)\n",
    "        # self.linear = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        # out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9464dcb0-a98a-4a0d-bd14-e4326c0816ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project1_model():\n",
    "    return ResNet(Bottleneck, [2, 3, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f35d814-7e04-43fa-a6a0-1cc434c03789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 32, 32]           3,456\n",
      "       BatchNorm2d-2          [-1, 128, 32, 32]             256\n",
      "            Conv2d-3          [-1, 128, 32, 32]          16,384\n",
      "       BatchNorm2d-4          [-1, 128, 32, 32]             256\n",
      "            Conv2d-5          [-1, 128, 16, 16]         147,456\n",
      "       BatchNorm2d-6          [-1, 128, 16, 16]             256\n",
      "            Conv2d-7          [-1, 512, 16, 16]          65,536\n",
      "       BatchNorm2d-8          [-1, 512, 16, 16]           1,024\n",
      "            Conv2d-9          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-10          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-11          [-1, 512, 16, 16]               0\n",
      "           Conv2d-12          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-13          [-1, 128, 16, 16]             256\n",
      "           Conv2d-14          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-15          [-1, 128, 16, 16]             256\n",
      "           Conv2d-16          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-17          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-18          [-1, 512, 16, 16]               0\n",
      "           Conv2d-19          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-20          [-1, 128, 16, 16]             256\n",
      "           Conv2d-21          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-22          [-1, 128, 16, 16]             256\n",
      "           Conv2d-23          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-24          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-25          [-1, 512, 16, 16]               0\n",
      "           Conv2d-26          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-27          [-1, 256, 16, 16]             512\n",
      "           Conv2d-28            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-29            [-1, 256, 8, 8]             512\n",
      "           Conv2d-30           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-31           [-1, 1024, 8, 8]           2,048\n",
      "           Conv2d-32           [-1, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-33           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-34           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-35            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-36            [-1, 256, 8, 8]             512\n",
      "           Conv2d-37            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-38            [-1, 256, 8, 8]             512\n",
      "           Conv2d-39           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-40           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-41           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-42            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-43            [-1, 256, 8, 8]             512\n",
      "           Conv2d-44            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 8, 8]             512\n",
      "           Conv2d-46           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-47           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-48           [-1, 1024, 8, 8]               0\n",
      "           Linear-49                   [-1, 10]          40,970\n",
      "================================================================\n",
      "Total params: 4,649,098\n",
      "Trainable params: 4,649,098\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 25.25\n",
      "Params size (MB): 17.73\n",
      "Estimated Total Size (MB): 43.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = project1_model().to(device)\n",
    "summary(model, (3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d711ca1c-34bf-4249-9293-2bca3a2ec9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetParams:\n",
    "   \"\"\"\n",
    "    A class to pass the hyperparameters to the model\n",
    "   \"\"\"\n",
    "   def __init__(self, arch='Model 1' ,epochs=100, start_epoch=0, batch_size=64, lr=0.1, momentum=0.9, weight_decay=1e-4, print_freq=50,\n",
    "                save_dir='save_temporary_checkpoints', save_every=10):\n",
    "        self.save_every = save_every #Saves checkpoints at every specified number of epochs\n",
    "        self.save_dir = save_dir #The directory used to save the trained models\n",
    "        self.print_freq = print_freq #print frequency \n",
    "        self.weight_decay = weight_decay #Weight decay for SGD\n",
    "        self.momentum = momentum #Momentum for SGD\n",
    "        self.lr = lr #Learning Rate\n",
    "        self.batch_size = batch_size #Batch Size for each epoch \n",
    "        self.start_epoch = start_epoch #Starting Epoch\n",
    "        self.epochs = epochs #Total Epochs\n",
    "        self.arch = arch #ResNet model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2d70bc-b3c1-4687-bbf4-971a161b586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs():\n",
    "    global args, best_precision\n",
    "    #Check if the save_dir exists or not\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    #Loading the model \n",
    "    model = project1_model()\n",
    "    model.cuda()\n",
    "\n",
    "    #Defining the Loss Function\n",
    "    loss_func = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    #Defining the Optimizer\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "    #                             momentum=args.momentum,\n",
    "    #                             weight_decay=args.weight_decay)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), args.lr, betas=(0.9, 0.99))\n",
    "    \n",
    "    #Defining the Learning Rate Scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                        milestones=[100, 150], last_epoch=args.start_epoch - 1)\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        #Train for one epoch\n",
    "        print('Training model: {}'.format(args.arch))\n",
    "        print('Current Learning Rate {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
    "        train(train_loader, model, loss_func, optimizer, epoch)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        #Test for one epoch\n",
    "        precision = validate(val_loader, model, loss_func)\n",
    "\n",
    "        #Save the best precision and make a checkpoint\n",
    "        is_best = precision > best_precision\n",
    "        best_precision = max(precision, best_precision)\n",
    "        if epoch > 0 and epoch % args.save_every == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(args.save_dir, 'project1_model_checkpoint.th'))\n",
    "        if is_best:\n",
    "            torch.save(model.state_dict(), os.path.join(args.save_dir, 'project1_model.th'))\n",
    "    return best_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c6ca600-7ed6-491a-bb1c-c80fc1446897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeepAverages(object):\n",
    "    #Computes and stores the average along with the current value\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19f4ab67-10a8-4f19-a7b8-b7dddeca5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    #Computes the top 1 precision\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def validate(val_loader, model, loss_func):\n",
    "    #Run an Evaluation\n",
    "    batch_time = KeepAverages()\n",
    "    losses = KeepAverages()\n",
    "    top1 = KeepAverages()\n",
    "\n",
    "    #Switch to Evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            input_var = input.cuda()\n",
    "            target_var = target.cuda()\n",
    "\n",
    "            #Compute the output of the Model and calculate the Loss\n",
    "            output = model(input_var)\n",
    "            loss = loss_func(output, target_var)\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            #Measure the Loss and Update it \n",
    "            precision = accuracy(output.data, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(precision.item(), input.size(0))\n",
    "\n",
    "            #Measure the elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "\n",
    "    print('Test Accuracy\\t  Top Precision: {top1.avg:.3f} (Error: {error:.3f} )\\n'\n",
    "          .format(top1=top1,error=100-top1.avg))\n",
    "    val_losses.append(100-top1.avg)\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "648d79b4-acfe-4461-9592-6a8126bddf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, loss_func, optimizer, epoch):\n",
    "    #Run one training epoch\n",
    "\n",
    "    batch_time = KeepAverages()\n",
    "    data_time = KeepAverages()\n",
    "    losses = KeepAverages()\n",
    "    top1 = KeepAverages()\n",
    "\n",
    "    #Switch to Train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # Measure the data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        target = target.cuda()\n",
    "        input_var = input.cuda()\n",
    "        target_var = target\n",
    "\n",
    "        #Compute the output and the Loss\n",
    "        output = model(input_var)\n",
    "        loss = loss_func(output, target_var)\n",
    "\n",
    "        #Compute the Gradient and do an SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        \n",
    "        #Measure the accuracy and record the loss\n",
    "        precision = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(precision.item(), input.size(0))\n",
    "\n",
    "        #Measure the Elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: No: [{0}] Batches: [{1}/{2}]\\t'\n",
    "                  'Loss: {loss.val:.4f} (Average: {loss.avg:.4f})\\t'\n",
    "                  'Precision: {top1.val:.3f} (Average: {top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))\n",
    "    train_losses.append(100-top1.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ebf3777-92f6-4855-baa8-0df7af075742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "  #Function to show an image\n",
    "  %matplotlib inline\n",
    "  %config InlineBackend.figure_format = 'retina'\n",
    "  img = img / 2 + 0.5     # un - Normalize\n",
    "  npimg = img.numpy()\n",
    "  plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "858ed804-c53a-478d-bb30-7bb9a7d397dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [0] Batches: [0/782]\tLoss: 2.5410 (Average: 2.5410)\tPrecision: 9.375 (Average: 9.375)\n",
      "Epoch: No: [0] Batches: [50/782]\tLoss: 2.2664 (Average: 19.5953)\tPrecision: 7.812 (Average: 10.233)\n",
      "Epoch: No: [0] Batches: [100/782]\tLoss: 2.1774 (Average: 10.9985)\tPrecision: 10.938 (Average: 13.258)\n",
      "Epoch: No: [0] Batches: [150/782]\tLoss: 2.1955 (Average: 8.0797)\tPrecision: 17.188 (Average: 14.983)\n",
      "Epoch: No: [0] Batches: [200/782]\tLoss: 1.9355 (Average: 6.5873)\tPrecision: 28.125 (Average: 16.884)\n",
      "Epoch: No: [0] Batches: [250/782]\tLoss: 1.9358 (Average: 5.6832)\tPrecision: 20.312 (Average: 17.941)\n",
      "Epoch: No: [0] Batches: [300/782]\tLoss: 1.9406 (Average: 5.0786)\tPrecision: 20.312 (Average: 18.812)\n",
      "Epoch: No: [0] Batches: [350/782]\tLoss: 1.9640 (Average: 4.6446)\tPrecision: 25.000 (Average: 19.694)\n",
      "Epoch: No: [0] Batches: [400/782]\tLoss: 1.8448 (Average: 4.3128)\tPrecision: 29.688 (Average: 20.453)\n",
      "Epoch: No: [0] Batches: [450/782]\tLoss: 1.8646 (Average: 4.0591)\tPrecision: 26.562 (Average: 21.005)\n",
      "Epoch: No: [0] Batches: [500/782]\tLoss: 1.8583 (Average: 3.8521)\tPrecision: 31.250 (Average: 21.532)\n",
      "Epoch: No: [0] Batches: [550/782]\tLoss: 1.9338 (Average: 3.6815)\tPrecision: 32.812 (Average: 21.986)\n",
      "Epoch: No: [0] Batches: [600/782]\tLoss: 2.2074 (Average: 3.5390)\tPrecision: 21.875 (Average: 22.463)\n",
      "Epoch: No: [0] Batches: [650/782]\tLoss: 1.9777 (Average: 3.4184)\tPrecision: 28.125 (Average: 22.854)\n",
      "Epoch: No: [0] Batches: [700/782]\tLoss: 1.9645 (Average: 3.3119)\tPrecision: 23.438 (Average: 23.288)\n",
      "Epoch: No: [0] Batches: [750/782]\tLoss: 2.0105 (Average: 3.2225)\tPrecision: 26.562 (Average: 23.591)\n",
      "Test Accuracy\t  Top Precision: 27.640 (Error: 72.360 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [1] Batches: [0/782]\tLoss: 1.9674 (Average: 1.9674)\tPrecision: 25.000 (Average: 25.000)\n",
      "Epoch: No: [1] Batches: [50/782]\tLoss: 1.8060 (Average: 1.9110)\tPrecision: 28.125 (Average: 29.105)\n",
      "Epoch: No: [1] Batches: [100/782]\tLoss: 1.9446 (Average: 1.9057)\tPrecision: 25.000 (Average: 29.239)\n",
      "Epoch: No: [1] Batches: [150/782]\tLoss: 1.9624 (Average: 1.8940)\tPrecision: 21.875 (Average: 29.315)\n",
      "Epoch: No: [1] Batches: [200/782]\tLoss: 1.8031 (Average: 1.8944)\tPrecision: 32.812 (Average: 29.283)\n",
      "Epoch: No: [1] Batches: [250/782]\tLoss: 1.9013 (Average: 1.8908)\tPrecision: 28.125 (Average: 29.638)\n",
      "Epoch: No: [1] Batches: [300/782]\tLoss: 1.8725 (Average: 1.8837)\tPrecision: 37.500 (Average: 29.906)\n",
      "Epoch: No: [1] Batches: [350/782]\tLoss: 1.7094 (Average: 1.8757)\tPrecision: 28.125 (Average: 29.977)\n",
      "Epoch: No: [1] Batches: [400/782]\tLoss: 2.0006 (Average: 1.8759)\tPrecision: 17.188 (Average: 29.913)\n",
      "Epoch: No: [1] Batches: [450/782]\tLoss: 1.6659 (Average: 1.8727)\tPrecision: 42.188 (Average: 30.020)\n",
      "Epoch: No: [1] Batches: [500/782]\tLoss: 1.6989 (Average: 1.8680)\tPrecision: 34.375 (Average: 30.133)\n",
      "Epoch: No: [1] Batches: [550/782]\tLoss: 1.7466 (Average: 1.8654)\tPrecision: 35.938 (Average: 30.283)\n",
      "Epoch: No: [1] Batches: [600/782]\tLoss: 1.7261 (Average: 1.8654)\tPrecision: 37.500 (Average: 30.244)\n",
      "Epoch: No: [1] Batches: [650/782]\tLoss: 1.8630 (Average: 1.8616)\tPrecision: 35.938 (Average: 30.309)\n",
      "Epoch: No: [1] Batches: [700/782]\tLoss: 1.7315 (Average: 1.8597)\tPrecision: 31.250 (Average: 30.303)\n",
      "Epoch: No: [1] Batches: [750/782]\tLoss: 1.7908 (Average: 1.8577)\tPrecision: 28.125 (Average: 30.380)\n",
      "Test Accuracy\t  Top Precision: 34.170 (Error: 65.830 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [2] Batches: [0/782]\tLoss: 1.8943 (Average: 1.8943)\tPrecision: 23.438 (Average: 23.438)\n",
      "Epoch: No: [2] Batches: [50/782]\tLoss: 1.8256 (Average: 1.8147)\tPrecision: 28.125 (Average: 30.913)\n",
      "Epoch: No: [2] Batches: [100/782]\tLoss: 1.8177 (Average: 1.8161)\tPrecision: 34.375 (Average: 31.327)\n",
      "Epoch: No: [2] Batches: [150/782]\tLoss: 1.8264 (Average: 1.8129)\tPrecision: 32.812 (Average: 31.809)\n",
      "Epoch: No: [2] Batches: [200/782]\tLoss: 1.8198 (Average: 1.8102)\tPrecision: 32.812 (Average: 31.810)\n",
      "Epoch: No: [2] Batches: [250/782]\tLoss: 1.5479 (Average: 1.8074)\tPrecision: 39.062 (Average: 31.904)\n",
      "Epoch: No: [2] Batches: [300/782]\tLoss: 1.8965 (Average: 1.8004)\tPrecision: 26.562 (Average: 32.091)\n",
      "Epoch: No: [2] Batches: [350/782]\tLoss: 1.7111 (Average: 1.8008)\tPrecision: 39.062 (Average: 32.140)\n",
      "Epoch: No: [2] Batches: [400/782]\tLoss: 1.7253 (Average: 1.8020)\tPrecision: 31.250 (Average: 32.138)\n",
      "Epoch: No: [2] Batches: [450/782]\tLoss: 1.7738 (Average: 1.8002)\tPrecision: 37.500 (Average: 32.262)\n",
      "Epoch: No: [2] Batches: [500/782]\tLoss: 1.6982 (Average: 1.7955)\tPrecision: 35.938 (Average: 32.572)\n",
      "Epoch: No: [2] Batches: [550/782]\tLoss: 1.7647 (Average: 1.7966)\tPrecision: 37.500 (Average: 32.659)\n",
      "Epoch: No: [2] Batches: [600/782]\tLoss: 1.8616 (Average: 1.7942)\tPrecision: 25.000 (Average: 32.615)\n",
      "Epoch: No: [2] Batches: [650/782]\tLoss: 1.5642 (Average: 1.7882)\tPrecision: 39.062 (Average: 32.849)\n",
      "Epoch: No: [2] Batches: [700/782]\tLoss: 1.7542 (Average: 1.7880)\tPrecision: 35.938 (Average: 32.777)\n",
      "Epoch: No: [2] Batches: [750/782]\tLoss: 1.9679 (Average: 1.7844)\tPrecision: 28.125 (Average: 32.927)\n",
      "Test Accuracy\t  Top Precision: 35.310 (Error: 64.690 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [3] Batches: [0/782]\tLoss: 1.7583 (Average: 1.7583)\tPrecision: 34.375 (Average: 34.375)\n",
      "Epoch: No: [3] Batches: [50/782]\tLoss: 1.7934 (Average: 1.7154)\tPrecision: 32.812 (Average: 35.447)\n",
      "Epoch: No: [3] Batches: [100/782]\tLoss: 1.7599 (Average: 1.7260)\tPrecision: 32.812 (Average: 35.520)\n",
      "Epoch: No: [3] Batches: [150/782]\tLoss: 1.6250 (Average: 1.7246)\tPrecision: 37.500 (Average: 35.596)\n",
      "Epoch: No: [3] Batches: [200/782]\tLoss: 1.7790 (Average: 1.7329)\tPrecision: 25.000 (Average: 35.215)\n",
      "Epoch: No: [3] Batches: [250/782]\tLoss: 1.9013 (Average: 1.7318)\tPrecision: 25.000 (Average: 35.016)\n",
      "Epoch: No: [3] Batches: [300/782]\tLoss: 1.6631 (Average: 1.7248)\tPrecision: 39.062 (Average: 35.366)\n",
      "Epoch: No: [3] Batches: [350/782]\tLoss: 1.6703 (Average: 1.7240)\tPrecision: 39.062 (Average: 35.466)\n",
      "Epoch: No: [3] Batches: [400/782]\tLoss: 1.6975 (Average: 1.7222)\tPrecision: 42.188 (Average: 35.439)\n",
      "Epoch: No: [3] Batches: [450/782]\tLoss: 1.5314 (Average: 1.7176)\tPrecision: 34.375 (Average: 35.556)\n",
      "Epoch: No: [3] Batches: [500/782]\tLoss: 1.4847 (Average: 1.7135)\tPrecision: 45.312 (Average: 35.644)\n",
      "Epoch: No: [3] Batches: [550/782]\tLoss: 2.0566 (Average: 1.7113)\tPrecision: 23.438 (Average: 35.728)\n",
      "Epoch: No: [3] Batches: [600/782]\tLoss: 1.6452 (Average: 1.7100)\tPrecision: 40.625 (Average: 35.691)\n",
      "Epoch: No: [3] Batches: [650/782]\tLoss: 1.7218 (Average: 1.7085)\tPrecision: 40.625 (Average: 35.678)\n",
      "Epoch: No: [3] Batches: [700/782]\tLoss: 1.6112 (Average: 1.7056)\tPrecision: 40.625 (Average: 35.822)\n",
      "Epoch: No: [3] Batches: [750/782]\tLoss: 1.5957 (Average: 1.7017)\tPrecision: 39.062 (Average: 36.037)\n",
      "Test Accuracy\t  Top Precision: 41.540 (Error: 58.460 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [4] Batches: [0/782]\tLoss: 1.7992 (Average: 1.7992)\tPrecision: 37.500 (Average: 37.500)\n",
      "Epoch: No: [4] Batches: [50/782]\tLoss: 1.5348 (Average: 1.6657)\tPrecision: 40.625 (Average: 37.040)\n",
      "Epoch: No: [4] Batches: [100/782]\tLoss: 1.5132 (Average: 1.6555)\tPrecision: 42.188 (Average: 37.283)\n",
      "Epoch: No: [4] Batches: [150/782]\tLoss: 1.6672 (Average: 1.6624)\tPrecision: 43.750 (Average: 37.303)\n",
      "Epoch: No: [4] Batches: [200/782]\tLoss: 1.5993 (Average: 1.6597)\tPrecision: 37.500 (Average: 37.554)\n",
      "Epoch: No: [4] Batches: [250/782]\tLoss: 1.7293 (Average: 1.6610)\tPrecision: 34.375 (Average: 37.743)\n",
      "Epoch: No: [4] Batches: [300/782]\tLoss: 1.6546 (Average: 1.6646)\tPrecision: 34.375 (Average: 37.775)\n",
      "Epoch: No: [4] Batches: [350/782]\tLoss: 1.7789 (Average: 1.6629)\tPrecision: 34.375 (Average: 37.812)\n",
      "Epoch: No: [4] Batches: [400/782]\tLoss: 1.6416 (Average: 1.6596)\tPrecision: 39.062 (Average: 37.890)\n",
      "Epoch: No: [4] Batches: [450/782]\tLoss: 1.6974 (Average: 1.6568)\tPrecision: 40.625 (Average: 38.037)\n",
      "Epoch: No: [4] Batches: [500/782]\tLoss: 1.5745 (Average: 1.6560)\tPrecision: 40.625 (Average: 38.015)\n",
      "Epoch: No: [4] Batches: [550/782]\tLoss: 1.8928 (Average: 1.6567)\tPrecision: 40.625 (Average: 37.971)\n",
      "Epoch: No: [4] Batches: [600/782]\tLoss: 1.7336 (Average: 1.6561)\tPrecision: 34.375 (Average: 38.095)\n",
      "Epoch: No: [4] Batches: [650/782]\tLoss: 1.7703 (Average: 1.6553)\tPrecision: 39.062 (Average: 38.237)\n",
      "Epoch: No: [4] Batches: [700/782]\tLoss: 1.5622 (Average: 1.6530)\tPrecision: 40.625 (Average: 38.305)\n",
      "Epoch: No: [4] Batches: [750/782]\tLoss: 1.8044 (Average: 1.6544)\tPrecision: 26.562 (Average: 38.157)\n",
      "Test Accuracy\t  Top Precision: 39.560 (Error: 60.440 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [5] Batches: [0/782]\tLoss: 1.5300 (Average: 1.5300)\tPrecision: 37.500 (Average: 37.500)\n",
      "Epoch: No: [5] Batches: [50/782]\tLoss: 1.6600 (Average: 1.6054)\tPrecision: 32.812 (Average: 39.798)\n",
      "Epoch: No: [5] Batches: [100/782]\tLoss: 1.2594 (Average: 1.5923)\tPrecision: 51.562 (Average: 40.254)\n",
      "Epoch: No: [5] Batches: [150/782]\tLoss: 1.4772 (Average: 1.5955)\tPrecision: 43.750 (Average: 40.346)\n",
      "Epoch: No: [5] Batches: [200/782]\tLoss: 1.6810 (Average: 1.6014)\tPrecision: 34.375 (Average: 40.353)\n",
      "Epoch: No: [5] Batches: [250/782]\tLoss: 1.8275 (Average: 1.6048)\tPrecision: 35.938 (Average: 40.158)\n",
      "Epoch: No: [5] Batches: [300/782]\tLoss: 1.3340 (Average: 1.6084)\tPrecision: 50.000 (Average: 40.054)\n",
      "Epoch: No: [5] Batches: [350/782]\tLoss: 1.5630 (Average: 1.6066)\tPrecision: 45.312 (Average: 40.028)\n",
      "Epoch: No: [5] Batches: [400/782]\tLoss: 1.3381 (Average: 1.6011)\tPrecision: 51.562 (Average: 40.270)\n",
      "Epoch: No: [5] Batches: [450/782]\tLoss: 1.5274 (Average: 1.5992)\tPrecision: 42.188 (Average: 40.220)\n",
      "Epoch: No: [5] Batches: [500/782]\tLoss: 1.7407 (Average: 1.5948)\tPrecision: 32.812 (Average: 40.276)\n",
      "Epoch: No: [5] Batches: [550/782]\tLoss: 1.6020 (Average: 1.5935)\tPrecision: 37.500 (Average: 40.358)\n",
      "Epoch: No: [5] Batches: [600/782]\tLoss: 1.4535 (Average: 1.5912)\tPrecision: 56.250 (Average: 40.578)\n",
      "Epoch: No: [5] Batches: [650/782]\tLoss: 1.6835 (Average: 1.5909)\tPrecision: 39.062 (Average: 40.601)\n",
      "Epoch: No: [5] Batches: [700/782]\tLoss: 1.7341 (Average: 1.5880)\tPrecision: 37.500 (Average: 40.670)\n",
      "Epoch: No: [5] Batches: [750/782]\tLoss: 1.4140 (Average: 1.5864)\tPrecision: 43.750 (Average: 40.689)\n",
      "Test Accuracy\t  Top Precision: 45.810 (Error: 54.190 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [6] Batches: [0/782]\tLoss: 1.4045 (Average: 1.4045)\tPrecision: 48.438 (Average: 48.438)\n",
      "Epoch: No: [6] Batches: [50/782]\tLoss: 1.4964 (Average: 1.5513)\tPrecision: 35.938 (Average: 41.422)\n",
      "Epoch: No: [6] Batches: [100/782]\tLoss: 1.4233 (Average: 1.5460)\tPrecision: 54.688 (Average: 41.832)\n",
      "Epoch: No: [6] Batches: [150/782]\tLoss: 1.5862 (Average: 1.5357)\tPrecision: 43.750 (Average: 42.632)\n",
      "Epoch: No: [6] Batches: [200/782]\tLoss: 1.5659 (Average: 1.5309)\tPrecision: 42.188 (Average: 42.879)\n",
      "Epoch: No: [6] Batches: [250/782]\tLoss: 1.6502 (Average: 1.5328)\tPrecision: 39.062 (Average: 42.972)\n",
      "Epoch: No: [6] Batches: [300/782]\tLoss: 1.4488 (Average: 1.5321)\tPrecision: 42.188 (Average: 43.013)\n",
      "Epoch: No: [6] Batches: [350/782]\tLoss: 1.9040 (Average: 1.5371)\tPrecision: 31.250 (Average: 42.842)\n",
      "Epoch: No: [6] Batches: [400/782]\tLoss: 1.5158 (Average: 1.5353)\tPrecision: 46.875 (Average: 42.873)\n",
      "Epoch: No: [6] Batches: [450/782]\tLoss: 1.4331 (Average: 1.5311)\tPrecision: 54.688 (Average: 43.099)\n",
      "Epoch: No: [6] Batches: [500/782]\tLoss: 1.3855 (Average: 1.5322)\tPrecision: 46.875 (Average: 43.101)\n",
      "Epoch: No: [6] Batches: [550/782]\tLoss: 1.4956 (Average: 1.5289)\tPrecision: 48.438 (Average: 43.166)\n",
      "Epoch: No: [6] Batches: [600/782]\tLoss: 1.6702 (Average: 1.5279)\tPrecision: 32.812 (Average: 43.230)\n",
      "Epoch: No: [6] Batches: [650/782]\tLoss: 1.4828 (Average: 1.5272)\tPrecision: 35.938 (Average: 43.236)\n",
      "Epoch: No: [6] Batches: [700/782]\tLoss: 1.3346 (Average: 1.5241)\tPrecision: 59.375 (Average: 43.409)\n",
      "Epoch: No: [6] Batches: [750/782]\tLoss: 1.3579 (Average: 1.5180)\tPrecision: 56.250 (Average: 43.623)\n",
      "Test Accuracy\t  Top Precision: 43.150 (Error: 56.850 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [7] Batches: [0/782]\tLoss: 1.4629 (Average: 1.4629)\tPrecision: 46.875 (Average: 46.875)\n",
      "Epoch: No: [7] Batches: [50/782]\tLoss: 1.4730 (Average: 1.4955)\tPrecision: 46.875 (Average: 44.485)\n",
      "Epoch: No: [7] Batches: [100/782]\tLoss: 1.4411 (Average: 1.4920)\tPrecision: 45.312 (Average: 44.524)\n",
      "Epoch: No: [7] Batches: [150/782]\tLoss: 1.5491 (Average: 1.4867)\tPrecision: 43.750 (Average: 44.826)\n",
      "Epoch: No: [7] Batches: [200/782]\tLoss: 1.5517 (Average: 1.4791)\tPrecision: 42.188 (Average: 45.180)\n",
      "Epoch: No: [7] Batches: [250/782]\tLoss: 1.6158 (Average: 1.4736)\tPrecision: 42.188 (Average: 45.356)\n",
      "Epoch: No: [7] Batches: [300/782]\tLoss: 1.4068 (Average: 1.4731)\tPrecision: 50.000 (Average: 45.250)\n",
      "Epoch: No: [7] Batches: [350/782]\tLoss: 1.4137 (Average: 1.4664)\tPrecision: 43.750 (Average: 45.477)\n",
      "Epoch: No: [7] Batches: [400/782]\tLoss: 1.6372 (Average: 1.4603)\tPrecision: 39.062 (Average: 45.593)\n",
      "Epoch: No: [7] Batches: [450/782]\tLoss: 1.2980 (Average: 1.4556)\tPrecision: 59.375 (Average: 45.707)\n",
      "Epoch: No: [7] Batches: [500/782]\tLoss: 1.3775 (Average: 1.4509)\tPrecision: 48.438 (Average: 45.846)\n",
      "Epoch: No: [7] Batches: [550/782]\tLoss: 1.4368 (Average: 1.4488)\tPrecision: 45.312 (Average: 45.985)\n",
      "Epoch: No: [7] Batches: [600/782]\tLoss: 1.5647 (Average: 1.4443)\tPrecision: 43.750 (Average: 46.079)\n",
      "Epoch: No: [7] Batches: [650/782]\tLoss: 1.3809 (Average: 1.4430)\tPrecision: 43.750 (Average: 46.136)\n",
      "Epoch: No: [7] Batches: [700/782]\tLoss: 1.3919 (Average: 1.4401)\tPrecision: 45.312 (Average: 46.224)\n",
      "Epoch: No: [7] Batches: [750/782]\tLoss: 1.5008 (Average: 1.4362)\tPrecision: 53.125 (Average: 46.403)\n",
      "Test Accuracy\t  Top Precision: 47.430 (Error: 52.570 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [8] Batches: [0/782]\tLoss: 1.2253 (Average: 1.2253)\tPrecision: 59.375 (Average: 59.375)\n",
      "Epoch: No: [8] Batches: [50/782]\tLoss: 1.4905 (Average: 1.4170)\tPrecision: 46.875 (Average: 46.569)\n",
      "Epoch: No: [8] Batches: [100/782]\tLoss: 1.2186 (Average: 1.4063)\tPrecision: 57.812 (Average: 47.788)\n",
      "Epoch: No: [8] Batches: [150/782]\tLoss: 1.4421 (Average: 1.4122)\tPrecision: 48.438 (Average: 47.682)\n",
      "Epoch: No: [8] Batches: [200/782]\tLoss: 1.6070 (Average: 1.4026)\tPrecision: 37.500 (Average: 47.886)\n",
      "Epoch: No: [8] Batches: [250/782]\tLoss: 1.4182 (Average: 1.3966)\tPrecision: 45.312 (Average: 48.332)\n",
      "Epoch: No: [8] Batches: [300/782]\tLoss: 1.3762 (Average: 1.3881)\tPrecision: 48.438 (Average: 48.707)\n",
      "Epoch: No: [8] Batches: [350/782]\tLoss: 1.7022 (Average: 1.3836)\tPrecision: 42.188 (Average: 48.749)\n",
      "Epoch: No: [8] Batches: [400/782]\tLoss: 1.6097 (Average: 1.3834)\tPrecision: 40.625 (Average: 48.702)\n",
      "Epoch: No: [8] Batches: [450/782]\tLoss: 1.3497 (Average: 1.3800)\tPrecision: 53.125 (Average: 48.877)\n",
      "Epoch: No: [8] Batches: [500/782]\tLoss: 1.3595 (Average: 1.3792)\tPrecision: 53.125 (Average: 48.890)\n",
      "Epoch: No: [8] Batches: [550/782]\tLoss: 1.4890 (Average: 1.3783)\tPrecision: 43.750 (Average: 48.928)\n",
      "Epoch: No: [8] Batches: [600/782]\tLoss: 1.4960 (Average: 1.3782)\tPrecision: 46.875 (Average: 48.999)\n",
      "Epoch: No: [8] Batches: [650/782]\tLoss: 1.3854 (Average: 1.3762)\tPrecision: 48.438 (Average: 49.124)\n",
      "Epoch: No: [8] Batches: [700/782]\tLoss: 1.7766 (Average: 1.3790)\tPrecision: 43.750 (Average: 49.064)\n",
      "Epoch: No: [8] Batches: [750/782]\tLoss: 1.3894 (Average: 1.3774)\tPrecision: 54.688 (Average: 49.134)\n",
      "Test Accuracy\t  Top Precision: 49.910 (Error: 50.090 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [9] Batches: [0/782]\tLoss: 1.6062 (Average: 1.6062)\tPrecision: 45.312 (Average: 45.312)\n",
      "Epoch: No: [9] Batches: [50/782]\tLoss: 1.1385 (Average: 1.3866)\tPrecision: 51.562 (Average: 48.866)\n",
      "Epoch: No: [9] Batches: [100/782]\tLoss: 1.0964 (Average: 1.3652)\tPrecision: 54.688 (Average: 49.675)\n",
      "Epoch: No: [9] Batches: [150/782]\tLoss: 1.4133 (Average: 1.3595)\tPrecision: 54.688 (Average: 49.752)\n",
      "Epoch: No: [9] Batches: [200/782]\tLoss: 1.3315 (Average: 1.3449)\tPrecision: 51.562 (Average: 50.047)\n",
      "Epoch: No: [9] Batches: [250/782]\tLoss: 1.5239 (Average: 1.3438)\tPrecision: 42.188 (Average: 50.255)\n",
      "Epoch: No: [9] Batches: [300/782]\tLoss: 1.2257 (Average: 1.3359)\tPrecision: 50.000 (Average: 50.654)\n",
      "Epoch: No: [9] Batches: [350/782]\tLoss: 1.5130 (Average: 1.3366)\tPrecision: 48.438 (Average: 50.619)\n",
      "Epoch: No: [9] Batches: [400/782]\tLoss: 1.1260 (Average: 1.3356)\tPrecision: 57.812 (Average: 50.604)\n",
      "Epoch: No: [9] Batches: [450/782]\tLoss: 1.4494 (Average: 1.3331)\tPrecision: 48.438 (Average: 50.734)\n",
      "Epoch: No: [9] Batches: [500/782]\tLoss: 1.4969 (Average: 1.3296)\tPrecision: 45.312 (Average: 50.842)\n",
      "Epoch: No: [9] Batches: [550/782]\tLoss: 1.3737 (Average: 1.3295)\tPrecision: 51.562 (Average: 50.828)\n",
      "Epoch: No: [9] Batches: [600/782]\tLoss: 1.0749 (Average: 1.3288)\tPrecision: 59.375 (Average: 50.858)\n",
      "Epoch: No: [9] Batches: [650/782]\tLoss: 1.1167 (Average: 1.3318)\tPrecision: 53.125 (Average: 50.833)\n",
      "Epoch: No: [9] Batches: [700/782]\tLoss: 1.3452 (Average: 1.3293)\tPrecision: 57.812 (Average: 50.974)\n",
      "Epoch: No: [9] Batches: [750/782]\tLoss: 1.4485 (Average: 1.3254)\tPrecision: 50.000 (Average: 51.111)\n",
      "Test Accuracy\t  Top Precision: 51.820 (Error: 48.180 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [10] Batches: [0/782]\tLoss: 1.3565 (Average: 1.3565)\tPrecision: 51.562 (Average: 51.562)\n",
      "Epoch: No: [10] Batches: [50/782]\tLoss: 1.3960 (Average: 1.3035)\tPrecision: 54.688 (Average: 52.328)\n",
      "Epoch: No: [10] Batches: [100/782]\tLoss: 1.2943 (Average: 1.2976)\tPrecision: 45.312 (Average: 52.088)\n",
      "Epoch: No: [10] Batches: [150/782]\tLoss: 1.0506 (Average: 1.2904)\tPrecision: 64.062 (Average: 52.701)\n",
      "Epoch: No: [10] Batches: [200/782]\tLoss: 1.3781 (Average: 1.3008)\tPrecision: 51.562 (Average: 52.503)\n",
      "Epoch: No: [10] Batches: [250/782]\tLoss: 1.2485 (Average: 1.3011)\tPrecision: 53.125 (Average: 52.328)\n",
      "Epoch: No: [10] Batches: [300/782]\tLoss: 1.2588 (Average: 1.3036)\tPrecision: 53.125 (Average: 52.372)\n",
      "Epoch: No: [10] Batches: [350/782]\tLoss: 1.3315 (Average: 1.3045)\tPrecision: 54.688 (Average: 52.390)\n",
      "Epoch: No: [10] Batches: [400/782]\tLoss: 1.0810 (Average: 1.3062)\tPrecision: 65.625 (Average: 52.420)\n",
      "Epoch: No: [10] Batches: [450/782]\tLoss: 1.1085 (Average: 1.3023)\tPrecision: 60.938 (Average: 52.494)\n",
      "Epoch: No: [10] Batches: [500/782]\tLoss: 1.2623 (Average: 1.2971)\tPrecision: 51.562 (Average: 52.735)\n",
      "Epoch: No: [10] Batches: [550/782]\tLoss: 1.4415 (Average: 1.2949)\tPrecision: 48.438 (Average: 52.785)\n",
      "Epoch: No: [10] Batches: [600/782]\tLoss: 0.9547 (Average: 1.2929)\tPrecision: 67.188 (Average: 52.818)\n",
      "Epoch: No: [10] Batches: [650/782]\tLoss: 1.0665 (Average: 1.2932)\tPrecision: 65.625 (Average: 52.700)\n",
      "Epoch: No: [10] Batches: [700/782]\tLoss: 1.3608 (Average: 1.2903)\tPrecision: 54.688 (Average: 52.851)\n",
      "Epoch: No: [10] Batches: [750/782]\tLoss: 1.3280 (Average: 1.2900)\tPrecision: 54.688 (Average: 52.867)\n",
      "Test Accuracy\t  Top Precision: 50.940 (Error: 49.060 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [11] Batches: [0/782]\tLoss: 1.1619 (Average: 1.1619)\tPrecision: 54.688 (Average: 54.688)\n",
      "Epoch: No: [11] Batches: [50/782]\tLoss: 1.0585 (Average: 1.2666)\tPrecision: 64.062 (Average: 54.013)\n",
      "Epoch: No: [11] Batches: [100/782]\tLoss: 1.2524 (Average: 1.2539)\tPrecision: 56.250 (Average: 54.440)\n",
      "Epoch: No: [11] Batches: [150/782]\tLoss: 1.0582 (Average: 1.2524)\tPrecision: 62.500 (Average: 54.429)\n",
      "Epoch: No: [11] Batches: [200/782]\tLoss: 1.3369 (Average: 1.2652)\tPrecision: 48.438 (Average: 53.817)\n",
      "Epoch: No: [11] Batches: [250/782]\tLoss: 1.2299 (Average: 1.2603)\tPrecision: 57.812 (Average: 53.909)\n",
      "Epoch: No: [11] Batches: [300/782]\tLoss: 1.3119 (Average: 1.2626)\tPrecision: 42.188 (Average: 53.893)\n",
      "Epoch: No: [11] Batches: [350/782]\tLoss: 1.1287 (Average: 1.2544)\tPrecision: 59.375 (Average: 54.189)\n",
      "Epoch: No: [11] Batches: [400/782]\tLoss: 1.4142 (Average: 1.2568)\tPrecision: 45.312 (Average: 54.080)\n",
      "Epoch: No: [11] Batches: [450/782]\tLoss: 1.3605 (Average: 1.2569)\tPrecision: 50.000 (Average: 54.164)\n",
      "Epoch: No: [11] Batches: [500/782]\tLoss: 1.0660 (Average: 1.2562)\tPrecision: 60.938 (Average: 54.111)\n",
      "Epoch: No: [11] Batches: [550/782]\tLoss: 1.3416 (Average: 1.2548)\tPrecision: 53.125 (Average: 54.177)\n",
      "Epoch: No: [11] Batches: [600/782]\tLoss: 1.2924 (Average: 1.2521)\tPrecision: 50.000 (Average: 54.365)\n",
      "Epoch: No: [11] Batches: [650/782]\tLoss: 1.2684 (Average: 1.2509)\tPrecision: 48.438 (Average: 54.373)\n",
      "Epoch: No: [11] Batches: [700/782]\tLoss: 1.2918 (Average: 1.2458)\tPrecision: 54.688 (Average: 54.514)\n",
      "Epoch: No: [11] Batches: [750/782]\tLoss: 1.1744 (Average: 1.2482)\tPrecision: 57.812 (Average: 54.430)\n",
      "Test Accuracy\t  Top Precision: 56.420 (Error: 43.580 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [12] Batches: [0/782]\tLoss: 1.3392 (Average: 1.3392)\tPrecision: 42.188 (Average: 42.188)\n",
      "Epoch: No: [12] Batches: [50/782]\tLoss: 1.0576 (Average: 1.2319)\tPrecision: 59.375 (Average: 55.392)\n",
      "Epoch: No: [12] Batches: [100/782]\tLoss: 1.2483 (Average: 1.2431)\tPrecision: 60.938 (Average: 54.796)\n",
      "Epoch: No: [12] Batches: [150/782]\tLoss: 1.2502 (Average: 1.2335)\tPrecision: 53.125 (Average: 55.112)\n",
      "Epoch: No: [12] Batches: [200/782]\tLoss: 1.3918 (Average: 1.2291)\tPrecision: 53.125 (Average: 54.998)\n",
      "Epoch: No: [12] Batches: [250/782]\tLoss: 1.5059 (Average: 1.2299)\tPrecision: 51.562 (Average: 54.961)\n",
      "Epoch: No: [12] Batches: [300/782]\tLoss: 1.1927 (Average: 1.2287)\tPrecision: 54.688 (Average: 55.025)\n",
      "Epoch: No: [12] Batches: [350/782]\tLoss: 1.2316 (Average: 1.2266)\tPrecision: 54.688 (Average: 55.079)\n",
      "Epoch: No: [12] Batches: [400/782]\tLoss: 1.2911 (Average: 1.2299)\tPrecision: 46.875 (Average: 55.019)\n",
      "Epoch: No: [12] Batches: [450/782]\tLoss: 1.3861 (Average: 1.2266)\tPrecision: 46.875 (Average: 55.353)\n",
      "Epoch: No: [12] Batches: [500/782]\tLoss: 1.3202 (Average: 1.2259)\tPrecision: 53.125 (Average: 55.495)\n",
      "Epoch: No: [12] Batches: [550/782]\tLoss: 1.3211 (Average: 1.2233)\tPrecision: 51.562 (Average: 55.584)\n",
      "Epoch: No: [12] Batches: [600/782]\tLoss: 0.9838 (Average: 1.2207)\tPrecision: 64.062 (Average: 55.665)\n",
      "Epoch: No: [12] Batches: [650/782]\tLoss: 1.2016 (Average: 1.2184)\tPrecision: 59.375 (Average: 55.727)\n",
      "Epoch: No: [12] Batches: [700/782]\tLoss: 1.0360 (Average: 1.2155)\tPrecision: 62.500 (Average: 55.818)\n",
      "Epoch: No: [12] Batches: [750/782]\tLoss: 1.1360 (Average: 1.2171)\tPrecision: 59.375 (Average: 55.734)\n",
      "Test Accuracy\t  Top Precision: 53.990 (Error: 46.010 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [13] Batches: [0/782]\tLoss: 1.2174 (Average: 1.2174)\tPrecision: 59.375 (Average: 59.375)\n",
      "Epoch: No: [13] Batches: [50/782]\tLoss: 0.9448 (Average: 1.1748)\tPrecision: 65.625 (Average: 57.200)\n",
      "Epoch: No: [13] Batches: [100/782]\tLoss: 1.1557 (Average: 1.2027)\tPrecision: 48.438 (Average: 56.683)\n",
      "Epoch: No: [13] Batches: [150/782]\tLoss: 1.1735 (Average: 1.1925)\tPrecision: 51.562 (Average: 56.695)\n",
      "Epoch: No: [13] Batches: [200/782]\tLoss: 1.0809 (Average: 1.2013)\tPrecision: 59.375 (Average: 56.615)\n",
      "Epoch: No: [13] Batches: [250/782]\tLoss: 1.4510 (Average: 1.1981)\tPrecision: 43.750 (Average: 56.580)\n",
      "Epoch: No: [13] Batches: [300/782]\tLoss: 1.0222 (Average: 1.1940)\tPrecision: 65.625 (Average: 56.676)\n",
      "Epoch: No: [13] Batches: [350/782]\tLoss: 1.4550 (Average: 1.1946)\tPrecision: 48.438 (Average: 56.682)\n",
      "Epoch: No: [13] Batches: [400/782]\tLoss: 1.2810 (Average: 1.1929)\tPrecision: 54.688 (Average: 56.628)\n",
      "Epoch: No: [13] Batches: [450/782]\tLoss: 1.1726 (Average: 1.1943)\tPrecision: 50.000 (Average: 56.465)\n",
      "Epoch: No: [13] Batches: [500/782]\tLoss: 1.1216 (Average: 1.1937)\tPrecision: 57.812 (Average: 56.450)\n",
      "Epoch: No: [13] Batches: [550/782]\tLoss: 1.3457 (Average: 1.1972)\tPrecision: 51.562 (Average: 56.315)\n",
      "Epoch: No: [13] Batches: [600/782]\tLoss: 1.0752 (Average: 1.1964)\tPrecision: 53.125 (Average: 56.375)\n",
      "Epoch: No: [13] Batches: [650/782]\tLoss: 1.3908 (Average: 1.1967)\tPrecision: 53.125 (Average: 56.401)\n",
      "Epoch: No: [13] Batches: [700/782]\tLoss: 1.1610 (Average: 1.1951)\tPrecision: 54.688 (Average: 56.509)\n",
      "Epoch: No: [13] Batches: [750/782]\tLoss: 0.8937 (Average: 1.1945)\tPrecision: 70.312 (Average: 56.597)\n",
      "Test Accuracy\t  Top Precision: 48.190 (Error: 51.810 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [14] Batches: [0/782]\tLoss: 1.2734 (Average: 1.2734)\tPrecision: 50.000 (Average: 50.000)\n",
      "Epoch: No: [14] Batches: [50/782]\tLoss: 1.1570 (Average: 1.1939)\tPrecision: 54.688 (Average: 55.331)\n",
      "Epoch: No: [14] Batches: [100/782]\tLoss: 1.2667 (Average: 1.1922)\tPrecision: 60.938 (Average: 55.925)\n",
      "Epoch: No: [14] Batches: [150/782]\tLoss: 1.1700 (Average: 1.1835)\tPrecision: 54.688 (Average: 56.353)\n",
      "Epoch: No: [14] Batches: [200/782]\tLoss: 1.0892 (Average: 1.1877)\tPrecision: 54.688 (Average: 56.211)\n",
      "Epoch: No: [14] Batches: [250/782]\tLoss: 1.2662 (Average: 1.1901)\tPrecision: 53.125 (Average: 56.424)\n",
      "Epoch: No: [14] Batches: [300/782]\tLoss: 0.8180 (Average: 1.1763)\tPrecision: 67.188 (Average: 56.951)\n",
      "Epoch: No: [14] Batches: [350/782]\tLoss: 0.9986 (Average: 1.1802)\tPrecision: 56.250 (Average: 56.940)\n",
      "Epoch: No: [14] Batches: [400/782]\tLoss: 1.2312 (Average: 1.1725)\tPrecision: 57.812 (Average: 57.259)\n",
      "Epoch: No: [14] Batches: [450/782]\tLoss: 1.1175 (Average: 1.1622)\tPrecision: 65.625 (Average: 57.691)\n",
      "Epoch: No: [14] Batches: [500/782]\tLoss: 1.4347 (Average: 1.1631)\tPrecision: 48.438 (Average: 57.756)\n",
      "Epoch: No: [14] Batches: [550/782]\tLoss: 1.1828 (Average: 1.1663)\tPrecision: 64.062 (Average: 57.691)\n",
      "Epoch: No: [14] Batches: [600/782]\tLoss: 0.8580 (Average: 1.1638)\tPrecision: 64.062 (Average: 57.771)\n",
      "Epoch: No: [14] Batches: [650/782]\tLoss: 1.0029 (Average: 1.1629)\tPrecision: 67.188 (Average: 57.803)\n",
      "Epoch: No: [14] Batches: [700/782]\tLoss: 0.9288 (Average: 1.1617)\tPrecision: 68.750 (Average: 57.913)\n",
      "Epoch: No: [14] Batches: [750/782]\tLoss: 1.1412 (Average: 1.1599)\tPrecision: 59.375 (Average: 57.994)\n",
      "Test Accuracy\t  Top Precision: 59.620 (Error: 40.380 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [15] Batches: [0/782]\tLoss: 1.1547 (Average: 1.1547)\tPrecision: 64.062 (Average: 64.062)\n",
      "Epoch: No: [15] Batches: [50/782]\tLoss: 0.8925 (Average: 1.1674)\tPrecision: 78.125 (Average: 57.200)\n",
      "Epoch: No: [15] Batches: [100/782]\tLoss: 1.2162 (Average: 1.1534)\tPrecision: 54.688 (Average: 58.447)\n",
      "Epoch: No: [15] Batches: [150/782]\tLoss: 1.1865 (Average: 1.1480)\tPrecision: 51.562 (Average: 58.764)\n",
      "Epoch: No: [15] Batches: [200/782]\tLoss: 1.3118 (Average: 1.1368)\tPrecision: 54.688 (Average: 58.916)\n",
      "Epoch: No: [15] Batches: [250/782]\tLoss: 1.1747 (Average: 1.1362)\tPrecision: 56.250 (Average: 59.163)\n",
      "Epoch: No: [15] Batches: [300/782]\tLoss: 1.1144 (Average: 1.1379)\tPrecision: 54.688 (Average: 59.058)\n",
      "Epoch: No: [15] Batches: [350/782]\tLoss: 1.2587 (Average: 1.1485)\tPrecision: 57.812 (Average: 58.783)\n",
      "Epoch: No: [15] Batches: [400/782]\tLoss: 1.0216 (Average: 1.1438)\tPrecision: 60.938 (Average: 58.966)\n",
      "Epoch: No: [15] Batches: [450/782]\tLoss: 1.1433 (Average: 1.1416)\tPrecision: 54.688 (Average: 58.928)\n",
      "Epoch: No: [15] Batches: [500/782]\tLoss: 1.2957 (Average: 1.1361)\tPrecision: 54.688 (Average: 59.038)\n",
      "Epoch: No: [15] Batches: [550/782]\tLoss: 1.2930 (Average: 1.1371)\tPrecision: 50.000 (Average: 58.938)\n",
      "Epoch: No: [15] Batches: [600/782]\tLoss: 1.0366 (Average: 1.1394)\tPrecision: 59.375 (Average: 58.821)\n",
      "Epoch: No: [15] Batches: [650/782]\tLoss: 1.2402 (Average: 1.1416)\tPrecision: 51.562 (Average: 58.715)\n",
      "Epoch: No: [15] Batches: [700/782]\tLoss: 1.2663 (Average: 1.1411)\tPrecision: 59.375 (Average: 58.733)\n",
      "Epoch: No: [15] Batches: [750/782]\tLoss: 1.2472 (Average: 1.1400)\tPrecision: 53.125 (Average: 58.713)\n",
      "Test Accuracy\t  Top Precision: 55.780 (Error: 44.220 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [16] Batches: [0/782]\tLoss: 1.2315 (Average: 1.2315)\tPrecision: 56.250 (Average: 56.250)\n",
      "Epoch: No: [16] Batches: [50/782]\tLoss: 1.2642 (Average: 1.1791)\tPrecision: 54.688 (Average: 57.200)\n",
      "Epoch: No: [16] Batches: [100/782]\tLoss: 1.3059 (Average: 1.1489)\tPrecision: 53.125 (Average: 58.431)\n",
      "Epoch: No: [16] Batches: [150/782]\tLoss: 1.0017 (Average: 1.1262)\tPrecision: 64.062 (Average: 59.499)\n",
      "Epoch: No: [16] Batches: [200/782]\tLoss: 1.2396 (Average: 1.1218)\tPrecision: 54.688 (Average: 59.406)\n",
      "Epoch: No: [16] Batches: [250/782]\tLoss: 0.9217 (Average: 1.1271)\tPrecision: 70.312 (Average: 59.126)\n",
      "Epoch: No: [16] Batches: [300/782]\tLoss: 1.2451 (Average: 1.1237)\tPrecision: 56.250 (Average: 59.209)\n",
      "Epoch: No: [16] Batches: [350/782]\tLoss: 1.0644 (Average: 1.1220)\tPrecision: 65.625 (Average: 59.148)\n",
      "Epoch: No: [16] Batches: [400/782]\tLoss: 1.1035 (Average: 1.1210)\tPrecision: 56.250 (Average: 59.258)\n",
      "Epoch: No: [16] Batches: [450/782]\tLoss: 1.1391 (Average: 1.1264)\tPrecision: 59.375 (Average: 58.956)\n",
      "Epoch: No: [16] Batches: [500/782]\tLoss: 0.8172 (Average: 1.1252)\tPrecision: 71.875 (Average: 59.032)\n",
      "Epoch: No: [16] Batches: [550/782]\tLoss: 1.1064 (Average: 1.1239)\tPrecision: 54.688 (Average: 59.117)\n",
      "Epoch: No: [16] Batches: [600/782]\tLoss: 0.9060 (Average: 1.1244)\tPrecision: 62.500 (Average: 59.089)\n",
      "Epoch: No: [16] Batches: [650/782]\tLoss: 1.0704 (Average: 1.1228)\tPrecision: 51.562 (Average: 59.159)\n",
      "Epoch: No: [16] Batches: [700/782]\tLoss: 0.9116 (Average: 1.1203)\tPrecision: 68.750 (Average: 59.324)\n",
      "Epoch: No: [16] Batches: [750/782]\tLoss: 1.0589 (Average: 1.1187)\tPrecision: 62.500 (Average: 59.404)\n",
      "Test Accuracy\t  Top Precision: 58.060 (Error: 41.940 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [17] Batches: [0/782]\tLoss: 1.0994 (Average: 1.0994)\tPrecision: 67.188 (Average: 67.188)\n",
      "Epoch: No: [17] Batches: [50/782]\tLoss: 1.0679 (Average: 1.0941)\tPrecision: 59.375 (Average: 60.386)\n",
      "Epoch: No: [17] Batches: [100/782]\tLoss: 1.3553 (Average: 1.1035)\tPrecision: 51.562 (Average: 59.886)\n",
      "Epoch: No: [17] Batches: [150/782]\tLoss: 1.1159 (Average: 1.1050)\tPrecision: 62.500 (Average: 59.975)\n",
      "Epoch: No: [17] Batches: [200/782]\tLoss: 1.0665 (Average: 1.1006)\tPrecision: 57.812 (Average: 59.981)\n",
      "Epoch: No: [17] Batches: [250/782]\tLoss: 1.2010 (Average: 1.0984)\tPrecision: 54.688 (Average: 60.122)\n",
      "Epoch: No: [17] Batches: [300/782]\tLoss: 0.8958 (Average: 1.0941)\tPrecision: 75.000 (Average: 60.418)\n",
      "Epoch: No: [17] Batches: [350/782]\tLoss: 1.0455 (Average: 1.0944)\tPrecision: 67.188 (Average: 60.323)\n",
      "Epoch: No: [17] Batches: [400/782]\tLoss: 0.9011 (Average: 1.0877)\tPrecision: 64.062 (Average: 60.540)\n",
      "Epoch: No: [17] Batches: [450/782]\tLoss: 1.1011 (Average: 1.0861)\tPrecision: 57.812 (Average: 60.615)\n",
      "Epoch: No: [17] Batches: [500/782]\tLoss: 0.9576 (Average: 1.0823)\tPrecision: 68.750 (Average: 60.825)\n",
      "Epoch: No: [17] Batches: [550/782]\tLoss: 1.2230 (Average: 1.0842)\tPrecision: 64.062 (Average: 60.881)\n",
      "Epoch: No: [17] Batches: [600/782]\tLoss: 0.9324 (Average: 1.0823)\tPrecision: 67.188 (Average: 60.925)\n",
      "Epoch: No: [17] Batches: [650/782]\tLoss: 0.9456 (Average: 1.0795)\tPrecision: 65.625 (Average: 60.978)\n",
      "Epoch: No: [17] Batches: [700/782]\tLoss: 1.2306 (Average: 1.0793)\tPrecision: 59.375 (Average: 61.000)\n",
      "Epoch: No: [17] Batches: [750/782]\tLoss: 1.1527 (Average: 1.0798)\tPrecision: 54.688 (Average: 60.925)\n",
      "Test Accuracy\t  Top Precision: 59.810 (Error: 40.190 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [18] Batches: [0/782]\tLoss: 1.2730 (Average: 1.2730)\tPrecision: 56.250 (Average: 56.250)\n",
      "Epoch: No: [18] Batches: [50/782]\tLoss: 1.3590 (Average: 1.0715)\tPrecision: 51.562 (Average: 61.183)\n",
      "Epoch: No: [18] Batches: [100/782]\tLoss: 1.3702 (Average: 1.0655)\tPrecision: 57.812 (Average: 61.866)\n",
      "Epoch: No: [18] Batches: [150/782]\tLoss: 0.9220 (Average: 1.0643)\tPrecision: 62.500 (Average: 62.169)\n",
      "Epoch: No: [18] Batches: [200/782]\tLoss: 0.8842 (Average: 1.0664)\tPrecision: 70.312 (Average: 62.189)\n",
      "Epoch: No: [18] Batches: [250/782]\tLoss: 1.0603 (Average: 1.0713)\tPrecision: 65.625 (Average: 62.120)\n",
      "Epoch: No: [18] Batches: [300/782]\tLoss: 1.4619 (Average: 1.0719)\tPrecision: 50.000 (Average: 62.017)\n",
      "Epoch: No: [18] Batches: [350/782]\tLoss: 1.0633 (Average: 1.0702)\tPrecision: 59.375 (Average: 62.090)\n",
      "Epoch: No: [18] Batches: [400/782]\tLoss: 0.9983 (Average: 1.0701)\tPrecision: 68.750 (Average: 62.060)\n",
      "Epoch: No: [18] Batches: [450/782]\tLoss: 1.0231 (Average: 1.0631)\tPrecision: 64.062 (Average: 62.209)\n",
      "Epoch: No: [18] Batches: [500/782]\tLoss: 0.9103 (Average: 1.0626)\tPrecision: 68.750 (Average: 62.123)\n",
      "Epoch: No: [18] Batches: [550/782]\tLoss: 1.2556 (Average: 1.0653)\tPrecision: 57.812 (Average: 62.060)\n",
      "Epoch: No: [18] Batches: [600/782]\tLoss: 1.0668 (Average: 1.0657)\tPrecision: 60.938 (Average: 62.084)\n",
      "Epoch: No: [18] Batches: [650/782]\tLoss: 1.0230 (Average: 1.0669)\tPrecision: 59.375 (Average: 62.078)\n",
      "Epoch: No: [18] Batches: [700/782]\tLoss: 1.0833 (Average: 1.0655)\tPrecision: 65.625 (Average: 62.192)\n",
      "Epoch: No: [18] Batches: [750/782]\tLoss: 1.1250 (Average: 1.0662)\tPrecision: 59.375 (Average: 62.103)\n",
      "Test Accuracy\t  Top Precision: 60.530 (Error: 39.470 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [19] Batches: [0/782]\tLoss: 1.1728 (Average: 1.1728)\tPrecision: 59.375 (Average: 59.375)\n",
      "Epoch: No: [19] Batches: [50/782]\tLoss: 0.8613 (Average: 1.0377)\tPrecision: 68.750 (Average: 63.021)\n",
      "Epoch: No: [19] Batches: [100/782]\tLoss: 1.3562 (Average: 1.0571)\tPrecision: 56.250 (Average: 62.593)\n",
      "Epoch: No: [19] Batches: [150/782]\tLoss: 1.1523 (Average: 1.0596)\tPrecision: 59.375 (Average: 62.624)\n",
      "Epoch: No: [19] Batches: [200/782]\tLoss: 1.2065 (Average: 1.0696)\tPrecision: 56.250 (Average: 62.057)\n",
      "Epoch: No: [19] Batches: [250/782]\tLoss: 1.2459 (Average: 1.0653)\tPrecision: 60.938 (Average: 62.114)\n",
      "Epoch: No: [19] Batches: [300/782]\tLoss: 1.0933 (Average: 1.0672)\tPrecision: 59.375 (Average: 62.054)\n",
      "Epoch: No: [19] Batches: [350/782]\tLoss: 0.8869 (Average: 1.0687)\tPrecision: 62.500 (Average: 61.877)\n",
      "Epoch: No: [19] Batches: [400/782]\tLoss: 0.9547 (Average: 1.0625)\tPrecision: 62.500 (Average: 61.974)\n",
      "Epoch: No: [19] Batches: [450/782]\tLoss: 1.1459 (Average: 1.0574)\tPrecision: 64.062 (Average: 62.133)\n",
      "Epoch: No: [19] Batches: [500/782]\tLoss: 0.8899 (Average: 1.0544)\tPrecision: 68.750 (Average: 62.285)\n",
      "Epoch: No: [19] Batches: [550/782]\tLoss: 1.1306 (Average: 1.0558)\tPrecision: 62.500 (Average: 62.185)\n",
      "Epoch: No: [19] Batches: [600/782]\tLoss: 1.1215 (Average: 1.0551)\tPrecision: 62.500 (Average: 62.185)\n",
      "Epoch: No: [19] Batches: [650/782]\tLoss: 0.9920 (Average: 1.0559)\tPrecision: 65.625 (Average: 62.205)\n",
      "Epoch: No: [19] Batches: [700/782]\tLoss: 0.9790 (Average: 1.0549)\tPrecision: 64.062 (Average: 62.208)\n",
      "Epoch: No: [19] Batches: [750/782]\tLoss: 1.2776 (Average: 1.0517)\tPrecision: 57.812 (Average: 62.304)\n",
      "Test Accuracy\t  Top Precision: 62.240 (Error: 37.760 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [20] Batches: [0/782]\tLoss: 1.0668 (Average: 1.0668)\tPrecision: 59.375 (Average: 59.375)\n",
      "Epoch: No: [20] Batches: [50/782]\tLoss: 1.0298 (Average: 1.0350)\tPrecision: 62.500 (Average: 62.898)\n",
      "Epoch: No: [20] Batches: [100/782]\tLoss: 0.9735 (Average: 1.0231)\tPrecision: 68.750 (Average: 63.026)\n",
      "Epoch: No: [20] Batches: [150/782]\tLoss: 1.1292 (Average: 1.0245)\tPrecision: 59.375 (Average: 63.131)\n",
      "Epoch: No: [20] Batches: [200/782]\tLoss: 1.0296 (Average: 1.0276)\tPrecision: 65.625 (Average: 62.881)\n",
      "Epoch: No: [20] Batches: [250/782]\tLoss: 0.7990 (Average: 1.0313)\tPrecision: 73.438 (Average: 62.768)\n",
      "Epoch: No: [20] Batches: [300/782]\tLoss: 1.1451 (Average: 1.0269)\tPrecision: 60.938 (Average: 63.040)\n",
      "Epoch: No: [20] Batches: [350/782]\tLoss: 0.9406 (Average: 1.0296)\tPrecision: 65.625 (Average: 62.914)\n",
      "Epoch: No: [20] Batches: [400/782]\tLoss: 1.2675 (Average: 1.0279)\tPrecision: 57.812 (Average: 63.073)\n",
      "Epoch: No: [20] Batches: [450/782]\tLoss: 0.6864 (Average: 1.0276)\tPrecision: 71.875 (Average: 63.148)\n",
      "Epoch: No: [20] Batches: [500/782]\tLoss: 1.2205 (Average: 1.0269)\tPrecision: 56.250 (Average: 63.230)\n",
      "Epoch: No: [20] Batches: [550/782]\tLoss: 1.0290 (Average: 1.0291)\tPrecision: 62.500 (Average: 63.096)\n",
      "Epoch: No: [20] Batches: [600/782]\tLoss: 1.2099 (Average: 1.0290)\tPrecision: 54.688 (Average: 63.111)\n",
      "Epoch: No: [20] Batches: [650/782]\tLoss: 1.1255 (Average: 1.0284)\tPrecision: 59.375 (Average: 63.196)\n",
      "Epoch: No: [20] Batches: [700/782]\tLoss: 0.9849 (Average: 1.0291)\tPrecision: 59.375 (Average: 63.109)\n",
      "Epoch: No: [20] Batches: [750/782]\tLoss: 0.8944 (Average: 1.0302)\tPrecision: 65.625 (Average: 62.987)\n",
      "Test Accuracy\t  Top Precision: 63.580 (Error: 36.420 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [21] Batches: [0/782]\tLoss: 1.1000 (Average: 1.1000)\tPrecision: 62.500 (Average: 62.500)\n",
      "Epoch: No: [21] Batches: [50/782]\tLoss: 1.0317 (Average: 0.9824)\tPrecision: 57.812 (Average: 64.767)\n",
      "Epoch: No: [21] Batches: [100/782]\tLoss: 0.9935 (Average: 0.9800)\tPrecision: 62.500 (Average: 64.387)\n",
      "Epoch: No: [21] Batches: [150/782]\tLoss: 1.0957 (Average: 0.9859)\tPrecision: 64.062 (Average: 64.683)\n",
      "Epoch: No: [21] Batches: [200/782]\tLoss: 0.7800 (Average: 0.9909)\tPrecision: 73.438 (Average: 64.568)\n",
      "Epoch: No: [21] Batches: [250/782]\tLoss: 1.0504 (Average: 0.9962)\tPrecision: 68.750 (Average: 64.461)\n",
      "Epoch: No: [21] Batches: [300/782]\tLoss: 1.2188 (Average: 0.9999)\tPrecision: 60.938 (Average: 64.317)\n",
      "Epoch: No: [21] Batches: [350/782]\tLoss: 0.7736 (Average: 0.9974)\tPrecision: 68.750 (Average: 64.276)\n",
      "Epoch: No: [21] Batches: [400/782]\tLoss: 0.9716 (Average: 0.9995)\tPrecision: 62.500 (Average: 64.253)\n",
      "Epoch: No: [21] Batches: [450/782]\tLoss: 1.2355 (Average: 1.0014)\tPrecision: 59.375 (Average: 64.218)\n",
      "Epoch: No: [21] Batches: [500/782]\tLoss: 1.0447 (Average: 1.0023)\tPrecision: 57.812 (Average: 64.103)\n",
      "Epoch: No: [21] Batches: [550/782]\tLoss: 1.0722 (Average: 1.0027)\tPrecision: 54.688 (Average: 64.122)\n",
      "Epoch: No: [21] Batches: [600/782]\tLoss: 0.7763 (Average: 1.0004)\tPrecision: 68.750 (Average: 64.185)\n",
      "Epoch: No: [21] Batches: [650/782]\tLoss: 1.1806 (Average: 0.9970)\tPrecision: 56.250 (Average: 64.295)\n",
      "Epoch: No: [21] Batches: [700/782]\tLoss: 1.0554 (Average: 0.9981)\tPrecision: 62.500 (Average: 64.297)\n",
      "Epoch: No: [21] Batches: [750/782]\tLoss: 1.2068 (Average: 1.0018)\tPrecision: 56.250 (Average: 64.191)\n",
      "Test Accuracy\t  Top Precision: 63.580 (Error: 36.420 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [22] Batches: [0/782]\tLoss: 0.9419 (Average: 0.9419)\tPrecision: 64.062 (Average: 64.062)\n",
      "Epoch: No: [22] Batches: [50/782]\tLoss: 0.9277 (Average: 0.9857)\tPrecision: 70.312 (Average: 65.594)\n",
      "Epoch: No: [22] Batches: [100/782]\tLoss: 1.0013 (Average: 0.9858)\tPrecision: 62.500 (Average: 64.975)\n",
      "Epoch: No: [22] Batches: [150/782]\tLoss: 0.9515 (Average: 0.9821)\tPrecision: 71.875 (Average: 65.108)\n",
      "Epoch: No: [22] Batches: [200/782]\tLoss: 0.8558 (Average: 0.9761)\tPrecision: 65.625 (Average: 65.407)\n",
      "Epoch: No: [22] Batches: [250/782]\tLoss: 1.0429 (Average: 0.9678)\tPrecision: 67.188 (Average: 65.538)\n",
      "Epoch: No: [22] Batches: [300/782]\tLoss: 1.0272 (Average: 0.9756)\tPrecision: 62.500 (Average: 65.085)\n",
      "Epoch: No: [22] Batches: [350/782]\tLoss: 1.0473 (Average: 0.9791)\tPrecision: 59.375 (Average: 65.015)\n",
      "Epoch: No: [22] Batches: [400/782]\tLoss: 0.7845 (Average: 0.9793)\tPrecision: 64.062 (Average: 64.935)\n",
      "Epoch: No: [22] Batches: [450/782]\tLoss: 1.2981 (Average: 0.9816)\tPrecision: 59.375 (Average: 64.790)\n",
      "Epoch: No: [22] Batches: [500/782]\tLoss: 0.9217 (Average: 0.9816)\tPrecision: 73.438 (Average: 64.702)\n",
      "Epoch: No: [22] Batches: [550/782]\tLoss: 1.1018 (Average: 0.9850)\tPrecision: 59.375 (Average: 64.712)\n",
      "Epoch: No: [22] Batches: [600/782]\tLoss: 1.1406 (Average: 0.9842)\tPrecision: 59.375 (Average: 64.798)\n",
      "Epoch: No: [22] Batches: [650/782]\tLoss: 1.0210 (Average: 0.9882)\tPrecision: 60.938 (Average: 64.737)\n",
      "Epoch: No: [22] Batches: [700/782]\tLoss: 1.0828 (Average: 0.9892)\tPrecision: 67.188 (Average: 64.642)\n",
      "Epoch: No: [22] Batches: [750/782]\tLoss: 1.1284 (Average: 0.9902)\tPrecision: 59.375 (Average: 64.603)\n",
      "Test Accuracy\t  Top Precision: 62.040 (Error: 37.960 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [23] Batches: [0/782]\tLoss: 0.8658 (Average: 0.8658)\tPrecision: 67.188 (Average: 67.188)\n",
      "Epoch: No: [23] Batches: [50/782]\tLoss: 0.9581 (Average: 1.0032)\tPrecision: 60.938 (Average: 64.308)\n",
      "Epoch: No: [23] Batches: [100/782]\tLoss: 0.9213 (Average: 0.9906)\tPrecision: 68.750 (Average: 64.542)\n",
      "Epoch: No: [23] Batches: [150/782]\tLoss: 0.9299 (Average: 0.9838)\tPrecision: 60.938 (Average: 64.932)\n",
      "Epoch: No: [23] Batches: [200/782]\tLoss: 0.9766 (Average: 0.9721)\tPrecision: 70.312 (Average: 65.260)\n",
      "Epoch: No: [23] Batches: [250/782]\tLoss: 1.1789 (Average: 0.9749)\tPrecision: 54.688 (Average: 65.127)\n",
      "Epoch: No: [23] Batches: [300/782]\tLoss: 1.0526 (Average: 0.9783)\tPrecision: 65.625 (Average: 64.976)\n",
      "Epoch: No: [23] Batches: [350/782]\tLoss: 0.9626 (Average: 0.9822)\tPrecision: 68.750 (Average: 64.926)\n",
      "Epoch: No: [23] Batches: [400/782]\tLoss: 1.0367 (Average: 0.9805)\tPrecision: 64.062 (Average: 64.889)\n",
      "Epoch: No: [23] Batches: [450/782]\tLoss: 0.7523 (Average: 0.9821)\tPrecision: 70.312 (Average: 64.825)\n",
      "Epoch: No: [23] Batches: [500/782]\tLoss: 0.8437 (Average: 0.9826)\tPrecision: 68.750 (Average: 64.833)\n",
      "Epoch: No: [23] Batches: [550/782]\tLoss: 0.8861 (Average: 0.9799)\tPrecision: 68.750 (Average: 64.947)\n",
      "Epoch: No: [23] Batches: [600/782]\tLoss: 0.6816 (Average: 0.9805)\tPrecision: 68.750 (Average: 64.894)\n",
      "Epoch: No: [23] Batches: [650/782]\tLoss: 0.9349 (Average: 0.9773)\tPrecision: 64.062 (Average: 64.999)\n",
      "Epoch: No: [23] Batches: [700/782]\tLoss: 0.9577 (Average: 0.9790)\tPrecision: 64.062 (Average: 64.921)\n",
      "Epoch: No: [23] Batches: [750/782]\tLoss: 0.9195 (Average: 0.9788)\tPrecision: 67.188 (Average: 64.909)\n",
      "Test Accuracy\t  Top Precision: 62.940 (Error: 37.060 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [24] Batches: [0/782]\tLoss: 0.8887 (Average: 0.8887)\tPrecision: 73.438 (Average: 73.438)\n",
      "Epoch: No: [24] Batches: [50/782]\tLoss: 0.8215 (Average: 0.9986)\tPrecision: 71.875 (Average: 63.450)\n",
      "Epoch: No: [24] Batches: [100/782]\tLoss: 0.7924 (Average: 0.9802)\tPrecision: 75.000 (Average: 64.573)\n",
      "Epoch: No: [24] Batches: [150/782]\tLoss: 0.9974 (Average: 0.9707)\tPrecision: 64.062 (Average: 65.097)\n",
      "Epoch: No: [24] Batches: [200/782]\tLoss: 0.8810 (Average: 0.9771)\tPrecision: 67.188 (Average: 65.190)\n",
      "Epoch: No: [24] Batches: [250/782]\tLoss: 1.0762 (Average: 0.9684)\tPrecision: 62.500 (Average: 65.463)\n",
      "Epoch: No: [24] Batches: [300/782]\tLoss: 1.1486 (Average: 0.9661)\tPrecision: 57.812 (Average: 65.516)\n",
      "Epoch: No: [24] Batches: [350/782]\tLoss: 0.7869 (Average: 0.9629)\tPrecision: 73.438 (Average: 65.616)\n",
      "Epoch: No: [24] Batches: [400/782]\tLoss: 0.9128 (Average: 0.9651)\tPrecision: 64.062 (Average: 65.344)\n",
      "Epoch: No: [24] Batches: [450/782]\tLoss: 0.9196 (Average: 0.9706)\tPrecision: 68.750 (Average: 65.230)\n",
      "Epoch: No: [24] Batches: [500/782]\tLoss: 1.0426 (Average: 0.9672)\tPrecision: 67.188 (Average: 65.335)\n",
      "Epoch: No: [24] Batches: [550/782]\tLoss: 0.9043 (Average: 0.9642)\tPrecision: 64.062 (Average: 65.373)\n",
      "Epoch: No: [24] Batches: [600/782]\tLoss: 1.0485 (Average: 0.9674)\tPrecision: 71.875 (Average: 65.258)\n",
      "Epoch: No: [24] Batches: [650/782]\tLoss: 0.9856 (Average: 0.9693)\tPrecision: 65.625 (Average: 65.222)\n",
      "Epoch: No: [24] Batches: [700/782]\tLoss: 0.8143 (Average: 0.9671)\tPrecision: 76.562 (Average: 65.362)\n",
      "Epoch: No: [24] Batches: [750/782]\tLoss: 1.0114 (Average: 0.9666)\tPrecision: 65.625 (Average: 65.392)\n",
      "Test Accuracy\t  Top Precision: 59.960 (Error: 40.040 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [25] Batches: [0/782]\tLoss: 1.2221 (Average: 1.2221)\tPrecision: 62.500 (Average: 62.500)\n",
      "Epoch: No: [25] Batches: [50/782]\tLoss: 1.0042 (Average: 0.9798)\tPrecision: 59.375 (Average: 64.951)\n",
      "Epoch: No: [25] Batches: [100/782]\tLoss: 0.8644 (Average: 0.9759)\tPrecision: 68.750 (Average: 64.944)\n",
      "Epoch: No: [25] Batches: [150/782]\tLoss: 0.9301 (Average: 0.9727)\tPrecision: 68.750 (Average: 65.108)\n",
      "Epoch: No: [25] Batches: [200/782]\tLoss: 0.9183 (Average: 0.9661)\tPrecision: 68.750 (Average: 65.711)\n",
      "Epoch: No: [25] Batches: [250/782]\tLoss: 0.7723 (Average: 0.9631)\tPrecision: 76.562 (Average: 65.874)\n",
      "Epoch: No: [25] Batches: [300/782]\tLoss: 0.8171 (Average: 0.9563)\tPrecision: 70.312 (Average: 66.217)\n",
      "Epoch: No: [25] Batches: [350/782]\tLoss: 0.8292 (Average: 0.9563)\tPrecision: 71.875 (Average: 66.155)\n",
      "Epoch: No: [25] Batches: [400/782]\tLoss: 0.9930 (Average: 0.9574)\tPrecision: 64.062 (Average: 66.163)\n",
      "Epoch: No: [25] Batches: [450/782]\tLoss: 1.1037 (Average: 0.9553)\tPrecision: 60.938 (Average: 66.211)\n",
      "Epoch: No: [25] Batches: [500/782]\tLoss: 1.0417 (Average: 0.9551)\tPrecision: 71.875 (Average: 66.271)\n",
      "Epoch: No: [25] Batches: [550/782]\tLoss: 0.9211 (Average: 0.9556)\tPrecision: 64.062 (Average: 66.243)\n",
      "Epoch: No: [25] Batches: [600/782]\tLoss: 0.8468 (Average: 0.9540)\tPrecision: 68.750 (Average: 66.280)\n",
      "Epoch: No: [25] Batches: [650/782]\tLoss: 1.0137 (Average: 0.9557)\tPrecision: 65.625 (Average: 66.158)\n",
      "Epoch: No: [25] Batches: [700/782]\tLoss: 0.6759 (Average: 0.9536)\tPrecision: 71.875 (Average: 66.184)\n",
      "Epoch: No: [25] Batches: [750/782]\tLoss: 0.6841 (Average: 0.9499)\tPrecision: 73.438 (Average: 66.278)\n",
      "Test Accuracy\t  Top Precision: 65.170 (Error: 34.830 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [26] Batches: [0/782]\tLoss: 1.0660 (Average: 1.0660)\tPrecision: 60.938 (Average: 60.938)\n",
      "Epoch: No: [26] Batches: [50/782]\tLoss: 1.1097 (Average: 0.9593)\tPrecision: 53.125 (Average: 65.196)\n",
      "Epoch: No: [26] Batches: [100/782]\tLoss: 1.1085 (Average: 0.9519)\tPrecision: 56.250 (Average: 64.929)\n",
      "Epoch: No: [26] Batches: [150/782]\tLoss: 0.7756 (Average: 0.9460)\tPrecision: 65.625 (Average: 65.553)\n",
      "Epoch: No: [26] Batches: [200/782]\tLoss: 0.9715 (Average: 0.9478)\tPrecision: 65.625 (Average: 65.742)\n",
      "Epoch: No: [26] Batches: [250/782]\tLoss: 0.8939 (Average: 0.9482)\tPrecision: 71.875 (Average: 65.731)\n",
      "Epoch: No: [26] Batches: [300/782]\tLoss: 0.8506 (Average: 0.9484)\tPrecision: 70.312 (Average: 65.822)\n",
      "Epoch: No: [26] Batches: [350/782]\tLoss: 0.6832 (Average: 0.9452)\tPrecision: 76.562 (Average: 66.012)\n",
      "Epoch: No: [26] Batches: [400/782]\tLoss: 0.6539 (Average: 0.9450)\tPrecision: 79.688 (Average: 65.980)\n",
      "Epoch: No: [26] Batches: [450/782]\tLoss: 1.1428 (Average: 0.9452)\tPrecision: 60.938 (Average: 65.909)\n",
      "Epoch: No: [26] Batches: [500/782]\tLoss: 0.9631 (Average: 0.9415)\tPrecision: 65.625 (Average: 66.243)\n",
      "Epoch: No: [26] Batches: [550/782]\tLoss: 0.7217 (Average: 0.9424)\tPrecision: 78.125 (Average: 66.291)\n",
      "Epoch: No: [26] Batches: [600/782]\tLoss: 0.7906 (Average: 0.9400)\tPrecision: 67.188 (Average: 66.483)\n",
      "Epoch: No: [26] Batches: [650/782]\tLoss: 0.9869 (Average: 0.9413)\tPrecision: 59.375 (Average: 66.443)\n",
      "Epoch: No: [26] Batches: [700/782]\tLoss: 0.9185 (Average: 0.9389)\tPrecision: 67.188 (Average: 66.552)\n",
      "Epoch: No: [26] Batches: [750/782]\tLoss: 0.6160 (Average: 0.9369)\tPrecision: 75.000 (Average: 66.599)\n",
      "Test Accuracy\t  Top Precision: 63.210 (Error: 36.790 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [27] Batches: [0/782]\tLoss: 0.7973 (Average: 0.7973)\tPrecision: 71.875 (Average: 71.875)\n",
      "Epoch: No: [27] Batches: [50/782]\tLoss: 0.5962 (Average: 0.9055)\tPrecision: 82.812 (Average: 67.984)\n",
      "Epoch: No: [27] Batches: [100/782]\tLoss: 0.9347 (Average: 0.9065)\tPrecision: 68.750 (Average: 68.085)\n",
      "Epoch: No: [27] Batches: [150/782]\tLoss: 1.1379 (Average: 0.9206)\tPrecision: 51.562 (Average: 67.260)\n",
      "Epoch: No: [27] Batches: [200/782]\tLoss: 1.0032 (Average: 0.9146)\tPrecision: 64.062 (Average: 67.397)\n",
      "Epoch: No: [27] Batches: [250/782]\tLoss: 1.1690 (Average: 0.9223)\tPrecision: 62.500 (Average: 67.206)\n",
      "Epoch: No: [27] Batches: [300/782]\tLoss: 0.9652 (Average: 0.9199)\tPrecision: 68.750 (Average: 67.286)\n",
      "Epoch: No: [27] Batches: [350/782]\tLoss: 0.9255 (Average: 0.9163)\tPrecision: 65.625 (Average: 67.481)\n",
      "Epoch: No: [27] Batches: [400/782]\tLoss: 0.5510 (Average: 0.9171)\tPrecision: 82.812 (Average: 67.308)\n",
      "Epoch: No: [27] Batches: [450/782]\tLoss: 0.8615 (Average: 0.9184)\tPrecision: 67.188 (Average: 67.278)\n",
      "Epoch: No: [27] Batches: [500/782]\tLoss: 0.7986 (Average: 0.9174)\tPrecision: 75.000 (Average: 67.300)\n",
      "Epoch: No: [27] Batches: [550/782]\tLoss: 0.6941 (Average: 0.9157)\tPrecision: 73.438 (Average: 67.321)\n",
      "Epoch: No: [27] Batches: [600/782]\tLoss: 1.0244 (Average: 0.9171)\tPrecision: 71.875 (Average: 67.175)\n",
      "Epoch: No: [27] Batches: [650/782]\tLoss: 1.0634 (Average: 0.9187)\tPrecision: 64.062 (Average: 67.137)\n",
      "Epoch: No: [27] Batches: [700/782]\tLoss: 0.8777 (Average: 0.9191)\tPrecision: 67.188 (Average: 67.147)\n",
      "Epoch: No: [27] Batches: [750/782]\tLoss: 0.8044 (Average: 0.9179)\tPrecision: 73.438 (Average: 67.202)\n",
      "Test Accuracy\t  Top Precision: 65.510 (Error: 34.490 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [28] Batches: [0/782]\tLoss: 1.0009 (Average: 1.0009)\tPrecision: 64.062 (Average: 64.062)\n",
      "Epoch: No: [28] Batches: [50/782]\tLoss: 0.9193 (Average: 0.9213)\tPrecision: 62.500 (Average: 66.850)\n",
      "Epoch: No: [28] Batches: [100/782]\tLoss: 0.8779 (Average: 0.9188)\tPrecision: 64.062 (Average: 67.110)\n",
      "Epoch: No: [28] Batches: [150/782]\tLoss: 0.5710 (Average: 0.9116)\tPrecision: 70.312 (Average: 67.301)\n",
      "Epoch: No: [28] Batches: [200/782]\tLoss: 0.6117 (Average: 0.9014)\tPrecision: 79.688 (Average: 67.615)\n",
      "Epoch: No: [28] Batches: [250/782]\tLoss: 0.6743 (Average: 0.9010)\tPrecision: 71.875 (Average: 67.524)\n",
      "Epoch: No: [28] Batches: [300/782]\tLoss: 0.8705 (Average: 0.8999)\tPrecision: 68.750 (Average: 67.572)\n",
      "Epoch: No: [28] Batches: [350/782]\tLoss: 0.8623 (Average: 0.8970)\tPrecision: 70.312 (Average: 67.722)\n",
      "Epoch: No: [28] Batches: [400/782]\tLoss: 0.9725 (Average: 0.8998)\tPrecision: 67.188 (Average: 67.772)\n",
      "Epoch: No: [28] Batches: [450/782]\tLoss: 0.7730 (Average: 0.9037)\tPrecision: 75.000 (Average: 67.645)\n",
      "Epoch: No: [28] Batches: [500/782]\tLoss: 0.9420 (Average: 0.9051)\tPrecision: 75.000 (Average: 67.711)\n",
      "Epoch: No: [28] Batches: [550/782]\tLoss: 1.0585 (Average: 0.9030)\tPrecision: 70.312 (Average: 67.811)\n",
      "Epoch: No: [28] Batches: [600/782]\tLoss: 1.0641 (Average: 0.9023)\tPrecision: 62.500 (Average: 67.832)\n",
      "Epoch: No: [28] Batches: [650/782]\tLoss: 0.9401 (Average: 0.9035)\tPrecision: 62.500 (Average: 67.821)\n",
      "Epoch: No: [28] Batches: [700/782]\tLoss: 0.8695 (Average: 0.9028)\tPrecision: 67.188 (Average: 67.890)\n",
      "Epoch: No: [28] Batches: [750/782]\tLoss: 1.0180 (Average: 0.9038)\tPrecision: 67.188 (Average: 67.860)\n",
      "Test Accuracy\t  Top Precision: 66.140 (Error: 33.860 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [29] Batches: [0/782]\tLoss: 1.0541 (Average: 1.0541)\tPrecision: 64.062 (Average: 64.062)\n",
      "Epoch: No: [29] Batches: [50/782]\tLoss: 0.8853 (Average: 0.9163)\tPrecision: 70.312 (Average: 67.371)\n",
      "Epoch: No: [29] Batches: [100/782]\tLoss: 0.6591 (Average: 0.9158)\tPrecision: 78.125 (Average: 67.899)\n",
      "Epoch: No: [29] Batches: [150/782]\tLoss: 1.0211 (Average: 0.9069)\tPrecision: 64.062 (Average: 67.891)\n",
      "Epoch: No: [29] Batches: [200/782]\tLoss: 1.0516 (Average: 0.9021)\tPrecision: 67.188 (Average: 68.198)\n",
      "Epoch: No: [29] Batches: [250/782]\tLoss: 0.7883 (Average: 0.8993)\tPrecision: 71.875 (Average: 68.252)\n",
      "Epoch: No: [29] Batches: [300/782]\tLoss: 0.7452 (Average: 0.8943)\tPrecision: 76.562 (Average: 68.459)\n",
      "Epoch: No: [29] Batches: [350/782]\tLoss: 0.7063 (Average: 0.8933)\tPrecision: 81.250 (Average: 68.421)\n",
      "Epoch: No: [29] Batches: [400/782]\tLoss: 0.9226 (Average: 0.8948)\tPrecision: 64.062 (Average: 68.321)\n",
      "Epoch: No: [29] Batches: [450/782]\tLoss: 0.9104 (Average: 0.8948)\tPrecision: 70.312 (Average: 68.341)\n",
      "Epoch: No: [29] Batches: [500/782]\tLoss: 0.8923 (Average: 0.8954)\tPrecision: 67.188 (Average: 68.376)\n",
      "Epoch: No: [29] Batches: [550/782]\tLoss: 0.8102 (Average: 0.8967)\tPrecision: 73.438 (Average: 68.384)\n",
      "Epoch: No: [29] Batches: [600/782]\tLoss: 1.0436 (Average: 0.8982)\tPrecision: 64.062 (Average: 68.279)\n",
      "Epoch: No: [29] Batches: [650/782]\tLoss: 0.8925 (Average: 0.8981)\tPrecision: 65.625 (Average: 68.215)\n",
      "Epoch: No: [29] Batches: [700/782]\tLoss: 0.7506 (Average: 0.8968)\tPrecision: 73.438 (Average: 68.293)\n",
      "Epoch: No: [29] Batches: [750/782]\tLoss: 1.0628 (Average: 0.8969)\tPrecision: 59.375 (Average: 68.305)\n",
      "Test Accuracy\t  Top Precision: 66.660 (Error: 33.340 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [30] Batches: [0/782]\tLoss: 0.7890 (Average: 0.7890)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [30] Batches: [50/782]\tLoss: 0.8406 (Average: 0.9030)\tPrecision: 67.188 (Average: 67.586)\n",
      "Epoch: No: [30] Batches: [100/782]\tLoss: 0.8587 (Average: 0.9173)\tPrecision: 64.062 (Average: 67.481)\n",
      "Epoch: No: [30] Batches: [150/782]\tLoss: 0.9169 (Average: 0.8928)\tPrecision: 65.625 (Average: 68.502)\n",
      "Epoch: No: [30] Batches: [200/782]\tLoss: 0.6170 (Average: 0.9041)\tPrecision: 78.125 (Average: 67.949)\n",
      "Epoch: No: [30] Batches: [250/782]\tLoss: 0.8732 (Average: 0.9054)\tPrecision: 65.625 (Average: 67.854)\n",
      "Epoch: No: [30] Batches: [300/782]\tLoss: 0.9833 (Average: 0.9045)\tPrecision: 64.062 (Average: 67.868)\n",
      "Epoch: No: [30] Batches: [350/782]\tLoss: 0.8176 (Average: 0.8982)\tPrecision: 70.312 (Average: 68.056)\n",
      "Epoch: No: [30] Batches: [400/782]\tLoss: 0.7380 (Average: 0.8940)\tPrecision: 70.312 (Average: 68.049)\n",
      "Epoch: No: [30] Batches: [450/782]\tLoss: 1.0186 (Average: 0.8867)\tPrecision: 57.812 (Average: 68.296)\n",
      "Epoch: No: [30] Batches: [500/782]\tLoss: 0.8404 (Average: 0.8875)\tPrecision: 75.000 (Average: 68.288)\n",
      "Epoch: No: [30] Batches: [550/782]\tLoss: 1.3232 (Average: 0.8874)\tPrecision: 54.688 (Average: 68.350)\n",
      "Epoch: No: [30] Batches: [600/782]\tLoss: 0.6890 (Average: 0.8842)\tPrecision: 78.125 (Average: 68.493)\n",
      "Epoch: No: [30] Batches: [650/782]\tLoss: 0.6845 (Average: 0.8822)\tPrecision: 79.688 (Average: 68.580)\n",
      "Epoch: No: [30] Batches: [700/782]\tLoss: 0.8364 (Average: 0.8813)\tPrecision: 68.750 (Average: 68.656)\n",
      "Epoch: No: [30] Batches: [750/782]\tLoss: 0.8712 (Average: 0.8820)\tPrecision: 70.312 (Average: 68.613)\n",
      "Test Accuracy\t  Top Precision: 65.250 (Error: 34.750 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [31] Batches: [0/782]\tLoss: 1.0779 (Average: 1.0779)\tPrecision: 60.938 (Average: 60.938)\n",
      "Epoch: No: [31] Batches: [50/782]\tLoss: 0.9262 (Average: 0.8937)\tPrecision: 65.625 (Average: 68.750)\n",
      "Epoch: No: [31] Batches: [100/782]\tLoss: 0.7292 (Average: 0.8506)\tPrecision: 71.875 (Average: 70.050)\n",
      "Epoch: No: [31] Batches: [150/782]\tLoss: 0.8929 (Average: 0.8476)\tPrecision: 73.438 (Average: 70.137)\n",
      "Epoch: No: [31] Batches: [200/782]\tLoss: 0.9373 (Average: 0.8696)\tPrecision: 73.438 (Average: 69.605)\n",
      "Epoch: No: [31] Batches: [250/782]\tLoss: 1.0512 (Average: 0.8610)\tPrecision: 62.500 (Average: 69.696)\n",
      "Epoch: No: [31] Batches: [300/782]\tLoss: 0.7677 (Average: 0.8657)\tPrecision: 65.625 (Average: 69.388)\n",
      "Epoch: No: [31] Batches: [350/782]\tLoss: 0.7319 (Average: 0.8647)\tPrecision: 71.875 (Average: 69.538)\n",
      "Epoch: No: [31] Batches: [400/782]\tLoss: 0.8754 (Average: 0.8627)\tPrecision: 70.312 (Average: 69.611)\n",
      "Epoch: No: [31] Batches: [450/782]\tLoss: 0.9345 (Average: 0.8624)\tPrecision: 65.625 (Average: 69.627)\n",
      "Epoch: No: [31] Batches: [500/782]\tLoss: 0.9981 (Average: 0.8621)\tPrecision: 71.875 (Average: 69.683)\n",
      "Epoch: No: [31] Batches: [550/782]\tLoss: 0.8050 (Average: 0.8626)\tPrecision: 60.938 (Average: 69.674)\n",
      "Epoch: No: [31] Batches: [600/782]\tLoss: 0.6924 (Average: 0.8669)\tPrecision: 71.875 (Average: 69.494)\n",
      "Epoch: No: [31] Batches: [650/782]\tLoss: 0.9464 (Average: 0.8706)\tPrecision: 60.938 (Average: 69.350)\n",
      "Epoch: No: [31] Batches: [700/782]\tLoss: 0.7890 (Average: 0.8700)\tPrecision: 67.188 (Average: 69.359)\n",
      "Epoch: No: [31] Batches: [750/782]\tLoss: 0.7582 (Average: 0.8682)\tPrecision: 70.312 (Average: 69.443)\n",
      "Test Accuracy\t  Top Precision: 62.260 (Error: 37.740 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [32] Batches: [0/782]\tLoss: 0.8948 (Average: 0.8948)\tPrecision: 65.625 (Average: 65.625)\n",
      "Epoch: No: [32] Batches: [50/782]\tLoss: 0.7087 (Average: 0.8456)\tPrecision: 78.125 (Average: 70.190)\n",
      "Epoch: No: [32] Batches: [100/782]\tLoss: 0.9103 (Average: 0.8355)\tPrecision: 62.500 (Average: 70.483)\n",
      "Epoch: No: [32] Batches: [150/782]\tLoss: 0.8532 (Average: 0.8403)\tPrecision: 67.188 (Average: 70.395)\n",
      "Epoch: No: [32] Batches: [200/782]\tLoss: 0.8805 (Average: 0.8471)\tPrecision: 64.062 (Average: 69.877)\n",
      "Epoch: No: [32] Batches: [250/782]\tLoss: 0.9163 (Average: 0.8395)\tPrecision: 64.062 (Average: 70.132)\n",
      "Epoch: No: [32] Batches: [300/782]\tLoss: 0.7861 (Average: 0.8418)\tPrecision: 60.938 (Average: 69.970)\n",
      "Epoch: No: [32] Batches: [350/782]\tLoss: 1.0066 (Average: 0.8414)\tPrecision: 62.500 (Average: 69.974)\n",
      "Epoch: No: [32] Batches: [400/782]\tLoss: 0.7992 (Average: 0.8446)\tPrecision: 62.500 (Average: 69.755)\n",
      "Epoch: No: [32] Batches: [450/782]\tLoss: 0.6400 (Average: 0.8457)\tPrecision: 73.438 (Average: 69.717)\n",
      "Epoch: No: [32] Batches: [500/782]\tLoss: 0.7745 (Average: 0.8445)\tPrecision: 75.000 (Average: 69.789)\n",
      "Epoch: No: [32] Batches: [550/782]\tLoss: 0.9788 (Average: 0.8458)\tPrecision: 68.750 (Average: 69.777)\n",
      "Epoch: No: [32] Batches: [600/782]\tLoss: 1.0931 (Average: 0.8481)\tPrecision: 64.062 (Average: 69.663)\n",
      "Epoch: No: [32] Batches: [650/782]\tLoss: 0.7805 (Average: 0.8496)\tPrecision: 70.312 (Average: 69.595)\n",
      "Epoch: No: [32] Batches: [700/782]\tLoss: 0.8080 (Average: 0.8497)\tPrecision: 64.062 (Average: 69.639)\n",
      "Epoch: No: [32] Batches: [750/782]\tLoss: 0.7306 (Average: 0.8519)\tPrecision: 68.750 (Average: 69.630)\n",
      "Test Accuracy\t  Top Precision: 69.090 (Error: 30.910 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [33] Batches: [0/782]\tLoss: 0.9201 (Average: 0.9201)\tPrecision: 68.750 (Average: 68.750)\n",
      "Epoch: No: [33] Batches: [50/782]\tLoss: 0.6829 (Average: 0.8224)\tPrecision: 73.438 (Average: 71.691)\n",
      "Epoch: No: [33] Batches: [100/782]\tLoss: 0.4642 (Average: 0.8334)\tPrecision: 81.250 (Average: 70.653)\n",
      "Epoch: No: [33] Batches: [150/782]\tLoss: 0.8608 (Average: 0.8437)\tPrecision: 68.750 (Average: 70.002)\n",
      "Epoch: No: [33] Batches: [200/782]\tLoss: 0.8683 (Average: 0.8473)\tPrecision: 68.750 (Average: 70.064)\n",
      "Epoch: No: [33] Batches: [250/782]\tLoss: 0.8293 (Average: 0.8471)\tPrecision: 70.312 (Average: 69.933)\n",
      "Epoch: No: [33] Batches: [300/782]\tLoss: 0.7916 (Average: 0.8475)\tPrecision: 75.000 (Average: 69.897)\n",
      "Epoch: No: [33] Batches: [350/782]\tLoss: 0.5883 (Average: 0.8484)\tPrecision: 75.000 (Average: 69.805)\n",
      "Epoch: No: [33] Batches: [400/782]\tLoss: 1.2041 (Average: 0.8504)\tPrecision: 57.812 (Average: 69.564)\n",
      "Epoch: No: [33] Batches: [450/782]\tLoss: 0.8909 (Average: 0.8514)\tPrecision: 70.312 (Average: 69.550)\n",
      "Epoch: No: [33] Batches: [500/782]\tLoss: 1.1414 (Average: 0.8511)\tPrecision: 54.688 (Average: 69.595)\n",
      "Epoch: No: [33] Batches: [550/782]\tLoss: 0.9201 (Average: 0.8548)\tPrecision: 67.188 (Average: 69.431)\n",
      "Epoch: No: [33] Batches: [600/782]\tLoss: 0.8744 (Average: 0.8533)\tPrecision: 67.188 (Average: 69.512)\n",
      "Epoch: No: [33] Batches: [650/782]\tLoss: 0.8517 (Average: 0.8496)\tPrecision: 62.500 (Average: 69.688)\n",
      "Epoch: No: [33] Batches: [700/782]\tLoss: 0.5875 (Average: 0.8508)\tPrecision: 73.438 (Average: 69.626)\n",
      "Epoch: No: [33] Batches: [750/782]\tLoss: 1.1598 (Average: 0.8520)\tPrecision: 59.375 (Average: 69.557)\n",
      "Test Accuracy\t  Top Precision: 68.580 (Error: 31.420 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [34] Batches: [0/782]\tLoss: 0.8702 (Average: 0.8702)\tPrecision: 67.188 (Average: 67.188)\n",
      "Epoch: No: [34] Batches: [50/782]\tLoss: 0.8962 (Average: 0.8944)\tPrecision: 71.875 (Average: 67.953)\n",
      "Epoch: No: [34] Batches: [100/782]\tLoss: 0.5611 (Average: 0.8391)\tPrecision: 76.562 (Average: 70.050)\n",
      "Epoch: No: [34] Batches: [150/782]\tLoss: 0.8635 (Average: 0.8379)\tPrecision: 64.062 (Average: 70.157)\n",
      "Epoch: No: [34] Batches: [200/782]\tLoss: 0.8764 (Average: 0.8331)\tPrecision: 70.312 (Average: 70.390)\n",
      "Epoch: No: [34] Batches: [250/782]\tLoss: 0.6410 (Average: 0.8380)\tPrecision: 78.125 (Average: 70.325)\n",
      "Epoch: No: [34] Batches: [300/782]\tLoss: 0.6823 (Average: 0.8338)\tPrecision: 67.188 (Average: 70.442)\n",
      "Epoch: No: [34] Batches: [350/782]\tLoss: 0.6804 (Average: 0.8388)\tPrecision: 79.688 (Average: 70.143)\n",
      "Epoch: No: [34] Batches: [400/782]\tLoss: 0.6895 (Average: 0.8413)\tPrecision: 75.000 (Average: 70.012)\n",
      "Epoch: No: [34] Batches: [450/782]\tLoss: 0.6849 (Average: 0.8411)\tPrecision: 73.438 (Average: 69.994)\n",
      "Epoch: No: [34] Batches: [500/782]\tLoss: 0.8905 (Average: 0.8424)\tPrecision: 73.438 (Average: 70.041)\n",
      "Epoch: No: [34] Batches: [550/782]\tLoss: 0.6264 (Average: 0.8409)\tPrecision: 78.125 (Average: 70.091)\n",
      "Epoch: No: [34] Batches: [600/782]\tLoss: 0.8826 (Average: 0.8383)\tPrecision: 75.000 (Average: 70.214)\n",
      "Epoch: No: [34] Batches: [650/782]\tLoss: 0.8325 (Average: 0.8373)\tPrecision: 67.188 (Average: 70.274)\n",
      "Epoch: No: [34] Batches: [700/782]\tLoss: 0.8269 (Average: 0.8387)\tPrecision: 65.625 (Average: 70.268)\n",
      "Epoch: No: [34] Batches: [750/782]\tLoss: 0.6424 (Average: 0.8398)\tPrecision: 78.125 (Average: 70.254)\n",
      "Test Accuracy\t  Top Precision: 67.830 (Error: 32.170 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [35] Batches: [0/782]\tLoss: 0.9938 (Average: 0.9938)\tPrecision: 67.188 (Average: 67.188)\n",
      "Epoch: No: [35] Batches: [50/782]\tLoss: 0.8641 (Average: 0.8366)\tPrecision: 75.000 (Average: 71.140)\n",
      "Epoch: No: [35] Batches: [100/782]\tLoss: 0.7749 (Average: 0.8357)\tPrecision: 78.125 (Average: 70.761)\n",
      "Epoch: No: [35] Batches: [150/782]\tLoss: 0.9654 (Average: 0.8459)\tPrecision: 68.750 (Average: 70.199)\n",
      "Epoch: No: [35] Batches: [200/782]\tLoss: 0.9598 (Average: 0.8552)\tPrecision: 60.938 (Average: 69.636)\n",
      "Epoch: No: [35] Batches: [250/782]\tLoss: 1.2145 (Average: 0.8516)\tPrecision: 48.438 (Average: 69.597)\n",
      "Epoch: No: [35] Batches: [300/782]\tLoss: 0.7201 (Average: 0.8462)\tPrecision: 79.688 (Average: 69.876)\n",
      "Epoch: No: [35] Batches: [350/782]\tLoss: 0.8646 (Average: 0.8464)\tPrecision: 65.625 (Average: 70.063)\n",
      "Epoch: No: [35] Batches: [400/782]\tLoss: 0.9064 (Average: 0.8447)\tPrecision: 70.312 (Average: 70.157)\n",
      "Epoch: No: [35] Batches: [450/782]\tLoss: 0.9185 (Average: 0.8423)\tPrecision: 64.062 (Average: 70.212)\n",
      "Epoch: No: [35] Batches: [500/782]\tLoss: 0.9345 (Average: 0.8452)\tPrecision: 65.625 (Average: 70.203)\n",
      "Epoch: No: [35] Batches: [550/782]\tLoss: 0.8058 (Average: 0.8437)\tPrecision: 73.438 (Average: 70.185)\n",
      "Epoch: No: [35] Batches: [600/782]\tLoss: 0.8274 (Average: 0.8435)\tPrecision: 76.562 (Average: 70.185)\n",
      "Epoch: No: [35] Batches: [650/782]\tLoss: 0.7829 (Average: 0.8427)\tPrecision: 75.000 (Average: 70.200)\n",
      "Epoch: No: [35] Batches: [700/782]\tLoss: 0.8218 (Average: 0.8428)\tPrecision: 68.750 (Average: 70.206)\n",
      "Epoch: No: [35] Batches: [750/782]\tLoss: 0.7339 (Average: 0.8405)\tPrecision: 76.562 (Average: 70.285)\n",
      "Test Accuracy\t  Top Precision: 69.260 (Error: 30.740 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [36] Batches: [0/782]\tLoss: 0.8872 (Average: 0.8872)\tPrecision: 67.188 (Average: 67.188)\n",
      "Epoch: No: [36] Batches: [50/782]\tLoss: 0.8750 (Average: 0.8257)\tPrecision: 78.125 (Average: 71.844)\n",
      "Epoch: No: [36] Batches: [100/782]\tLoss: 0.7943 (Average: 0.8153)\tPrecision: 70.312 (Average: 71.612)\n",
      "Epoch: No: [36] Batches: [150/782]\tLoss: 0.8369 (Average: 0.8085)\tPrecision: 60.938 (Average: 71.596)\n",
      "Epoch: No: [36] Batches: [200/782]\tLoss: 0.7770 (Average: 0.8178)\tPrecision: 71.875 (Average: 71.315)\n",
      "Epoch: No: [36] Batches: [250/782]\tLoss: 0.8455 (Average: 0.8193)\tPrecision: 73.438 (Average: 71.066)\n",
      "Epoch: No: [36] Batches: [300/782]\tLoss: 0.6339 (Average: 0.8142)\tPrecision: 73.438 (Average: 71.294)\n",
      "Epoch: No: [36] Batches: [350/782]\tLoss: 0.6152 (Average: 0.8128)\tPrecision: 79.688 (Average: 71.319)\n",
      "Epoch: No: [36] Batches: [400/782]\tLoss: 0.6709 (Average: 0.8108)\tPrecision: 78.125 (Average: 71.372)\n",
      "Epoch: No: [36] Batches: [450/782]\tLoss: 0.9135 (Average: 0.8116)\tPrecision: 67.188 (Average: 71.348)\n",
      "Epoch: No: [36] Batches: [500/782]\tLoss: 0.9048 (Average: 0.8145)\tPrecision: 68.750 (Average: 71.211)\n",
      "Epoch: No: [36] Batches: [550/782]\tLoss: 0.8498 (Average: 0.8141)\tPrecision: 73.438 (Average: 71.248)\n",
      "Epoch: No: [36] Batches: [600/782]\tLoss: 0.7034 (Average: 0.8169)\tPrecision: 75.000 (Average: 71.176)\n",
      "Epoch: No: [36] Batches: [650/782]\tLoss: 0.5062 (Average: 0.8145)\tPrecision: 84.375 (Average: 71.239)\n",
      "Epoch: No: [36] Batches: [700/782]\tLoss: 0.7542 (Average: 0.8160)\tPrecision: 70.312 (Average: 71.197)\n",
      "Epoch: No: [36] Batches: [750/782]\tLoss: 0.9140 (Average: 0.8196)\tPrecision: 68.750 (Average: 71.086)\n",
      "Test Accuracy\t  Top Precision: 68.090 (Error: 31.910 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [37] Batches: [0/782]\tLoss: 0.7217 (Average: 0.7217)\tPrecision: 75.000 (Average: 75.000)\n",
      "Epoch: No: [37] Batches: [50/782]\tLoss: 0.8784 (Average: 0.8302)\tPrecision: 64.062 (Average: 71.109)\n",
      "Epoch: No: [37] Batches: [100/782]\tLoss: 0.7832 (Average: 0.8045)\tPrecision: 73.438 (Average: 71.829)\n",
      "Epoch: No: [37] Batches: [150/782]\tLoss: 0.6195 (Average: 0.8131)\tPrecision: 78.125 (Average: 71.430)\n",
      "Epoch: No: [37] Batches: [200/782]\tLoss: 0.7230 (Average: 0.8127)\tPrecision: 79.688 (Average: 71.401)\n",
      "Epoch: No: [37] Batches: [250/782]\tLoss: 0.8420 (Average: 0.8055)\tPrecision: 73.438 (Average: 71.576)\n",
      "Epoch: No: [37] Batches: [300/782]\tLoss: 0.9951 (Average: 0.8068)\tPrecision: 70.312 (Average: 71.610)\n",
      "Epoch: No: [37] Batches: [350/782]\tLoss: 0.8654 (Average: 0.8091)\tPrecision: 70.312 (Average: 71.483)\n",
      "Epoch: No: [37] Batches: [400/782]\tLoss: 0.8086 (Average: 0.8103)\tPrecision: 68.750 (Average: 71.446)\n",
      "Epoch: No: [37] Batches: [450/782]\tLoss: 0.5417 (Average: 0.8106)\tPrecision: 84.375 (Average: 71.373)\n",
      "Epoch: No: [37] Batches: [500/782]\tLoss: 0.7847 (Average: 0.8112)\tPrecision: 70.312 (Average: 71.370)\n",
      "Epoch: No: [37] Batches: [550/782]\tLoss: 0.9442 (Average: 0.8117)\tPrecision: 62.500 (Average: 71.365)\n",
      "Epoch: No: [37] Batches: [600/782]\tLoss: 0.6575 (Average: 0.8112)\tPrecision: 76.562 (Average: 71.376)\n",
      "Epoch: No: [37] Batches: [650/782]\tLoss: 0.7204 (Average: 0.8137)\tPrecision: 76.562 (Average: 71.239)\n",
      "Epoch: No: [37] Batches: [700/782]\tLoss: 0.6829 (Average: 0.8137)\tPrecision: 76.562 (Average: 71.197)\n",
      "Epoch: No: [37] Batches: [750/782]\tLoss: 0.9522 (Average: 0.8160)\tPrecision: 67.188 (Average: 71.120)\n",
      "Test Accuracy\t  Top Precision: 69.380 (Error: 30.620 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [38] Batches: [0/782]\tLoss: 0.8575 (Average: 0.8575)\tPrecision: 73.438 (Average: 73.438)\n",
      "Epoch: No: [38] Batches: [50/782]\tLoss: 0.7410 (Average: 0.8078)\tPrecision: 71.875 (Average: 72.089)\n",
      "Epoch: No: [38] Batches: [100/782]\tLoss: 0.6838 (Average: 0.7950)\tPrecision: 79.688 (Average: 72.432)\n",
      "Epoch: No: [38] Batches: [150/782]\tLoss: 0.6821 (Average: 0.7914)\tPrecision: 73.438 (Average: 71.989)\n",
      "Epoch: No: [38] Batches: [200/782]\tLoss: 0.9352 (Average: 0.8009)\tPrecision: 67.188 (Average: 71.510)\n",
      "Epoch: No: [38] Batches: [250/782]\tLoss: 0.6562 (Average: 0.8055)\tPrecision: 79.688 (Average: 71.433)\n",
      "Epoch: No: [38] Batches: [300/782]\tLoss: 0.7003 (Average: 0.8008)\tPrecision: 82.812 (Average: 71.626)\n",
      "Epoch: No: [38] Batches: [350/782]\tLoss: 0.8345 (Average: 0.7974)\tPrecision: 67.188 (Average: 71.741)\n",
      "Epoch: No: [38] Batches: [400/782]\tLoss: 0.8539 (Average: 0.7996)\tPrecision: 68.750 (Average: 71.828)\n",
      "Epoch: No: [38] Batches: [450/782]\tLoss: 0.8487 (Average: 0.7952)\tPrecision: 75.000 (Average: 71.948)\n",
      "Epoch: No: [38] Batches: [500/782]\tLoss: 0.7837 (Average: 0.7975)\tPrecision: 73.438 (Average: 71.900)\n",
      "Epoch: No: [38] Batches: [550/782]\tLoss: 0.8386 (Average: 0.8000)\tPrecision: 64.062 (Average: 71.858)\n",
      "Epoch: No: [38] Batches: [600/782]\tLoss: 1.1748 (Average: 0.8011)\tPrecision: 57.812 (Average: 71.807)\n",
      "Epoch: No: [38] Batches: [650/782]\tLoss: 0.9170 (Average: 0.8008)\tPrecision: 65.625 (Average: 71.820)\n",
      "Epoch: No: [38] Batches: [700/782]\tLoss: 1.1170 (Average: 0.8034)\tPrecision: 65.625 (Average: 71.715)\n",
      "Epoch: No: [38] Batches: [750/782]\tLoss: 0.6787 (Average: 0.8022)\tPrecision: 76.562 (Average: 71.758)\n",
      "Test Accuracy\t  Top Precision: 69.870 (Error: 30.130 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [39] Batches: [0/782]\tLoss: 0.7466 (Average: 0.7466)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [39] Batches: [50/782]\tLoss: 0.6716 (Average: 0.8183)\tPrecision: 81.250 (Average: 71.048)\n",
      "Epoch: No: [39] Batches: [100/782]\tLoss: 0.5001 (Average: 0.7877)\tPrecision: 82.812 (Average: 72.463)\n",
      "Epoch: No: [39] Batches: [150/782]\tLoss: 1.1850 (Average: 0.7817)\tPrecision: 59.375 (Average: 72.568)\n",
      "Epoch: No: [39] Batches: [200/782]\tLoss: 0.9088 (Average: 0.7903)\tPrecision: 70.312 (Average: 72.240)\n",
      "Epoch: No: [39] Batches: [250/782]\tLoss: 0.5949 (Average: 0.7901)\tPrecision: 78.125 (Average: 72.317)\n",
      "Epoch: No: [39] Batches: [300/782]\tLoss: 0.7343 (Average: 0.7931)\tPrecision: 71.875 (Average: 72.353)\n",
      "Epoch: No: [39] Batches: [350/782]\tLoss: 1.1062 (Average: 0.7963)\tPrecision: 67.188 (Average: 72.222)\n",
      "Epoch: No: [39] Batches: [400/782]\tLoss: 0.8446 (Average: 0.7938)\tPrecision: 64.062 (Average: 72.233)\n",
      "Epoch: No: [39] Batches: [450/782]\tLoss: 0.7454 (Average: 0.7933)\tPrecision: 71.875 (Average: 72.232)\n",
      "Epoch: No: [39] Batches: [500/782]\tLoss: 0.6736 (Average: 0.7901)\tPrecision: 73.438 (Average: 72.315)\n",
      "Epoch: No: [39] Batches: [550/782]\tLoss: 0.9426 (Average: 0.7930)\tPrecision: 67.188 (Average: 72.215)\n",
      "Epoch: No: [39] Batches: [600/782]\tLoss: 0.6545 (Average: 0.7947)\tPrecision: 70.312 (Average: 72.135)\n",
      "Epoch: No: [39] Batches: [650/782]\tLoss: 0.9434 (Average: 0.7976)\tPrecision: 68.750 (Average: 72.074)\n",
      "Epoch: No: [39] Batches: [700/782]\tLoss: 0.7496 (Average: 0.7979)\tPrecision: 70.312 (Average: 72.076)\n",
      "Epoch: No: [39] Batches: [750/782]\tLoss: 0.7777 (Average: 0.7984)\tPrecision: 75.000 (Average: 72.093)\n",
      "Test Accuracy\t  Top Precision: 68.080 (Error: 31.920 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [40] Batches: [0/782]\tLoss: 0.7467 (Average: 0.7467)\tPrecision: 67.188 (Average: 67.188)\n",
      "Epoch: No: [40] Batches: [50/782]\tLoss: 0.6924 (Average: 0.7853)\tPrecision: 76.562 (Average: 72.181)\n",
      "Epoch: No: [40] Batches: [100/782]\tLoss: 0.7976 (Average: 0.7950)\tPrecision: 75.000 (Average: 71.860)\n",
      "Epoch: No: [40] Batches: [150/782]\tLoss: 0.8413 (Average: 0.7919)\tPrecision: 67.188 (Average: 72.041)\n",
      "Epoch: No: [40] Batches: [200/782]\tLoss: 0.5631 (Average: 0.7948)\tPrecision: 85.938 (Average: 71.953)\n",
      "Epoch: No: [40] Batches: [250/782]\tLoss: 0.7458 (Average: 0.7906)\tPrecision: 76.562 (Average: 72.112)\n",
      "Epoch: No: [40] Batches: [300/782]\tLoss: 0.6923 (Average: 0.7895)\tPrecision: 73.438 (Average: 72.275)\n",
      "Epoch: No: [40] Batches: [350/782]\tLoss: 0.8464 (Average: 0.7850)\tPrecision: 67.188 (Average: 72.423)\n",
      "Epoch: No: [40] Batches: [400/782]\tLoss: 0.9235 (Average: 0.7882)\tPrecision: 68.750 (Average: 72.284)\n",
      "Epoch: No: [40] Batches: [450/782]\tLoss: 0.8927 (Average: 0.7874)\tPrecision: 70.312 (Average: 72.218)\n",
      "Epoch: No: [40] Batches: [500/782]\tLoss: 0.8042 (Average: 0.7885)\tPrecision: 67.188 (Average: 72.243)\n",
      "Epoch: No: [40] Batches: [550/782]\tLoss: 1.0174 (Average: 0.7874)\tPrecision: 64.062 (Average: 72.235)\n",
      "Epoch: No: [40] Batches: [600/782]\tLoss: 0.7607 (Average: 0.7880)\tPrecision: 75.000 (Average: 72.218)\n",
      "Epoch: No: [40] Batches: [650/782]\tLoss: 1.2356 (Average: 0.7891)\tPrecision: 57.812 (Average: 72.228)\n",
      "Epoch: No: [40] Batches: [700/782]\tLoss: 0.6997 (Average: 0.7878)\tPrecision: 76.562 (Average: 72.232)\n",
      "Epoch: No: [40] Batches: [750/782]\tLoss: 0.8340 (Average: 0.7866)\tPrecision: 70.312 (Average: 72.339)\n",
      "Test Accuracy\t  Top Precision: 72.080 (Error: 27.920 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [41] Batches: [0/782]\tLoss: 0.6704 (Average: 0.6704)\tPrecision: 78.125 (Average: 78.125)\n",
      "Epoch: No: [41] Batches: [50/782]\tLoss: 0.5983 (Average: 0.7626)\tPrecision: 75.000 (Average: 73.009)\n",
      "Epoch: No: [41] Batches: [100/782]\tLoss: 0.7592 (Average: 0.7504)\tPrecision: 71.875 (Average: 73.438)\n",
      "Epoch: No: [41] Batches: [150/782]\tLoss: 0.7938 (Average: 0.7572)\tPrecision: 71.875 (Average: 73.220)\n",
      "Epoch: No: [41] Batches: [200/782]\tLoss: 0.6622 (Average: 0.7638)\tPrecision: 70.312 (Average: 73.212)\n",
      "Epoch: No: [41] Batches: [250/782]\tLoss: 0.8685 (Average: 0.7754)\tPrecision: 64.062 (Average: 72.659)\n",
      "Epoch: No: [41] Batches: [300/782]\tLoss: 0.6580 (Average: 0.7687)\tPrecision: 76.562 (Average: 72.892)\n",
      "Epoch: No: [41] Batches: [350/782]\tLoss: 0.6973 (Average: 0.7713)\tPrecision: 70.312 (Average: 72.788)\n",
      "Epoch: No: [41] Batches: [400/782]\tLoss: 0.6954 (Average: 0.7747)\tPrecision: 73.438 (Average: 72.740)\n",
      "Epoch: No: [41] Batches: [450/782]\tLoss: 0.7483 (Average: 0.7759)\tPrecision: 75.000 (Average: 72.776)\n",
      "Epoch: No: [41] Batches: [500/782]\tLoss: 0.8613 (Average: 0.7769)\tPrecision: 73.438 (Average: 72.677)\n",
      "Epoch: No: [41] Batches: [550/782]\tLoss: 0.8802 (Average: 0.7811)\tPrecision: 65.625 (Average: 72.490)\n",
      "Epoch: No: [41] Batches: [600/782]\tLoss: 0.6841 (Average: 0.7802)\tPrecision: 67.188 (Average: 72.496)\n",
      "Epoch: No: [41] Batches: [650/782]\tLoss: 0.6187 (Average: 0.7803)\tPrecision: 70.312 (Average: 72.509)\n",
      "Epoch: No: [41] Batches: [700/782]\tLoss: 0.8231 (Average: 0.7810)\tPrecision: 67.188 (Average: 72.484)\n",
      "Epoch: No: [41] Batches: [750/782]\tLoss: 0.7109 (Average: 0.7820)\tPrecision: 75.000 (Average: 72.397)\n",
      "Test Accuracy\t  Top Precision: 67.080 (Error: 32.920 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [42] Batches: [0/782]\tLoss: 0.7824 (Average: 0.7824)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [42] Batches: [50/782]\tLoss: 0.8649 (Average: 0.7281)\tPrecision: 75.000 (Average: 74.939)\n",
      "Epoch: No: [42] Batches: [100/782]\tLoss: 0.8533 (Average: 0.7463)\tPrecision: 75.000 (Average: 73.994)\n",
      "Epoch: No: [42] Batches: [150/782]\tLoss: 0.7475 (Average: 0.7691)\tPrecision: 75.000 (Average: 73.386)\n",
      "Epoch: No: [42] Batches: [200/782]\tLoss: 1.1411 (Average: 0.7708)\tPrecision: 64.062 (Average: 73.298)\n",
      "Epoch: No: [42] Batches: [250/782]\tLoss: 0.6947 (Average: 0.7811)\tPrecision: 75.000 (Average: 72.958)\n",
      "Epoch: No: [42] Batches: [300/782]\tLoss: 0.9382 (Average: 0.7791)\tPrecision: 68.750 (Average: 73.053)\n",
      "Epoch: No: [42] Batches: [350/782]\tLoss: 0.5358 (Average: 0.7772)\tPrecision: 81.250 (Average: 72.934)\n",
      "Epoch: No: [42] Batches: [400/782]\tLoss: 0.5241 (Average: 0.7786)\tPrecision: 79.688 (Average: 72.791)\n",
      "Epoch: No: [42] Batches: [450/782]\tLoss: 0.8665 (Average: 0.7819)\tPrecision: 68.750 (Average: 72.641)\n",
      "Epoch: No: [42] Batches: [500/782]\tLoss: 0.7087 (Average: 0.7792)\tPrecision: 75.000 (Average: 72.726)\n",
      "Epoch: No: [42] Batches: [550/782]\tLoss: 0.7091 (Average: 0.7801)\tPrecision: 76.562 (Average: 72.621)\n",
      "Epoch: No: [42] Batches: [600/782]\tLoss: 0.9001 (Average: 0.7801)\tPrecision: 65.625 (Average: 72.564)\n",
      "Epoch: No: [42] Batches: [650/782]\tLoss: 0.9352 (Average: 0.7801)\tPrecision: 68.750 (Average: 72.600)\n",
      "Epoch: No: [42] Batches: [700/782]\tLoss: 0.7115 (Average: 0.7800)\tPrecision: 71.875 (Average: 72.613)\n",
      "Epoch: No: [42] Batches: [750/782]\tLoss: 0.7967 (Average: 0.7806)\tPrecision: 75.000 (Average: 72.562)\n",
      "Test Accuracy\t  Top Precision: 69.130 (Error: 30.870 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [43] Batches: [0/782]\tLoss: 0.7941 (Average: 0.7941)\tPrecision: 71.875 (Average: 71.875)\n",
      "Epoch: No: [43] Batches: [50/782]\tLoss: 0.8508 (Average: 0.8234)\tPrecision: 70.312 (Average: 71.385)\n",
      "Epoch: No: [43] Batches: [100/782]\tLoss: 0.7563 (Average: 0.7995)\tPrecision: 71.875 (Average: 71.829)\n",
      "Epoch: No: [43] Batches: [150/782]\tLoss: 1.0844 (Average: 0.7904)\tPrecision: 68.750 (Average: 71.947)\n",
      "Epoch: No: [43] Batches: [200/782]\tLoss: 0.6697 (Average: 0.7747)\tPrecision: 78.125 (Average: 72.582)\n",
      "Epoch: No: [43] Batches: [250/782]\tLoss: 0.8103 (Average: 0.7729)\tPrecision: 73.438 (Average: 72.722)\n",
      "Epoch: No: [43] Batches: [300/782]\tLoss: 0.8419 (Average: 0.7724)\tPrecision: 70.312 (Average: 72.851)\n",
      "Epoch: No: [43] Batches: [350/782]\tLoss: 0.5608 (Average: 0.7679)\tPrecision: 75.000 (Average: 72.970)\n",
      "Epoch: No: [43] Batches: [400/782]\tLoss: 0.6217 (Average: 0.7664)\tPrecision: 73.438 (Average: 72.989)\n",
      "Epoch: No: [43] Batches: [450/782]\tLoss: 0.7275 (Average: 0.7645)\tPrecision: 70.312 (Average: 72.987)\n",
      "Epoch: No: [43] Batches: [500/782]\tLoss: 0.6450 (Average: 0.7686)\tPrecision: 81.250 (Average: 72.879)\n",
      "Epoch: No: [43] Batches: [550/782]\tLoss: 0.6254 (Average: 0.7651)\tPrecision: 78.125 (Average: 72.953)\n",
      "Epoch: No: [43] Batches: [600/782]\tLoss: 0.7749 (Average: 0.7658)\tPrecision: 73.438 (Average: 72.907)\n",
      "Epoch: No: [43] Batches: [650/782]\tLoss: 0.5801 (Average: 0.7672)\tPrecision: 79.688 (Average: 72.852)\n",
      "Epoch: No: [43] Batches: [700/782]\tLoss: 0.5410 (Average: 0.7663)\tPrecision: 78.125 (Average: 72.909)\n",
      "Epoch: No: [43] Batches: [750/782]\tLoss: 0.7039 (Average: 0.7637)\tPrecision: 75.000 (Average: 73.067)\n",
      "Test Accuracy\t  Top Precision: 72.370 (Error: 27.630 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [44] Batches: [0/782]\tLoss: 0.5861 (Average: 0.5861)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [44] Batches: [50/782]\tLoss: 0.7498 (Average: 0.7375)\tPrecision: 71.875 (Average: 74.020)\n",
      "Epoch: No: [44] Batches: [100/782]\tLoss: 0.8793 (Average: 0.7275)\tPrecision: 68.750 (Average: 74.923)\n",
      "Epoch: No: [44] Batches: [150/782]\tLoss: 0.6393 (Average: 0.7282)\tPrecision: 84.375 (Average: 74.555)\n",
      "Epoch: No: [44] Batches: [200/782]\tLoss: 0.8615 (Average: 0.7295)\tPrecision: 71.875 (Average: 74.433)\n",
      "Epoch: No: [44] Batches: [250/782]\tLoss: 0.6841 (Average: 0.7448)\tPrecision: 73.438 (Average: 73.954)\n",
      "Epoch: No: [44] Batches: [300/782]\tLoss: 1.1508 (Average: 0.7438)\tPrecision: 59.375 (Average: 73.931)\n",
      "Epoch: No: [44] Batches: [350/782]\tLoss: 0.8310 (Average: 0.7450)\tPrecision: 73.438 (Average: 73.776)\n",
      "Epoch: No: [44] Batches: [400/782]\tLoss: 0.7776 (Average: 0.7465)\tPrecision: 68.750 (Average: 73.663)\n",
      "Epoch: No: [44] Batches: [450/782]\tLoss: 0.9732 (Average: 0.7496)\tPrecision: 62.500 (Average: 73.444)\n",
      "Epoch: No: [44] Batches: [500/782]\tLoss: 0.6656 (Average: 0.7528)\tPrecision: 76.562 (Average: 73.338)\n",
      "Epoch: No: [44] Batches: [550/782]\tLoss: 0.6271 (Average: 0.7532)\tPrecision: 78.125 (Average: 73.415)\n",
      "Epoch: No: [44] Batches: [600/782]\tLoss: 0.5684 (Average: 0.7553)\tPrecision: 82.812 (Average: 73.380)\n",
      "Epoch: No: [44] Batches: [650/782]\tLoss: 0.8618 (Average: 0.7531)\tPrecision: 70.312 (Average: 73.404)\n",
      "Epoch: No: [44] Batches: [700/782]\tLoss: 0.7420 (Average: 0.7527)\tPrecision: 67.188 (Average: 73.438)\n",
      "Epoch: No: [44] Batches: [750/782]\tLoss: 0.7273 (Average: 0.7557)\tPrecision: 73.438 (Average: 73.361)\n",
      "Test Accuracy\t  Top Precision: 70.080 (Error: 29.920 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [45] Batches: [0/782]\tLoss: 0.6603 (Average: 0.6603)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [45] Batches: [50/782]\tLoss: 0.6956 (Average: 0.7078)\tPrecision: 73.438 (Average: 75.123)\n",
      "Epoch: No: [45] Batches: [100/782]\tLoss: 0.8407 (Average: 0.7380)\tPrecision: 71.875 (Average: 73.530)\n",
      "Epoch: No: [45] Batches: [150/782]\tLoss: 0.7859 (Average: 0.7408)\tPrecision: 76.562 (Average: 73.510)\n",
      "Epoch: No: [45] Batches: [200/782]\tLoss: 0.7546 (Average: 0.7377)\tPrecision: 71.875 (Average: 73.546)\n",
      "Epoch: No: [45] Batches: [250/782]\tLoss: 0.5660 (Average: 0.7455)\tPrecision: 82.812 (Average: 73.288)\n",
      "Epoch: No: [45] Batches: [300/782]\tLoss: 1.0328 (Average: 0.7493)\tPrecision: 64.062 (Average: 73.365)\n",
      "Epoch: No: [45] Batches: [350/782]\tLoss: 0.9094 (Average: 0.7470)\tPrecision: 68.750 (Average: 73.513)\n",
      "Epoch: No: [45] Batches: [400/782]\tLoss: 0.5428 (Average: 0.7413)\tPrecision: 79.688 (Average: 73.780)\n",
      "Epoch: No: [45] Batches: [450/782]\tLoss: 0.6107 (Average: 0.7417)\tPrecision: 76.562 (Average: 73.753)\n",
      "Epoch: No: [45] Batches: [500/782]\tLoss: 0.8429 (Average: 0.7408)\tPrecision: 68.750 (Average: 73.771)\n",
      "Epoch: No: [45] Batches: [550/782]\tLoss: 0.8981 (Average: 0.7430)\tPrecision: 62.500 (Average: 73.679)\n",
      "Epoch: No: [45] Batches: [600/782]\tLoss: 0.7317 (Average: 0.7462)\tPrecision: 70.312 (Average: 73.596)\n",
      "Epoch: No: [45] Batches: [650/782]\tLoss: 0.7360 (Average: 0.7464)\tPrecision: 71.875 (Average: 73.586)\n",
      "Epoch: No: [45] Batches: [700/782]\tLoss: 0.4609 (Average: 0.7461)\tPrecision: 84.375 (Average: 73.551)\n",
      "Epoch: No: [45] Batches: [750/782]\tLoss: 0.7030 (Average: 0.7468)\tPrecision: 73.438 (Average: 73.539)\n",
      "Test Accuracy\t  Top Precision: 72.580 (Error: 27.420 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [46] Batches: [0/782]\tLoss: 0.7253 (Average: 0.7253)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [46] Batches: [50/782]\tLoss: 0.7929 (Average: 0.7490)\tPrecision: 71.875 (Average: 73.438)\n",
      "Epoch: No: [46] Batches: [100/782]\tLoss: 0.8525 (Average: 0.7469)\tPrecision: 73.438 (Average: 73.546)\n",
      "Epoch: No: [46] Batches: [150/782]\tLoss: 0.6602 (Average: 0.7366)\tPrecision: 71.875 (Average: 73.738)\n",
      "Epoch: No: [46] Batches: [200/782]\tLoss: 0.9489 (Average: 0.7345)\tPrecision: 65.625 (Average: 73.678)\n",
      "Epoch: No: [46] Batches: [250/782]\tLoss: 0.8712 (Average: 0.7390)\tPrecision: 65.625 (Average: 73.618)\n",
      "Epoch: No: [46] Batches: [300/782]\tLoss: 0.9040 (Average: 0.7428)\tPrecision: 75.000 (Average: 73.604)\n",
      "Epoch: No: [46] Batches: [350/782]\tLoss: 0.6873 (Average: 0.7474)\tPrecision: 73.438 (Average: 73.344)\n",
      "Epoch: No: [46] Batches: [400/782]\tLoss: 0.7397 (Average: 0.7504)\tPrecision: 78.125 (Average: 73.278)\n",
      "Epoch: No: [46] Batches: [450/782]\tLoss: 0.5771 (Average: 0.7468)\tPrecision: 81.250 (Average: 73.389)\n",
      "Epoch: No: [46] Batches: [500/782]\tLoss: 0.7453 (Average: 0.7487)\tPrecision: 71.875 (Average: 73.403)\n",
      "Epoch: No: [46] Batches: [550/782]\tLoss: 0.5034 (Average: 0.7484)\tPrecision: 81.250 (Average: 73.472)\n",
      "Epoch: No: [46] Batches: [600/782]\tLoss: 0.8870 (Average: 0.7465)\tPrecision: 70.312 (Average: 73.456)\n",
      "Epoch: No: [46] Batches: [650/782]\tLoss: 0.8010 (Average: 0.7479)\tPrecision: 71.875 (Average: 73.440)\n",
      "Epoch: No: [46] Batches: [700/782]\tLoss: 0.7611 (Average: 0.7473)\tPrecision: 73.438 (Average: 73.478)\n",
      "Epoch: No: [46] Batches: [750/782]\tLoss: 0.7684 (Average: 0.7477)\tPrecision: 70.312 (Average: 73.481)\n",
      "Test Accuracy\t  Top Precision: 66.530 (Error: 33.470 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [47] Batches: [0/782]\tLoss: 0.9344 (Average: 0.9344)\tPrecision: 67.188 (Average: 67.188)\n",
      "Epoch: No: [47] Batches: [50/782]\tLoss: 0.6802 (Average: 0.7445)\tPrecision: 68.750 (Average: 74.081)\n",
      "Epoch: No: [47] Batches: [100/782]\tLoss: 0.7464 (Average: 0.7433)\tPrecision: 76.562 (Average: 74.165)\n",
      "Epoch: No: [47] Batches: [150/782]\tLoss: 0.6555 (Average: 0.7505)\tPrecision: 73.438 (Average: 73.707)\n",
      "Epoch: No: [47] Batches: [200/782]\tLoss: 0.8140 (Average: 0.7434)\tPrecision: 65.625 (Average: 74.122)\n",
      "Epoch: No: [47] Batches: [250/782]\tLoss: 0.9076 (Average: 0.7484)\tPrecision: 73.438 (Average: 73.792)\n",
      "Epoch: No: [47] Batches: [300/782]\tLoss: 0.6915 (Average: 0.7514)\tPrecision: 76.562 (Average: 73.728)\n",
      "Epoch: No: [47] Batches: [350/782]\tLoss: 0.9611 (Average: 0.7463)\tPrecision: 68.750 (Average: 73.896)\n",
      "Epoch: No: [47] Batches: [400/782]\tLoss: 0.7398 (Average: 0.7463)\tPrecision: 71.875 (Average: 73.870)\n",
      "Epoch: No: [47] Batches: [450/782]\tLoss: 0.7615 (Average: 0.7462)\tPrecision: 73.438 (Average: 73.836)\n",
      "Epoch: No: [47] Batches: [500/782]\tLoss: 0.6231 (Average: 0.7443)\tPrecision: 79.688 (Average: 73.905)\n",
      "Epoch: No: [47] Batches: [550/782]\tLoss: 0.6560 (Average: 0.7424)\tPrecision: 79.688 (Average: 73.985)\n",
      "Epoch: No: [47] Batches: [600/782]\tLoss: 0.8804 (Average: 0.7445)\tPrecision: 70.312 (Average: 73.877)\n",
      "Epoch: No: [47] Batches: [650/782]\tLoss: 0.5059 (Average: 0.7435)\tPrecision: 81.250 (Average: 73.975)\n",
      "Epoch: No: [47] Batches: [700/782]\tLoss: 0.7067 (Average: 0.7437)\tPrecision: 70.312 (Average: 74.006)\n",
      "Epoch: No: [47] Batches: [750/782]\tLoss: 0.6875 (Average: 0.7431)\tPrecision: 75.000 (Average: 73.970)\n",
      "Test Accuracy\t  Top Precision: 68.130 (Error: 31.870 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [48] Batches: [0/782]\tLoss: 0.8743 (Average: 0.8743)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [48] Batches: [50/782]\tLoss: 0.8857 (Average: 0.7273)\tPrecision: 73.438 (Average: 74.173)\n",
      "Epoch: No: [48] Batches: [100/782]\tLoss: 0.8523 (Average: 0.7112)\tPrecision: 73.438 (Average: 74.737)\n",
      "Epoch: No: [48] Batches: [150/782]\tLoss: 0.5894 (Average: 0.7318)\tPrecision: 78.125 (Average: 74.245)\n",
      "Epoch: No: [48] Batches: [200/782]\tLoss: 0.7790 (Average: 0.7294)\tPrecision: 71.875 (Average: 74.487)\n",
      "Epoch: No: [48] Batches: [250/782]\tLoss: 0.5360 (Average: 0.7276)\tPrecision: 81.250 (Average: 74.502)\n",
      "Epoch: No: [48] Batches: [300/782]\tLoss: 0.8010 (Average: 0.7304)\tPrecision: 78.125 (Average: 74.273)\n",
      "Epoch: No: [48] Batches: [350/782]\tLoss: 0.6795 (Average: 0.7282)\tPrecision: 79.688 (Average: 74.475)\n",
      "Epoch: No: [48] Batches: [400/782]\tLoss: 0.7621 (Average: 0.7265)\tPrecision: 76.562 (Average: 74.669)\n",
      "Epoch: No: [48] Batches: [450/782]\tLoss: 0.6246 (Average: 0.7291)\tPrecision: 78.125 (Average: 74.574)\n",
      "Epoch: No: [48] Batches: [500/782]\tLoss: 0.6921 (Average: 0.7324)\tPrecision: 71.875 (Average: 74.420)\n",
      "Epoch: No: [48] Batches: [550/782]\tLoss: 1.0282 (Average: 0.7332)\tPrecision: 68.750 (Average: 74.382)\n",
      "Epoch: No: [48] Batches: [600/782]\tLoss: 0.8175 (Average: 0.7347)\tPrecision: 67.188 (Average: 74.321)\n",
      "Epoch: No: [48] Batches: [650/782]\tLoss: 0.6408 (Average: 0.7314)\tPrecision: 79.688 (Average: 74.414)\n",
      "Epoch: No: [48] Batches: [700/782]\tLoss: 0.7052 (Average: 0.7301)\tPrecision: 76.562 (Average: 74.391)\n",
      "Epoch: No: [48] Batches: [750/782]\tLoss: 0.7946 (Average: 0.7320)\tPrecision: 65.625 (Average: 74.301)\n",
      "Test Accuracy\t  Top Precision: 69.200 (Error: 30.800 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [49] Batches: [0/782]\tLoss: 0.6246 (Average: 0.6246)\tPrecision: 84.375 (Average: 84.375)\n",
      "Epoch: No: [49] Batches: [50/782]\tLoss: 0.6038 (Average: 0.7142)\tPrecision: 79.688 (Average: 74.418)\n",
      "Epoch: No: [49] Batches: [100/782]\tLoss: 0.5433 (Average: 0.7253)\tPrecision: 82.812 (Average: 74.288)\n",
      "Epoch: No: [49] Batches: [150/782]\tLoss: 0.6643 (Average: 0.7265)\tPrecision: 76.562 (Average: 74.379)\n",
      "Epoch: No: [49] Batches: [200/782]\tLoss: 0.6843 (Average: 0.7255)\tPrecision: 78.125 (Average: 74.456)\n",
      "Epoch: No: [49] Batches: [250/782]\tLoss: 0.9412 (Average: 0.7352)\tPrecision: 60.938 (Average: 74.309)\n",
      "Epoch: No: [49] Batches: [300/782]\tLoss: 0.5866 (Average: 0.7327)\tPrecision: 79.688 (Average: 74.393)\n",
      "Epoch: No: [49] Batches: [350/782]\tLoss: 0.7730 (Average: 0.7283)\tPrecision: 71.875 (Average: 74.475)\n",
      "Epoch: No: [49] Batches: [400/782]\tLoss: 0.6830 (Average: 0.7295)\tPrecision: 71.875 (Average: 74.295)\n",
      "Epoch: No: [49] Batches: [450/782]\tLoss: 0.7903 (Average: 0.7333)\tPrecision: 67.188 (Average: 74.186)\n",
      "Epoch: No: [49] Batches: [500/782]\tLoss: 0.5944 (Average: 0.7336)\tPrecision: 78.125 (Average: 74.214)\n",
      "Epoch: No: [49] Batches: [550/782]\tLoss: 0.5688 (Average: 0.7312)\tPrecision: 81.250 (Average: 74.328)\n",
      "Epoch: No: [49] Batches: [600/782]\tLoss: 0.5742 (Average: 0.7287)\tPrecision: 75.000 (Average: 74.384)\n",
      "Epoch: No: [49] Batches: [650/782]\tLoss: 0.6643 (Average: 0.7300)\tPrecision: 76.562 (Average: 74.381)\n",
      "Epoch: No: [49] Batches: [700/782]\tLoss: 0.7446 (Average: 0.7286)\tPrecision: 67.188 (Average: 74.447)\n",
      "Epoch: No: [49] Batches: [750/782]\tLoss: 0.7265 (Average: 0.7286)\tPrecision: 78.125 (Average: 74.417)\n",
      "Test Accuracy\t  Top Precision: 68.750 (Error: 31.250 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [50] Batches: [0/782]\tLoss: 0.5172 (Average: 0.5172)\tPrecision: 79.688 (Average: 79.688)\n",
      "Epoch: No: [50] Batches: [50/782]\tLoss: 0.8720 (Average: 0.7340)\tPrecision: 68.750 (Average: 74.571)\n",
      "Epoch: No: [50] Batches: [100/782]\tLoss: 0.7217 (Average: 0.7320)\tPrecision: 78.125 (Average: 74.242)\n",
      "Epoch: No: [50] Batches: [150/782]\tLoss: 0.6742 (Average: 0.7296)\tPrecision: 79.688 (Average: 74.421)\n",
      "Epoch: No: [50] Batches: [200/782]\tLoss: 0.6850 (Average: 0.7334)\tPrecision: 76.562 (Average: 74.355)\n",
      "Epoch: No: [50] Batches: [250/782]\tLoss: 0.8896 (Average: 0.7332)\tPrecision: 71.875 (Average: 74.272)\n",
      "Epoch: No: [50] Batches: [300/782]\tLoss: 0.5534 (Average: 0.7309)\tPrecision: 76.562 (Average: 74.408)\n",
      "Epoch: No: [50] Batches: [350/782]\tLoss: 1.1840 (Average: 0.7332)\tPrecision: 76.562 (Average: 74.439)\n",
      "Epoch: No: [50] Batches: [400/782]\tLoss: 0.8110 (Average: 0.7281)\tPrecision: 73.438 (Average: 74.735)\n",
      "Epoch: No: [50] Batches: [450/782]\tLoss: 0.6436 (Average: 0.7278)\tPrecision: 75.000 (Average: 74.674)\n",
      "Epoch: No: [50] Batches: [500/782]\tLoss: 0.4376 (Average: 0.7243)\tPrecision: 84.375 (Average: 74.750)\n",
      "Epoch: No: [50] Batches: [550/782]\tLoss: 0.6477 (Average: 0.7281)\tPrecision: 78.125 (Average: 74.643)\n",
      "Epoch: No: [50] Batches: [600/782]\tLoss: 0.7077 (Average: 0.7252)\tPrecision: 68.750 (Average: 74.722)\n",
      "Epoch: No: [50] Batches: [650/782]\tLoss: 0.8638 (Average: 0.7258)\tPrecision: 71.875 (Average: 74.700)\n",
      "Epoch: No: [50] Batches: [700/782]\tLoss: 1.1501 (Average: 0.7231)\tPrecision: 67.188 (Average: 74.775)\n",
      "Epoch: No: [50] Batches: [750/782]\tLoss: 0.5472 (Average: 0.7218)\tPrecision: 81.250 (Average: 74.913)\n",
      "Test Accuracy\t  Top Precision: 69.210 (Error: 30.790 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [51] Batches: [0/782]\tLoss: 0.6357 (Average: 0.6357)\tPrecision: 73.438 (Average: 73.438)\n",
      "Epoch: No: [51] Batches: [50/782]\tLoss: 0.6395 (Average: 0.7007)\tPrecision: 76.562 (Average: 75.521)\n",
      "Epoch: No: [51] Batches: [100/782]\tLoss: 1.0017 (Average: 0.6879)\tPrecision: 65.625 (Average: 75.913)\n",
      "Epoch: No: [51] Batches: [150/782]\tLoss: 0.7169 (Average: 0.6942)\tPrecision: 76.562 (Average: 75.590)\n",
      "Epoch: No: [51] Batches: [200/782]\tLoss: 0.5396 (Average: 0.7031)\tPrecision: 75.000 (Average: 75.093)\n",
      "Epoch: No: [51] Batches: [250/782]\tLoss: 0.6427 (Average: 0.7056)\tPrecision: 76.562 (Average: 75.081)\n",
      "Epoch: No: [51] Batches: [300/782]\tLoss: 0.6970 (Average: 0.7004)\tPrecision: 68.750 (Average: 75.187)\n",
      "Epoch: No: [51] Batches: [350/782]\tLoss: 0.7384 (Average: 0.7067)\tPrecision: 68.750 (Average: 74.969)\n",
      "Epoch: No: [51] Batches: [400/782]\tLoss: 0.7388 (Average: 0.7097)\tPrecision: 73.438 (Average: 74.934)\n",
      "Epoch: No: [51] Batches: [450/782]\tLoss: 0.6860 (Average: 0.7104)\tPrecision: 76.562 (Average: 74.965)\n",
      "Epoch: No: [51] Batches: [500/782]\tLoss: 0.9227 (Average: 0.7095)\tPrecision: 75.000 (Average: 74.966)\n",
      "Epoch: No: [51] Batches: [550/782]\tLoss: 0.7643 (Average: 0.7123)\tPrecision: 76.562 (Average: 74.906)\n",
      "Epoch: No: [51] Batches: [600/782]\tLoss: 0.7866 (Average: 0.7123)\tPrecision: 73.438 (Average: 74.914)\n",
      "Epoch: No: [51] Batches: [650/782]\tLoss: 0.5979 (Average: 0.7111)\tPrecision: 75.000 (Average: 74.959)\n",
      "Epoch: No: [51] Batches: [700/782]\tLoss: 0.5668 (Average: 0.7120)\tPrecision: 79.688 (Average: 74.924)\n",
      "Epoch: No: [51] Batches: [750/782]\tLoss: 0.5699 (Average: 0.7121)\tPrecision: 78.125 (Average: 74.898)\n",
      "Test Accuracy\t  Top Precision: 73.660 (Error: 26.340 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [52] Batches: [0/782]\tLoss: 0.6946 (Average: 0.6946)\tPrecision: 71.875 (Average: 71.875)\n",
      "Epoch: No: [52] Batches: [50/782]\tLoss: 0.5245 (Average: 0.7144)\tPrecision: 78.125 (Average: 74.908)\n",
      "Epoch: No: [52] Batches: [100/782]\tLoss: 0.6834 (Average: 0.7060)\tPrecision: 76.562 (Average: 75.557)\n",
      "Epoch: No: [52] Batches: [150/782]\tLoss: 0.6536 (Average: 0.7074)\tPrecision: 79.688 (Average: 75.197)\n",
      "Epoch: No: [52] Batches: [200/782]\tLoss: 0.6950 (Average: 0.7110)\tPrecision: 76.562 (Average: 74.868)\n",
      "Epoch: No: [52] Batches: [250/782]\tLoss: 1.2550 (Average: 0.7097)\tPrecision: 64.062 (Average: 74.950)\n",
      "Epoch: No: [52] Batches: [300/782]\tLoss: 0.7695 (Average: 0.7088)\tPrecision: 70.312 (Average: 75.057)\n",
      "Epoch: No: [52] Batches: [350/782]\tLoss: 0.6202 (Average: 0.7105)\tPrecision: 75.000 (Average: 75.134)\n",
      "Epoch: No: [52] Batches: [400/782]\tLoss: 0.8780 (Average: 0.7121)\tPrecision: 68.750 (Average: 75.094)\n",
      "Epoch: No: [52] Batches: [450/782]\tLoss: 0.9151 (Average: 0.7131)\tPrecision: 73.438 (Average: 75.094)\n",
      "Epoch: No: [52] Batches: [500/782]\tLoss: 0.5265 (Average: 0.7177)\tPrecision: 82.812 (Average: 74.991)\n",
      "Epoch: No: [52] Batches: [550/782]\tLoss: 0.7543 (Average: 0.7164)\tPrecision: 68.750 (Average: 74.977)\n",
      "Epoch: No: [52] Batches: [600/782]\tLoss: 0.9964 (Average: 0.7171)\tPrecision: 57.812 (Average: 74.992)\n",
      "Epoch: No: [52] Batches: [650/782]\tLoss: 0.6404 (Average: 0.7170)\tPrecision: 79.688 (Average: 75.012)\n",
      "Epoch: No: [52] Batches: [700/782]\tLoss: 0.6029 (Average: 0.7175)\tPrecision: 76.562 (Average: 75.013)\n",
      "Epoch: No: [52] Batches: [750/782]\tLoss: 0.7357 (Average: 0.7181)\tPrecision: 68.750 (Average: 74.983)\n",
      "Test Accuracy\t  Top Precision: 72.360 (Error: 27.640 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [53] Batches: [0/782]\tLoss: 0.6119 (Average: 0.6119)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [53] Batches: [50/782]\tLoss: 0.7277 (Average: 0.7036)\tPrecision: 75.000 (Average: 75.368)\n",
      "Epoch: No: [53] Batches: [100/782]\tLoss: 0.6560 (Average: 0.7110)\tPrecision: 75.000 (Average: 75.031)\n",
      "Epoch: No: [53] Batches: [150/782]\tLoss: 0.4180 (Average: 0.6962)\tPrecision: 87.500 (Average: 75.455)\n",
      "Epoch: No: [53] Batches: [200/782]\tLoss: 0.6527 (Average: 0.6925)\tPrecision: 73.438 (Average: 75.777)\n",
      "Epoch: No: [53] Batches: [250/782]\tLoss: 0.7510 (Average: 0.6968)\tPrecision: 71.875 (Average: 75.554)\n",
      "Epoch: No: [53] Batches: [300/782]\tLoss: 0.8434 (Average: 0.7065)\tPrecision: 75.000 (Average: 75.254)\n",
      "Epoch: No: [53] Batches: [350/782]\tLoss: 0.4397 (Average: 0.7060)\tPrecision: 81.250 (Average: 75.227)\n",
      "Epoch: No: [53] Batches: [400/782]\tLoss: 0.4981 (Average: 0.7059)\tPrecision: 87.500 (Average: 75.140)\n",
      "Epoch: No: [53] Batches: [450/782]\tLoss: 0.7845 (Average: 0.7029)\tPrecision: 75.000 (Average: 75.253)\n",
      "Epoch: No: [53] Batches: [500/782]\tLoss: 0.8011 (Average: 0.7037)\tPrecision: 71.875 (Average: 75.246)\n",
      "Epoch: No: [53] Batches: [550/782]\tLoss: 0.6909 (Average: 0.7000)\tPrecision: 73.438 (Average: 75.406)\n",
      "Epoch: No: [53] Batches: [600/782]\tLoss: 0.7445 (Average: 0.6994)\tPrecision: 75.000 (Average: 75.471)\n",
      "Epoch: No: [53] Batches: [650/782]\tLoss: 0.7230 (Average: 0.6985)\tPrecision: 71.875 (Average: 75.434)\n",
      "Epoch: No: [53] Batches: [700/782]\tLoss: 0.4821 (Average: 0.6977)\tPrecision: 82.812 (Average: 75.441)\n",
      "Epoch: No: [53] Batches: [750/782]\tLoss: 0.9097 (Average: 0.7025)\tPrecision: 70.312 (Average: 75.356)\n",
      "Test Accuracy\t  Top Precision: 71.240 (Error: 28.760 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [54] Batches: [0/782]\tLoss: 0.5456 (Average: 0.5456)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [54] Batches: [50/782]\tLoss: 0.5674 (Average: 0.6964)\tPrecision: 73.438 (Average: 74.510)\n",
      "Epoch: No: [54] Batches: [100/782]\tLoss: 0.6011 (Average: 0.6800)\tPrecision: 79.688 (Average: 75.681)\n",
      "Epoch: No: [54] Batches: [150/782]\tLoss: 0.5714 (Average: 0.6758)\tPrecision: 82.812 (Average: 75.983)\n",
      "Epoch: No: [54] Batches: [200/782]\tLoss: 0.6495 (Average: 0.6806)\tPrecision: 75.000 (Average: 76.026)\n",
      "Epoch: No: [54] Batches: [250/782]\tLoss: 0.7864 (Average: 0.6885)\tPrecision: 71.875 (Average: 75.847)\n",
      "Epoch: No: [54] Batches: [300/782]\tLoss: 0.6360 (Average: 0.6962)\tPrecision: 70.312 (Average: 75.446)\n",
      "Epoch: No: [54] Batches: [350/782]\tLoss: 0.8219 (Average: 0.7049)\tPrecision: 67.188 (Average: 75.191)\n",
      "Epoch: No: [54] Batches: [400/782]\tLoss: 0.8636 (Average: 0.7009)\tPrecision: 64.062 (Average: 75.292)\n",
      "Epoch: No: [54] Batches: [450/782]\tLoss: 0.6129 (Average: 0.6998)\tPrecision: 71.875 (Average: 75.398)\n",
      "Epoch: No: [54] Batches: [500/782]\tLoss: 0.9313 (Average: 0.6988)\tPrecision: 65.625 (Average: 75.524)\n",
      "Epoch: No: [54] Batches: [550/782]\tLoss: 0.5275 (Average: 0.6965)\tPrecision: 82.812 (Average: 75.587)\n",
      "Epoch: No: [54] Batches: [600/782]\tLoss: 0.7995 (Average: 0.6981)\tPrecision: 71.875 (Average: 75.512)\n",
      "Epoch: No: [54] Batches: [650/782]\tLoss: 0.6217 (Average: 0.6983)\tPrecision: 79.688 (Average: 75.571)\n",
      "Epoch: No: [54] Batches: [700/782]\tLoss: 0.6226 (Average: 0.7014)\tPrecision: 75.000 (Average: 75.435)\n",
      "Epoch: No: [54] Batches: [750/782]\tLoss: 0.8020 (Average: 0.7013)\tPrecision: 68.750 (Average: 75.404)\n",
      "Test Accuracy\t  Top Precision: 66.880 (Error: 33.120 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [55] Batches: [0/782]\tLoss: 0.7585 (Average: 0.7585)\tPrecision: 70.312 (Average: 70.312)\n",
      "Epoch: No: [55] Batches: [50/782]\tLoss: 0.7303 (Average: 0.7147)\tPrecision: 70.312 (Average: 75.398)\n",
      "Epoch: No: [55] Batches: [100/782]\tLoss: 0.5883 (Average: 0.6806)\tPrecision: 76.562 (Average: 76.083)\n",
      "Epoch: No: [55] Batches: [150/782]\tLoss: 0.8603 (Average: 0.6685)\tPrecision: 68.750 (Average: 76.407)\n",
      "Epoch: No: [55] Batches: [200/782]\tLoss: 0.4629 (Average: 0.6730)\tPrecision: 87.500 (Average: 76.415)\n",
      "Epoch: No: [55] Batches: [250/782]\tLoss: 0.5996 (Average: 0.6777)\tPrecision: 81.250 (Average: 76.164)\n",
      "Epoch: No: [55] Batches: [300/782]\tLoss: 0.7660 (Average: 0.6850)\tPrecision: 75.000 (Average: 75.960)\n",
      "Epoch: No: [55] Batches: [350/782]\tLoss: 0.6294 (Average: 0.6834)\tPrecision: 73.438 (Average: 76.068)\n",
      "Epoch: No: [55] Batches: [400/782]\tLoss: 0.6660 (Average: 0.6826)\tPrecision: 76.562 (Average: 75.974)\n",
      "Epoch: No: [55] Batches: [450/782]\tLoss: 0.5170 (Average: 0.6852)\tPrecision: 81.250 (Average: 75.946)\n",
      "Epoch: No: [55] Batches: [500/782]\tLoss: 0.9379 (Average: 0.6854)\tPrecision: 70.312 (Average: 75.901)\n",
      "Epoch: No: [55] Batches: [550/782]\tLoss: 0.8876 (Average: 0.6866)\tPrecision: 68.750 (Average: 75.879)\n",
      "Epoch: No: [55] Batches: [600/782]\tLoss: 0.5962 (Average: 0.6878)\tPrecision: 84.375 (Average: 75.793)\n",
      "Epoch: No: [55] Batches: [650/782]\tLoss: 0.8471 (Average: 0.6892)\tPrecision: 70.312 (Average: 75.816)\n",
      "Epoch: No: [55] Batches: [700/782]\tLoss: 0.7187 (Average: 0.6905)\tPrecision: 73.438 (Average: 75.778)\n",
      "Epoch: No: [55] Batches: [750/782]\tLoss: 0.7781 (Average: 0.6930)\tPrecision: 71.875 (Average: 75.693)\n",
      "Test Accuracy\t  Top Precision: 71.260 (Error: 28.740 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [56] Batches: [0/782]\tLoss: 0.5850 (Average: 0.5850)\tPrecision: 78.125 (Average: 78.125)\n",
      "Epoch: No: [56] Batches: [50/782]\tLoss: 0.6583 (Average: 0.7028)\tPrecision: 73.438 (Average: 75.153)\n",
      "Epoch: No: [56] Batches: [100/782]\tLoss: 0.5694 (Average: 0.6947)\tPrecision: 81.250 (Average: 75.743)\n",
      "Epoch: No: [56] Batches: [150/782]\tLoss: 0.8052 (Average: 0.6996)\tPrecision: 71.875 (Average: 75.859)\n",
      "Epoch: No: [56] Batches: [200/782]\tLoss: 0.9296 (Average: 0.6984)\tPrecision: 67.188 (Average: 75.653)\n",
      "Epoch: No: [56] Batches: [250/782]\tLoss: 0.7522 (Average: 0.6939)\tPrecision: 78.125 (Average: 75.784)\n",
      "Epoch: No: [56] Batches: [300/782]\tLoss: 0.9682 (Average: 0.6938)\tPrecision: 71.875 (Average: 75.649)\n",
      "Epoch: No: [56] Batches: [350/782]\tLoss: 0.7783 (Average: 0.6888)\tPrecision: 73.438 (Average: 75.783)\n",
      "Epoch: No: [56] Batches: [400/782]\tLoss: 0.6802 (Average: 0.6907)\tPrecision: 71.875 (Average: 75.768)\n",
      "Epoch: No: [56] Batches: [450/782]\tLoss: 0.5493 (Average: 0.6908)\tPrecision: 85.938 (Average: 75.845)\n",
      "Epoch: No: [56] Batches: [500/782]\tLoss: 0.8188 (Average: 0.6899)\tPrecision: 65.625 (Average: 75.870)\n",
      "Epoch: No: [56] Batches: [550/782]\tLoss: 0.5838 (Average: 0.6863)\tPrecision: 82.812 (Average: 76.066)\n",
      "Epoch: No: [56] Batches: [600/782]\tLoss: 0.6531 (Average: 0.6876)\tPrecision: 79.688 (Average: 76.089)\n",
      "Epoch: No: [56] Batches: [650/782]\tLoss: 0.4962 (Average: 0.6869)\tPrecision: 84.375 (Average: 76.159)\n",
      "Epoch: No: [56] Batches: [700/782]\tLoss: 0.6816 (Average: 0.6870)\tPrecision: 75.000 (Average: 76.119)\n",
      "Epoch: No: [56] Batches: [750/782]\tLoss: 0.5718 (Average: 0.6895)\tPrecision: 82.812 (Average: 76.094)\n",
      "Test Accuracy\t  Top Precision: 58.630 (Error: 41.370 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [57] Batches: [0/782]\tLoss: 0.8795 (Average: 0.8795)\tPrecision: 70.312 (Average: 70.312)\n",
      "Epoch: No: [57] Batches: [50/782]\tLoss: 0.8008 (Average: 0.6922)\tPrecision: 68.750 (Average: 76.409)\n",
      "Epoch: No: [57] Batches: [100/782]\tLoss: 0.7933 (Average: 0.6757)\tPrecision: 68.750 (Average: 76.439)\n",
      "Epoch: No: [57] Batches: [150/782]\tLoss: 0.7949 (Average: 0.6709)\tPrecision: 71.875 (Average: 76.562)\n",
      "Epoch: No: [57] Batches: [200/782]\tLoss: 0.3843 (Average: 0.6751)\tPrecision: 85.938 (Average: 76.399)\n",
      "Epoch: No: [57] Batches: [250/782]\tLoss: 0.8368 (Average: 0.6769)\tPrecision: 71.875 (Average: 76.513)\n",
      "Epoch: No: [57] Batches: [300/782]\tLoss: 0.5401 (Average: 0.6715)\tPrecision: 82.812 (Average: 76.588)\n",
      "Epoch: No: [57] Batches: [350/782]\tLoss: 0.6335 (Average: 0.6746)\tPrecision: 79.688 (Average: 76.518)\n",
      "Epoch: No: [57] Batches: [400/782]\tLoss: 0.7438 (Average: 0.6762)\tPrecision: 67.188 (Average: 76.325)\n",
      "Epoch: No: [57] Batches: [450/782]\tLoss: 0.7791 (Average: 0.6810)\tPrecision: 76.562 (Average: 76.233)\n",
      "Epoch: No: [57] Batches: [500/782]\tLoss: 0.4254 (Average: 0.6814)\tPrecision: 81.250 (Average: 76.238)\n",
      "Epoch: No: [57] Batches: [550/782]\tLoss: 1.0060 (Average: 0.6810)\tPrecision: 68.750 (Average: 76.296)\n",
      "Epoch: No: [57] Batches: [600/782]\tLoss: 0.5281 (Average: 0.6807)\tPrecision: 78.125 (Average: 76.305)\n",
      "Epoch: No: [57] Batches: [650/782]\tLoss: 0.5824 (Average: 0.6791)\tPrecision: 76.562 (Average: 76.363)\n",
      "Epoch: No: [57] Batches: [700/782]\tLoss: 0.6774 (Average: 0.6800)\tPrecision: 76.562 (Average: 76.362)\n",
      "Epoch: No: [57] Batches: [750/782]\tLoss: 0.6605 (Average: 0.6825)\tPrecision: 71.875 (Average: 76.275)\n",
      "Test Accuracy\t  Top Precision: 73.750 (Error: 26.250 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [58] Batches: [0/782]\tLoss: 0.5111 (Average: 0.5111)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [58] Batches: [50/782]\tLoss: 0.4906 (Average: 0.6676)\tPrecision: 84.375 (Average: 76.838)\n",
      "Epoch: No: [58] Batches: [100/782]\tLoss: 0.6957 (Average: 0.6652)\tPrecision: 78.125 (Average: 77.027)\n",
      "Epoch: No: [58] Batches: [150/782]\tLoss: 0.5390 (Average: 0.6730)\tPrecision: 81.250 (Average: 76.562)\n",
      "Epoch: No: [58] Batches: [200/782]\tLoss: 0.7977 (Average: 0.6781)\tPrecision: 73.438 (Average: 76.275)\n",
      "Epoch: No: [58] Batches: [250/782]\tLoss: 0.5876 (Average: 0.6792)\tPrecision: 81.250 (Average: 76.127)\n",
      "Epoch: No: [58] Batches: [300/782]\tLoss: 0.6902 (Average: 0.6836)\tPrecision: 76.562 (Average: 76.049)\n",
      "Epoch: No: [58] Batches: [350/782]\tLoss: 0.4834 (Average: 0.6866)\tPrecision: 82.812 (Average: 75.993)\n",
      "Epoch: No: [58] Batches: [400/782]\tLoss: 0.6613 (Average: 0.6815)\tPrecision: 76.562 (Average: 76.173)\n",
      "Epoch: No: [58] Batches: [450/782]\tLoss: 0.5496 (Average: 0.6800)\tPrecision: 79.688 (Average: 76.213)\n",
      "Epoch: No: [58] Batches: [500/782]\tLoss: 0.7837 (Average: 0.6814)\tPrecision: 73.438 (Average: 76.191)\n",
      "Epoch: No: [58] Batches: [550/782]\tLoss: 0.6107 (Average: 0.6781)\tPrecision: 76.562 (Average: 76.268)\n",
      "Epoch: No: [58] Batches: [600/782]\tLoss: 0.7128 (Average: 0.6791)\tPrecision: 75.000 (Average: 76.266)\n",
      "Epoch: No: [58] Batches: [650/782]\tLoss: 0.6601 (Average: 0.6769)\tPrecision: 75.000 (Average: 76.298)\n",
      "Epoch: No: [58] Batches: [700/782]\tLoss: 0.5783 (Average: 0.6767)\tPrecision: 84.375 (Average: 76.295)\n",
      "Epoch: No: [58] Batches: [750/782]\tLoss: 0.8114 (Average: 0.6769)\tPrecision: 73.438 (Average: 76.294)\n",
      "Test Accuracy\t  Top Precision: 73.870 (Error: 26.130 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [59] Batches: [0/782]\tLoss: 0.4780 (Average: 0.4780)\tPrecision: 85.938 (Average: 85.938)\n",
      "Epoch: No: [59] Batches: [50/782]\tLoss: 0.5862 (Average: 0.6589)\tPrecision: 79.688 (Average: 77.574)\n",
      "Epoch: No: [59] Batches: [100/782]\tLoss: 0.6143 (Average: 0.6660)\tPrecision: 79.688 (Average: 77.243)\n",
      "Epoch: No: [59] Batches: [150/782]\tLoss: 0.9081 (Average: 0.6676)\tPrecision: 67.188 (Average: 76.914)\n",
      "Epoch: No: [59] Batches: [200/782]\tLoss: 0.8093 (Average: 0.6641)\tPrecision: 65.625 (Average: 76.943)\n",
      "Epoch: No: [59] Batches: [250/782]\tLoss: 0.7606 (Average: 0.6697)\tPrecision: 71.875 (Average: 76.836)\n",
      "Epoch: No: [59] Batches: [300/782]\tLoss: 0.6831 (Average: 0.6678)\tPrecision: 73.438 (Average: 76.832)\n",
      "Epoch: No: [59] Batches: [350/782]\tLoss: 0.6287 (Average: 0.6669)\tPrecision: 79.688 (Average: 76.861)\n",
      "Epoch: No: [59] Batches: [400/782]\tLoss: 0.7378 (Average: 0.6676)\tPrecision: 78.125 (Average: 76.882)\n",
      "Epoch: No: [59] Batches: [450/782]\tLoss: 0.6701 (Average: 0.6698)\tPrecision: 67.188 (Average: 76.715)\n",
      "Epoch: No: [59] Batches: [500/782]\tLoss: 0.7070 (Average: 0.6683)\tPrecision: 78.125 (Average: 76.775)\n",
      "Epoch: No: [59] Batches: [550/782]\tLoss: 0.6746 (Average: 0.6690)\tPrecision: 76.562 (Average: 76.755)\n",
      "Epoch: No: [59] Batches: [600/782]\tLoss: 0.6408 (Average: 0.6697)\tPrecision: 81.250 (Average: 76.744)\n",
      "Epoch: No: [59] Batches: [650/782]\tLoss: 0.6747 (Average: 0.6703)\tPrecision: 70.312 (Average: 76.647)\n",
      "Epoch: No: [59] Batches: [700/782]\tLoss: 0.7238 (Average: 0.6705)\tPrecision: 76.562 (Average: 76.620)\n",
      "Epoch: No: [59] Batches: [750/782]\tLoss: 0.8943 (Average: 0.6720)\tPrecision: 67.188 (Average: 76.527)\n",
      "Test Accuracy\t  Top Precision: 72.610 (Error: 27.390 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [60] Batches: [0/782]\tLoss: 0.6067 (Average: 0.6067)\tPrecision: 81.250 (Average: 81.250)\n",
      "Epoch: No: [60] Batches: [50/782]\tLoss: 0.7305 (Average: 0.6889)\tPrecision: 71.875 (Average: 75.919)\n",
      "Epoch: No: [60] Batches: [100/782]\tLoss: 0.7887 (Average: 0.6630)\tPrecision: 75.000 (Average: 76.779)\n",
      "Epoch: No: [60] Batches: [150/782]\tLoss: 0.6752 (Average: 0.6551)\tPrecision: 70.312 (Average: 77.256)\n",
      "Epoch: No: [60] Batches: [200/782]\tLoss: 0.7496 (Average: 0.6597)\tPrecision: 71.875 (Average: 77.037)\n",
      "Epoch: No: [60] Batches: [250/782]\tLoss: 0.6495 (Average: 0.6619)\tPrecision: 84.375 (Average: 76.924)\n",
      "Epoch: No: [60] Batches: [300/782]\tLoss: 0.6966 (Average: 0.6666)\tPrecision: 73.438 (Average: 76.791)\n",
      "Epoch: No: [60] Batches: [350/782]\tLoss: 0.5657 (Average: 0.6619)\tPrecision: 76.562 (Average: 76.847)\n",
      "Epoch: No: [60] Batches: [400/782]\tLoss: 0.4964 (Average: 0.6618)\tPrecision: 81.250 (Average: 77.022)\n",
      "Epoch: No: [60] Batches: [450/782]\tLoss: 0.4805 (Average: 0.6622)\tPrecision: 78.125 (Average: 76.892)\n",
      "Epoch: No: [60] Batches: [500/782]\tLoss: 0.6314 (Average: 0.6644)\tPrecision: 78.125 (Average: 76.831)\n",
      "Epoch: No: [60] Batches: [550/782]\tLoss: 0.6834 (Average: 0.6657)\tPrecision: 73.438 (Average: 76.772)\n",
      "Epoch: No: [60] Batches: [600/782]\tLoss: 0.6315 (Average: 0.6683)\tPrecision: 75.000 (Average: 76.622)\n",
      "Epoch: No: [60] Batches: [650/782]\tLoss: 0.7005 (Average: 0.6668)\tPrecision: 79.688 (Average: 76.716)\n",
      "Epoch: No: [60] Batches: [700/782]\tLoss: 0.8626 (Average: 0.6665)\tPrecision: 70.312 (Average: 76.692)\n",
      "Epoch: No: [60] Batches: [750/782]\tLoss: 0.4919 (Average: 0.6675)\tPrecision: 84.375 (Average: 76.662)\n",
      "Test Accuracy\t  Top Precision: 73.540 (Error: 26.460 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [61] Batches: [0/782]\tLoss: 0.8473 (Average: 0.8473)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [61] Batches: [50/782]\tLoss: 0.5798 (Average: 0.6919)\tPrecision: 79.688 (Average: 75.184)\n",
      "Epoch: No: [61] Batches: [100/782]\tLoss: 0.5074 (Average: 0.6752)\tPrecision: 79.688 (Average: 75.944)\n",
      "Epoch: No: [61] Batches: [150/782]\tLoss: 0.5177 (Average: 0.6685)\tPrecision: 82.812 (Average: 76.335)\n",
      "Epoch: No: [61] Batches: [200/782]\tLoss: 0.5957 (Average: 0.6649)\tPrecision: 78.125 (Average: 76.555)\n",
      "Epoch: No: [61] Batches: [250/782]\tLoss: 0.6472 (Average: 0.6665)\tPrecision: 75.000 (Average: 76.544)\n",
      "Epoch: No: [61] Batches: [300/782]\tLoss: 0.6581 (Average: 0.6683)\tPrecision: 75.000 (Average: 76.547)\n",
      "Epoch: No: [61] Batches: [350/782]\tLoss: 0.6595 (Average: 0.6662)\tPrecision: 81.250 (Average: 76.656)\n",
      "Epoch: No: [61] Batches: [400/782]\tLoss: 0.6618 (Average: 0.6671)\tPrecision: 76.562 (Average: 76.656)\n",
      "Epoch: No: [61] Batches: [450/782]\tLoss: 0.8023 (Average: 0.6658)\tPrecision: 71.875 (Average: 76.691)\n",
      "Epoch: No: [61] Batches: [500/782]\tLoss: 0.5861 (Average: 0.6637)\tPrecision: 79.688 (Average: 76.709)\n",
      "Epoch: No: [61] Batches: [550/782]\tLoss: 0.6546 (Average: 0.6636)\tPrecision: 65.625 (Average: 76.679)\n",
      "Epoch: No: [61] Batches: [600/782]\tLoss: 0.5080 (Average: 0.6611)\tPrecision: 84.375 (Average: 76.739)\n",
      "Epoch: No: [61] Batches: [650/782]\tLoss: 0.7037 (Average: 0.6575)\tPrecision: 76.562 (Average: 76.870)\n",
      "Epoch: No: [61] Batches: [700/782]\tLoss: 0.5190 (Average: 0.6590)\tPrecision: 76.562 (Average: 76.801)\n",
      "Epoch: No: [61] Batches: [750/782]\tLoss: 0.5433 (Average: 0.6611)\tPrecision: 75.000 (Average: 76.746)\n",
      "Test Accuracy\t  Top Precision: 66.920 (Error: 33.080 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [62] Batches: [0/782]\tLoss: 0.4881 (Average: 0.4881)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [62] Batches: [50/782]\tLoss: 0.6414 (Average: 0.6610)\tPrecision: 73.438 (Average: 76.746)\n",
      "Epoch: No: [62] Batches: [100/782]\tLoss: 0.4420 (Average: 0.6696)\tPrecision: 85.938 (Average: 76.562)\n",
      "Epoch: No: [62] Batches: [150/782]\tLoss: 0.9868 (Average: 0.6682)\tPrecision: 68.750 (Average: 76.614)\n",
      "Epoch: No: [62] Batches: [200/782]\tLoss: 0.4561 (Average: 0.6737)\tPrecision: 79.688 (Average: 76.430)\n",
      "Epoch: No: [62] Batches: [250/782]\tLoss: 0.6374 (Average: 0.6672)\tPrecision: 70.312 (Average: 76.699)\n",
      "Epoch: No: [62] Batches: [300/782]\tLoss: 0.6477 (Average: 0.6664)\tPrecision: 76.562 (Average: 76.786)\n",
      "Epoch: No: [62] Batches: [350/782]\tLoss: 0.6117 (Average: 0.6665)\tPrecision: 79.688 (Average: 76.856)\n",
      "Epoch: No: [62] Batches: [400/782]\tLoss: 0.7556 (Average: 0.6672)\tPrecision: 71.875 (Average: 76.765)\n",
      "Epoch: No: [62] Batches: [450/782]\tLoss: 0.5443 (Average: 0.6660)\tPrecision: 76.562 (Average: 76.836)\n",
      "Epoch: No: [62] Batches: [500/782]\tLoss: 0.5453 (Average: 0.6638)\tPrecision: 78.125 (Average: 76.974)\n",
      "Epoch: No: [62] Batches: [550/782]\tLoss: 0.4193 (Average: 0.6643)\tPrecision: 81.250 (Average: 76.960)\n",
      "Epoch: No: [62] Batches: [600/782]\tLoss: 0.8172 (Average: 0.6639)\tPrecision: 75.000 (Average: 76.965)\n",
      "Epoch: No: [62] Batches: [650/782]\tLoss: 0.6672 (Average: 0.6627)\tPrecision: 73.438 (Average: 76.966)\n",
      "Epoch: No: [62] Batches: [700/782]\tLoss: 0.7916 (Average: 0.6629)\tPrecision: 73.438 (Average: 76.964)\n",
      "Epoch: No: [62] Batches: [750/782]\tLoss: 0.5807 (Average: 0.6627)\tPrecision: 76.562 (Average: 76.968)\n",
      "Test Accuracy\t  Top Precision: 73.480 (Error: 26.520 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [63] Batches: [0/782]\tLoss: 0.7476 (Average: 0.7476)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [63] Batches: [50/782]\tLoss: 0.7263 (Average: 0.6480)\tPrecision: 71.875 (Average: 76.900)\n",
      "Epoch: No: [63] Batches: [100/782]\tLoss: 0.5257 (Average: 0.6515)\tPrecision: 81.250 (Average: 77.181)\n",
      "Epoch: No: [63] Batches: [150/782]\tLoss: 0.6235 (Average: 0.6446)\tPrecision: 75.000 (Average: 77.432)\n",
      "Epoch: No: [63] Batches: [200/782]\tLoss: 0.5255 (Average: 0.6514)\tPrecision: 81.250 (Average: 77.379)\n",
      "Epoch: No: [63] Batches: [250/782]\tLoss: 0.4616 (Average: 0.6436)\tPrecision: 84.375 (Average: 77.683)\n",
      "Epoch: No: [63] Batches: [300/782]\tLoss: 0.6292 (Average: 0.6422)\tPrecision: 79.688 (Average: 77.777)\n",
      "Epoch: No: [63] Batches: [350/782]\tLoss: 0.8574 (Average: 0.6430)\tPrecision: 73.438 (Average: 77.666)\n",
      "Epoch: No: [63] Batches: [400/782]\tLoss: 0.5634 (Average: 0.6385)\tPrecision: 73.438 (Average: 77.712)\n",
      "Epoch: No: [63] Batches: [450/782]\tLoss: 0.6863 (Average: 0.6403)\tPrecision: 79.688 (Average: 77.529)\n",
      "Epoch: No: [63] Batches: [500/782]\tLoss: 0.8859 (Average: 0.6423)\tPrecision: 67.188 (Average: 77.398)\n",
      "Epoch: No: [63] Batches: [550/782]\tLoss: 0.6103 (Average: 0.6437)\tPrecision: 81.250 (Average: 77.339)\n",
      "Epoch: No: [63] Batches: [600/782]\tLoss: 0.8069 (Average: 0.6444)\tPrecision: 70.312 (Average: 77.389)\n",
      "Epoch: No: [63] Batches: [650/782]\tLoss: 0.4584 (Average: 0.6444)\tPrecision: 84.375 (Average: 77.376)\n",
      "Epoch: No: [63] Batches: [700/782]\tLoss: 0.6576 (Average: 0.6445)\tPrecision: 78.125 (Average: 77.414)\n",
      "Epoch: No: [63] Batches: [750/782]\tLoss: 1.0658 (Average: 0.6451)\tPrecision: 68.750 (Average: 77.455)\n",
      "Test Accuracy\t  Top Precision: 72.990 (Error: 27.010 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [64] Batches: [0/782]\tLoss: 0.5722 (Average: 0.5722)\tPrecision: 75.000 (Average: 75.000)\n",
      "Epoch: No: [64] Batches: [50/782]\tLoss: 0.6002 (Average: 0.6340)\tPrecision: 78.125 (Average: 77.206)\n",
      "Epoch: No: [64] Batches: [100/782]\tLoss: 0.6466 (Average: 0.6496)\tPrecision: 81.250 (Average: 77.104)\n",
      "Epoch: No: [64] Batches: [150/782]\tLoss: 0.5255 (Average: 0.6392)\tPrecision: 79.688 (Average: 77.608)\n",
      "Epoch: No: [64] Batches: [200/782]\tLoss: 0.4085 (Average: 0.6379)\tPrecision: 89.062 (Average: 77.791)\n",
      "Epoch: No: [64] Batches: [250/782]\tLoss: 0.5184 (Average: 0.6374)\tPrecision: 84.375 (Average: 77.969)\n",
      "Epoch: No: [64] Batches: [300/782]\tLoss: 0.6083 (Average: 0.6386)\tPrecision: 79.688 (Average: 77.855)\n",
      "Epoch: No: [64] Batches: [350/782]\tLoss: 0.8927 (Average: 0.6448)\tPrecision: 68.750 (Average: 77.666)\n",
      "Epoch: No: [64] Batches: [400/782]\tLoss: 0.7128 (Average: 0.6448)\tPrecision: 68.750 (Average: 77.607)\n",
      "Epoch: No: [64] Batches: [450/782]\tLoss: 0.6649 (Average: 0.6409)\tPrecision: 73.438 (Average: 77.692)\n",
      "Epoch: No: [64] Batches: [500/782]\tLoss: 0.5262 (Average: 0.6368)\tPrecision: 81.250 (Average: 77.829)\n",
      "Epoch: No: [64] Batches: [550/782]\tLoss: 0.8446 (Average: 0.6379)\tPrecision: 78.125 (Average: 77.799)\n",
      "Epoch: No: [64] Batches: [600/782]\tLoss: 0.6644 (Average: 0.6377)\tPrecision: 76.562 (Average: 77.847)\n",
      "Epoch: No: [64] Batches: [650/782]\tLoss: 0.5964 (Average: 0.6401)\tPrecision: 79.688 (Average: 77.753)\n",
      "Epoch: No: [64] Batches: [700/782]\tLoss: 0.4956 (Average: 0.6404)\tPrecision: 84.375 (Average: 77.719)\n",
      "Epoch: No: [64] Batches: [750/782]\tLoss: 0.5917 (Average: 0.6424)\tPrecision: 78.125 (Average: 77.674)\n",
      "Test Accuracy\t  Top Precision: 75.930 (Error: 24.070 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [65] Batches: [0/782]\tLoss: 0.5781 (Average: 0.5781)\tPrecision: 79.688 (Average: 79.688)\n",
      "Epoch: No: [65] Batches: [50/782]\tLoss: 0.6921 (Average: 0.6295)\tPrecision: 79.688 (Average: 77.083)\n",
      "Epoch: No: [65] Batches: [100/782]\tLoss: 0.5391 (Average: 0.6274)\tPrecision: 84.375 (Average: 77.754)\n",
      "Epoch: No: [65] Batches: [150/782]\tLoss: 0.6098 (Average: 0.6375)\tPrecision: 78.125 (Average: 77.442)\n",
      "Epoch: No: [65] Batches: [200/782]\tLoss: 0.5358 (Average: 0.6354)\tPrecision: 79.688 (Average: 77.620)\n",
      "Epoch: No: [65] Batches: [250/782]\tLoss: 0.7813 (Average: 0.6352)\tPrecision: 73.438 (Average: 77.714)\n",
      "Epoch: No: [65] Batches: [300/782]\tLoss: 0.6800 (Average: 0.6350)\tPrecision: 79.688 (Average: 77.590)\n",
      "Epoch: No: [65] Batches: [350/782]\tLoss: 0.6217 (Average: 0.6366)\tPrecision: 79.688 (Average: 77.564)\n",
      "Epoch: No: [65] Batches: [400/782]\tLoss: 0.5626 (Average: 0.6316)\tPrecision: 84.375 (Average: 77.731)\n",
      "Epoch: No: [65] Batches: [450/782]\tLoss: 0.5610 (Average: 0.6348)\tPrecision: 76.562 (Average: 77.602)\n",
      "Epoch: No: [65] Batches: [500/782]\tLoss: 0.5124 (Average: 0.6340)\tPrecision: 81.250 (Average: 77.695)\n",
      "Epoch: No: [65] Batches: [550/782]\tLoss: 0.7796 (Average: 0.6362)\tPrecision: 73.438 (Average: 77.651)\n",
      "Epoch: No: [65] Batches: [600/782]\tLoss: 0.6642 (Average: 0.6368)\tPrecision: 79.688 (Average: 77.631)\n",
      "Epoch: No: [65] Batches: [650/782]\tLoss: 0.7656 (Average: 0.6366)\tPrecision: 73.438 (Average: 77.595)\n",
      "Epoch: No: [65] Batches: [700/782]\tLoss: 0.7136 (Average: 0.6389)\tPrecision: 75.000 (Average: 77.508)\n",
      "Epoch: No: [65] Batches: [750/782]\tLoss: 0.7199 (Average: 0.6390)\tPrecision: 79.688 (Average: 77.530)\n",
      "Test Accuracy\t  Top Precision: 74.690 (Error: 25.310 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [66] Batches: [0/782]\tLoss: 0.7869 (Average: 0.7869)\tPrecision: 73.438 (Average: 73.438)\n",
      "Epoch: No: [66] Batches: [50/782]\tLoss: 0.4265 (Average: 0.6097)\tPrecision: 85.938 (Average: 79.657)\n",
      "Epoch: No: [66] Batches: [100/782]\tLoss: 0.5748 (Average: 0.6151)\tPrecision: 85.938 (Average: 79.022)\n",
      "Epoch: No: [66] Batches: [150/782]\tLoss: 0.6021 (Average: 0.6258)\tPrecision: 79.688 (Average: 78.539)\n",
      "Epoch: No: [66] Batches: [200/782]\tLoss: 0.5216 (Average: 0.6255)\tPrecision: 81.250 (Average: 78.459)\n",
      "Epoch: No: [66] Batches: [250/782]\tLoss: 0.6309 (Average: 0.6267)\tPrecision: 81.250 (Average: 78.262)\n",
      "Epoch: No: [66] Batches: [300/782]\tLoss: 0.6226 (Average: 0.6253)\tPrecision: 76.562 (Average: 78.239)\n",
      "Epoch: No: [66] Batches: [350/782]\tLoss: 0.6813 (Average: 0.6275)\tPrecision: 81.250 (Average: 78.156)\n",
      "Epoch: No: [66] Batches: [400/782]\tLoss: 0.5235 (Average: 0.6274)\tPrecision: 75.000 (Average: 78.222)\n",
      "Epoch: No: [66] Batches: [450/782]\tLoss: 0.5418 (Average: 0.6279)\tPrecision: 75.000 (Average: 78.177)\n",
      "Epoch: No: [66] Batches: [500/782]\tLoss: 0.6359 (Average: 0.6301)\tPrecision: 81.250 (Average: 78.060)\n",
      "Epoch: No: [66] Batches: [550/782]\tLoss: 0.6854 (Average: 0.6326)\tPrecision: 75.000 (Average: 77.949)\n",
      "Epoch: No: [66] Batches: [600/782]\tLoss: 0.4760 (Average: 0.6357)\tPrecision: 84.375 (Average: 77.894)\n",
      "Epoch: No: [66] Batches: [650/782]\tLoss: 0.6097 (Average: 0.6355)\tPrecision: 79.688 (Average: 77.885)\n",
      "Epoch: No: [66] Batches: [700/782]\tLoss: 0.5777 (Average: 0.6362)\tPrecision: 78.125 (Average: 77.808)\n",
      "Epoch: No: [66] Batches: [750/782]\tLoss: 0.9756 (Average: 0.6374)\tPrecision: 60.938 (Average: 77.694)\n",
      "Test Accuracy\t  Top Precision: 72.410 (Error: 27.590 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [67] Batches: [0/782]\tLoss: 0.6089 (Average: 0.6089)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [67] Batches: [50/782]\tLoss: 0.6273 (Average: 0.6249)\tPrecision: 73.438 (Average: 77.727)\n",
      "Epoch: No: [67] Batches: [100/782]\tLoss: 0.6698 (Average: 0.6262)\tPrecision: 87.500 (Average: 78.079)\n",
      "Epoch: No: [67] Batches: [150/782]\tLoss: 0.6126 (Average: 0.6386)\tPrecision: 79.688 (Average: 77.825)\n",
      "Epoch: No: [67] Batches: [200/782]\tLoss: 0.6526 (Average: 0.6368)\tPrecision: 81.250 (Average: 77.923)\n",
      "Epoch: No: [67] Batches: [250/782]\tLoss: 0.5038 (Average: 0.6325)\tPrecision: 79.688 (Average: 78.088)\n",
      "Epoch: No: [67] Batches: [300/782]\tLoss: 0.5782 (Average: 0.6324)\tPrecision: 76.562 (Average: 78.104)\n",
      "Epoch: No: [67] Batches: [350/782]\tLoss: 0.6089 (Average: 0.6291)\tPrecision: 82.812 (Average: 78.201)\n",
      "Epoch: No: [67] Batches: [400/782]\tLoss: 0.4383 (Average: 0.6294)\tPrecision: 85.938 (Average: 78.160)\n",
      "Epoch: No: [67] Batches: [450/782]\tLoss: 0.6073 (Average: 0.6331)\tPrecision: 75.000 (Average: 78.031)\n",
      "Epoch: No: [67] Batches: [500/782]\tLoss: 0.7667 (Average: 0.6320)\tPrecision: 71.875 (Average: 78.050)\n",
      "Epoch: No: [67] Batches: [550/782]\tLoss: 0.7863 (Average: 0.6340)\tPrecision: 68.750 (Average: 77.929)\n",
      "Epoch: No: [67] Batches: [600/782]\tLoss: 0.4897 (Average: 0.6356)\tPrecision: 82.812 (Average: 77.901)\n",
      "Epoch: No: [67] Batches: [650/782]\tLoss: 0.5251 (Average: 0.6352)\tPrecision: 84.375 (Average: 77.933)\n",
      "Epoch: No: [67] Batches: [700/782]\tLoss: 0.6289 (Average: 0.6327)\tPrecision: 75.000 (Average: 78.005)\n",
      "Epoch: No: [67] Batches: [750/782]\tLoss: 0.6759 (Average: 0.6346)\tPrecision: 76.562 (Average: 77.946)\n",
      "Test Accuracy\t  Top Precision: 77.700 (Error: 22.300 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [68] Batches: [0/782]\tLoss: 0.6446 (Average: 0.6446)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [68] Batches: [50/782]\tLoss: 0.4803 (Average: 0.6458)\tPrecision: 87.500 (Average: 78.033)\n",
      "Epoch: No: [68] Batches: [100/782]\tLoss: 0.4275 (Average: 0.6233)\tPrecision: 84.375 (Average: 78.527)\n",
      "Epoch: No: [68] Batches: [150/782]\tLoss: 0.4892 (Average: 0.6407)\tPrecision: 78.125 (Average: 77.597)\n",
      "Epoch: No: [68] Batches: [200/782]\tLoss: 0.6907 (Average: 0.6400)\tPrecision: 76.562 (Average: 77.713)\n",
      "Epoch: No: [68] Batches: [250/782]\tLoss: 0.6658 (Average: 0.6362)\tPrecision: 73.438 (Average: 77.888)\n",
      "Epoch: No: [68] Batches: [300/782]\tLoss: 0.4378 (Average: 0.6348)\tPrecision: 82.812 (Average: 77.974)\n",
      "Epoch: No: [68] Batches: [350/782]\tLoss: 0.7234 (Average: 0.6369)\tPrecision: 73.438 (Average: 77.885)\n",
      "Epoch: No: [68] Batches: [400/782]\tLoss: 0.8059 (Average: 0.6326)\tPrecision: 70.312 (Average: 78.063)\n",
      "Epoch: No: [68] Batches: [450/782]\tLoss: 0.6958 (Average: 0.6323)\tPrecision: 71.875 (Average: 78.052)\n",
      "Epoch: No: [68] Batches: [500/782]\tLoss: 0.4891 (Average: 0.6318)\tPrecision: 82.812 (Average: 78.047)\n",
      "Epoch: No: [68] Batches: [550/782]\tLoss: 0.6516 (Average: 0.6317)\tPrecision: 81.250 (Average: 78.040)\n",
      "Epoch: No: [68] Batches: [600/782]\tLoss: 0.5957 (Average: 0.6338)\tPrecision: 76.562 (Average: 77.935)\n",
      "Epoch: No: [68] Batches: [650/782]\tLoss: 0.4055 (Average: 0.6327)\tPrecision: 81.250 (Average: 77.952)\n",
      "Epoch: No: [68] Batches: [700/782]\tLoss: 0.5078 (Average: 0.6326)\tPrecision: 85.938 (Average: 77.965)\n",
      "Epoch: No: [68] Batches: [750/782]\tLoss: 0.7643 (Average: 0.6289)\tPrecision: 75.000 (Average: 78.085)\n",
      "Test Accuracy\t  Top Precision: 75.940 (Error: 24.060 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [69] Batches: [0/782]\tLoss: 0.5000 (Average: 0.5000)\tPrecision: 85.938 (Average: 85.938)\n",
      "Epoch: No: [69] Batches: [50/782]\tLoss: 0.5433 (Average: 0.6289)\tPrecision: 79.688 (Average: 78.431)\n",
      "Epoch: No: [69] Batches: [100/782]\tLoss: 0.7954 (Average: 0.6160)\tPrecision: 75.000 (Average: 78.728)\n",
      "Epoch: No: [69] Batches: [150/782]\tLoss: 0.6785 (Average: 0.6278)\tPrecision: 78.125 (Average: 77.990)\n",
      "Epoch: No: [69] Batches: [200/782]\tLoss: 0.5676 (Average: 0.6259)\tPrecision: 79.688 (Average: 78.234)\n",
      "Epoch: No: [69] Batches: [250/782]\tLoss: 0.5899 (Average: 0.6310)\tPrecision: 82.812 (Average: 77.951)\n",
      "Epoch: No: [69] Batches: [300/782]\tLoss: 0.5011 (Average: 0.6300)\tPrecision: 82.812 (Average: 78.141)\n",
      "Epoch: No: [69] Batches: [350/782]\tLoss: 0.7011 (Average: 0.6289)\tPrecision: 76.562 (Average: 78.098)\n",
      "Epoch: No: [69] Batches: [400/782]\tLoss: 0.6981 (Average: 0.6275)\tPrecision: 76.562 (Average: 78.261)\n",
      "Epoch: No: [69] Batches: [450/782]\tLoss: 0.5612 (Average: 0.6245)\tPrecision: 79.688 (Average: 78.257)\n",
      "Epoch: No: [69] Batches: [500/782]\tLoss: 0.7562 (Average: 0.6260)\tPrecision: 73.438 (Average: 78.209)\n",
      "Epoch: No: [69] Batches: [550/782]\tLoss: 0.7697 (Average: 0.6264)\tPrecision: 76.562 (Average: 78.156)\n",
      "Epoch: No: [69] Batches: [600/782]\tLoss: 0.6874 (Average: 0.6259)\tPrecision: 78.125 (Average: 78.154)\n",
      "Epoch: No: [69] Batches: [650/782]\tLoss: 0.6463 (Average: 0.6241)\tPrecision: 76.562 (Average: 78.255)\n",
      "Epoch: No: [69] Batches: [700/782]\tLoss: 0.8836 (Average: 0.6234)\tPrecision: 68.750 (Average: 78.308)\n",
      "Epoch: No: [69] Batches: [750/782]\tLoss: 0.4758 (Average: 0.6237)\tPrecision: 81.250 (Average: 78.323)\n",
      "Test Accuracy\t  Top Precision: 74.910 (Error: 25.090 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [70] Batches: [0/782]\tLoss: 0.5318 (Average: 0.5318)\tPrecision: 81.250 (Average: 81.250)\n",
      "Epoch: No: [70] Batches: [50/782]\tLoss: 0.7146 (Average: 0.6442)\tPrecision: 76.562 (Average: 77.788)\n",
      "Epoch: No: [70] Batches: [100/782]\tLoss: 0.5430 (Average: 0.6324)\tPrecision: 81.250 (Average: 77.645)\n",
      "Epoch: No: [70] Batches: [150/782]\tLoss: 0.6121 (Average: 0.6217)\tPrecision: 71.875 (Average: 77.949)\n",
      "Epoch: No: [70] Batches: [200/782]\tLoss: 0.5336 (Average: 0.6287)\tPrecision: 79.688 (Average: 77.853)\n",
      "Epoch: No: [70] Batches: [250/782]\tLoss: 0.7524 (Average: 0.6337)\tPrecision: 68.750 (Average: 77.739)\n",
      "Epoch: No: [70] Batches: [300/782]\tLoss: 0.5728 (Average: 0.6313)\tPrecision: 79.688 (Average: 77.808)\n",
      "Epoch: No: [70] Batches: [350/782]\tLoss: 0.5067 (Average: 0.6277)\tPrecision: 89.062 (Average: 77.991)\n",
      "Epoch: No: [70] Batches: [400/782]\tLoss: 0.3928 (Average: 0.6232)\tPrecision: 85.938 (Average: 78.129)\n",
      "Epoch: No: [70] Batches: [450/782]\tLoss: 0.4924 (Average: 0.6243)\tPrecision: 81.250 (Average: 78.097)\n",
      "Epoch: No: [70] Batches: [500/782]\tLoss: 0.5751 (Average: 0.6210)\tPrecision: 81.250 (Average: 78.209)\n",
      "Epoch: No: [70] Batches: [550/782]\tLoss: 0.6277 (Average: 0.6203)\tPrecision: 78.125 (Average: 78.233)\n",
      "Epoch: No: [70] Batches: [600/782]\tLoss: 0.6359 (Average: 0.6202)\tPrecision: 73.438 (Average: 78.237)\n",
      "Epoch: No: [70] Batches: [650/782]\tLoss: 0.7476 (Average: 0.6216)\tPrecision: 73.438 (Average: 78.178)\n",
      "Epoch: No: [70] Batches: [700/782]\tLoss: 0.5761 (Average: 0.6219)\tPrecision: 73.438 (Average: 78.154)\n",
      "Epoch: No: [70] Batches: [750/782]\tLoss: 0.6385 (Average: 0.6203)\tPrecision: 78.125 (Average: 78.256)\n",
      "Test Accuracy\t  Top Precision: 72.170 (Error: 27.830 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [71] Batches: [0/782]\tLoss: 0.5853 (Average: 0.5853)\tPrecision: 78.125 (Average: 78.125)\n",
      "Epoch: No: [71] Batches: [50/782]\tLoss: 0.6966 (Average: 0.6384)\tPrecision: 71.875 (Average: 77.635)\n",
      "Epoch: No: [71] Batches: [100/782]\tLoss: 0.7521 (Average: 0.6177)\tPrecision: 79.688 (Average: 78.388)\n",
      "Epoch: No: [71] Batches: [150/782]\tLoss: 0.3980 (Average: 0.6176)\tPrecision: 84.375 (Average: 78.487)\n",
      "Epoch: No: [71] Batches: [200/782]\tLoss: 0.5883 (Average: 0.6174)\tPrecision: 76.562 (Average: 78.490)\n",
      "Epoch: No: [71] Batches: [250/782]\tLoss: 0.7244 (Average: 0.6187)\tPrecision: 76.562 (Average: 78.548)\n",
      "Epoch: No: [71] Batches: [300/782]\tLoss: 0.6446 (Average: 0.6212)\tPrecision: 82.812 (Average: 78.442)\n",
      "Epoch: No: [71] Batches: [350/782]\tLoss: 0.8337 (Average: 0.6198)\tPrecision: 70.312 (Average: 78.472)\n",
      "Epoch: No: [71] Batches: [400/782]\tLoss: 0.5211 (Average: 0.6209)\tPrecision: 79.688 (Average: 78.421)\n",
      "Epoch: No: [71] Batches: [450/782]\tLoss: 0.6706 (Average: 0.6194)\tPrecision: 78.125 (Average: 78.437)\n",
      "Epoch: No: [71] Batches: [500/782]\tLoss: 0.4605 (Average: 0.6219)\tPrecision: 82.812 (Average: 78.359)\n",
      "Epoch: No: [71] Batches: [550/782]\tLoss: 0.5365 (Average: 0.6220)\tPrecision: 79.688 (Average: 78.360)\n",
      "Epoch: No: [71] Batches: [600/782]\tLoss: 0.7731 (Average: 0.6193)\tPrecision: 71.875 (Average: 78.395)\n",
      "Epoch: No: [71] Batches: [650/782]\tLoss: 0.5615 (Average: 0.6189)\tPrecision: 82.812 (Average: 78.444)\n",
      "Epoch: No: [71] Batches: [700/782]\tLoss: 0.6782 (Average: 0.6187)\tPrecision: 76.562 (Average: 78.410)\n",
      "Epoch: No: [71] Batches: [750/782]\tLoss: 0.5088 (Average: 0.6182)\tPrecision: 81.250 (Average: 78.443)\n",
      "Test Accuracy\t  Top Precision: 77.210 (Error: 22.790 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [72] Batches: [0/782]\tLoss: 0.4218 (Average: 0.4218)\tPrecision: 87.500 (Average: 87.500)\n",
      "Epoch: No: [72] Batches: [50/782]\tLoss: 0.4251 (Average: 0.6060)\tPrecision: 84.375 (Average: 79.289)\n",
      "Epoch: No: [72] Batches: [100/782]\tLoss: 0.3601 (Average: 0.6108)\tPrecision: 90.625 (Average: 78.682)\n",
      "Epoch: No: [72] Batches: [150/782]\tLoss: 0.7591 (Average: 0.6036)\tPrecision: 71.875 (Average: 78.984)\n",
      "Epoch: No: [72] Batches: [200/782]\tLoss: 0.4614 (Average: 0.6105)\tPrecision: 84.375 (Average: 78.692)\n",
      "Epoch: No: [72] Batches: [250/782]\tLoss: 0.6931 (Average: 0.6169)\tPrecision: 71.875 (Average: 78.492)\n",
      "Epoch: No: [72] Batches: [300/782]\tLoss: 0.7458 (Average: 0.6192)\tPrecision: 67.188 (Average: 78.577)\n",
      "Epoch: No: [72] Batches: [350/782]\tLoss: 0.6253 (Average: 0.6264)\tPrecision: 78.125 (Average: 78.276)\n",
      "Epoch: No: [72] Batches: [400/782]\tLoss: 0.5940 (Average: 0.6228)\tPrecision: 75.000 (Average: 78.390)\n",
      "Epoch: No: [72] Batches: [450/782]\tLoss: 0.5529 (Average: 0.6229)\tPrecision: 79.688 (Average: 78.395)\n",
      "Epoch: No: [72] Batches: [500/782]\tLoss: 0.6508 (Average: 0.6245)\tPrecision: 73.438 (Average: 78.309)\n",
      "Epoch: No: [72] Batches: [550/782]\tLoss: 0.6637 (Average: 0.6245)\tPrecision: 75.000 (Average: 78.346)\n",
      "Epoch: No: [72] Batches: [600/782]\tLoss: 0.7603 (Average: 0.6220)\tPrecision: 71.875 (Average: 78.421)\n",
      "Epoch: No: [72] Batches: [650/782]\tLoss: 0.6856 (Average: 0.6213)\tPrecision: 75.000 (Average: 78.408)\n",
      "Epoch: No: [72] Batches: [700/782]\tLoss: 0.7459 (Average: 0.6199)\tPrecision: 75.000 (Average: 78.424)\n",
      "Epoch: No: [72] Batches: [750/782]\tLoss: 0.7297 (Average: 0.6203)\tPrecision: 73.438 (Average: 78.420)\n",
      "Test Accuracy\t  Top Precision: 72.010 (Error: 27.990 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [73] Batches: [0/782]\tLoss: 0.7481 (Average: 0.7481)\tPrecision: 73.438 (Average: 73.438)\n",
      "Epoch: No: [73] Batches: [50/782]\tLoss: 0.6634 (Average: 0.5761)\tPrecision: 81.250 (Average: 80.239)\n",
      "Epoch: No: [73] Batches: [100/782]\tLoss: 0.5659 (Average: 0.5861)\tPrecision: 78.125 (Average: 80.121)\n",
      "Epoch: No: [73] Batches: [150/782]\tLoss: 0.8688 (Average: 0.5946)\tPrecision: 70.312 (Average: 79.801)\n",
      "Epoch: No: [73] Batches: [200/782]\tLoss: 0.5959 (Average: 0.5995)\tPrecision: 81.250 (Average: 79.618)\n",
      "Epoch: No: [73] Batches: [250/782]\tLoss: 0.6354 (Average: 0.6094)\tPrecision: 76.562 (Average: 79.208)\n",
      "Epoch: No: [73] Batches: [300/782]\tLoss: 0.5340 (Average: 0.6097)\tPrecision: 78.125 (Average: 79.101)\n",
      "Epoch: No: [73] Batches: [350/782]\tLoss: 0.7748 (Average: 0.6089)\tPrecision: 73.438 (Average: 79.042)\n",
      "Epoch: No: [73] Batches: [400/782]\tLoss: 0.7656 (Average: 0.6087)\tPrecision: 73.438 (Average: 78.951)\n",
      "Epoch: No: [73] Batches: [450/782]\tLoss: 0.7393 (Average: 0.6068)\tPrecision: 68.750 (Average: 78.998)\n",
      "Epoch: No: [73] Batches: [500/782]\tLoss: 0.8404 (Average: 0.6082)\tPrecision: 71.875 (Average: 79.014)\n",
      "Epoch: No: [73] Batches: [550/782]\tLoss: 0.6247 (Average: 0.6082)\tPrecision: 73.438 (Average: 78.979)\n",
      "Epoch: No: [73] Batches: [600/782]\tLoss: 0.6469 (Average: 0.6102)\tPrecision: 78.125 (Average: 78.947)\n",
      "Epoch: No: [73] Batches: [650/782]\tLoss: 0.7884 (Average: 0.6123)\tPrecision: 70.312 (Average: 78.847)\n",
      "Epoch: No: [73] Batches: [700/782]\tLoss: 0.6666 (Average: 0.6132)\tPrecision: 82.812 (Average: 78.816)\n",
      "Epoch: No: [73] Batches: [750/782]\tLoss: 0.8064 (Average: 0.6106)\tPrecision: 68.750 (Average: 78.874)\n",
      "Test Accuracy\t  Top Precision: 71.910 (Error: 28.090 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [74] Batches: [0/782]\tLoss: 0.7149 (Average: 0.7149)\tPrecision: 78.125 (Average: 78.125)\n",
      "Epoch: No: [74] Batches: [50/782]\tLoss: 0.5481 (Average: 0.5711)\tPrecision: 81.250 (Average: 80.116)\n",
      "Epoch: No: [74] Batches: [100/782]\tLoss: 0.7446 (Average: 0.5618)\tPrecision: 73.438 (Average: 80.538)\n",
      "Epoch: No: [74] Batches: [150/782]\tLoss: 0.4333 (Average: 0.5767)\tPrecision: 85.938 (Average: 80.081)\n",
      "Epoch: No: [74] Batches: [200/782]\tLoss: 0.6347 (Average: 0.5885)\tPrecision: 70.312 (Average: 79.540)\n",
      "Epoch: No: [74] Batches: [250/782]\tLoss: 0.4374 (Average: 0.5950)\tPrecision: 82.812 (Average: 79.308)\n",
      "Epoch: No: [74] Batches: [300/782]\tLoss: 0.7028 (Average: 0.5934)\tPrecision: 71.875 (Average: 79.366)\n",
      "Epoch: No: [74] Batches: [350/782]\tLoss: 0.6446 (Average: 0.5962)\tPrecision: 81.250 (Average: 79.167)\n",
      "Epoch: No: [74] Batches: [400/782]\tLoss: 0.8603 (Average: 0.5964)\tPrecision: 70.312 (Average: 79.278)\n",
      "Epoch: No: [74] Batches: [450/782]\tLoss: 0.6432 (Average: 0.5982)\tPrecision: 79.688 (Average: 79.223)\n",
      "Epoch: No: [74] Batches: [500/782]\tLoss: 0.5817 (Average: 0.6018)\tPrecision: 78.125 (Average: 79.117)\n",
      "Epoch: No: [74] Batches: [550/782]\tLoss: 0.5793 (Average: 0.6043)\tPrecision: 82.812 (Average: 79.007)\n",
      "Epoch: No: [74] Batches: [600/782]\tLoss: 0.3954 (Average: 0.6015)\tPrecision: 87.500 (Average: 79.038)\n",
      "Epoch: No: [74] Batches: [650/782]\tLoss: 0.5072 (Average: 0.6030)\tPrecision: 87.500 (Average: 79.011)\n",
      "Epoch: No: [74] Batches: [700/782]\tLoss: 0.6824 (Average: 0.6041)\tPrecision: 73.438 (Average: 78.979)\n",
      "Epoch: No: [74] Batches: [750/782]\tLoss: 0.6198 (Average: 0.6074)\tPrecision: 78.125 (Average: 78.903)\n",
      "Test Accuracy\t  Top Precision: 70.560 (Error: 29.440 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [75] Batches: [0/782]\tLoss: 0.8073 (Average: 0.8073)\tPrecision: 75.000 (Average: 75.000)\n",
      "Epoch: No: [75] Batches: [50/782]\tLoss: 0.5236 (Average: 0.6325)\tPrecision: 78.125 (Average: 78.033)\n",
      "Epoch: No: [75] Batches: [100/782]\tLoss: 0.8094 (Average: 0.6160)\tPrecision: 68.750 (Average: 78.434)\n",
      "Epoch: No: [75] Batches: [150/782]\tLoss: 0.6126 (Average: 0.5939)\tPrecision: 81.250 (Average: 79.367)\n",
      "Epoch: No: [75] Batches: [200/782]\tLoss: 0.5108 (Average: 0.5941)\tPrecision: 78.125 (Average: 79.237)\n",
      "Epoch: No: [75] Batches: [250/782]\tLoss: 0.6464 (Average: 0.5931)\tPrecision: 79.688 (Average: 79.351)\n",
      "Epoch: No: [75] Batches: [300/782]\tLoss: 0.7150 (Average: 0.5915)\tPrecision: 78.125 (Average: 79.490)\n",
      "Epoch: No: [75] Batches: [350/782]\tLoss: 0.6233 (Average: 0.5865)\tPrecision: 76.562 (Average: 79.630)\n",
      "Epoch: No: [75] Batches: [400/782]\tLoss: 0.4045 (Average: 0.5855)\tPrecision: 85.938 (Average: 79.598)\n",
      "Epoch: No: [75] Batches: [450/782]\tLoss: 0.8878 (Average: 0.5889)\tPrecision: 75.000 (Average: 79.476)\n",
      "Epoch: No: [75] Batches: [500/782]\tLoss: 0.5699 (Average: 0.5866)\tPrecision: 76.562 (Average: 79.572)\n",
      "Epoch: No: [75] Batches: [550/782]\tLoss: 0.8144 (Average: 0.5862)\tPrecision: 70.312 (Average: 79.580)\n",
      "Epoch: No: [75] Batches: [600/782]\tLoss: 0.6283 (Average: 0.5917)\tPrecision: 79.688 (Average: 79.438)\n",
      "Epoch: No: [75] Batches: [650/782]\tLoss: 0.5586 (Average: 0.5928)\tPrecision: 82.812 (Average: 79.344)\n",
      "Epoch: No: [75] Batches: [700/782]\tLoss: 0.4427 (Average: 0.5939)\tPrecision: 84.375 (Average: 79.304)\n",
      "Epoch: No: [75] Batches: [750/782]\tLoss: 0.9126 (Average: 0.5952)\tPrecision: 73.438 (Average: 79.301)\n",
      "Test Accuracy\t  Top Precision: 76.010 (Error: 23.990 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [76] Batches: [0/782]\tLoss: 0.4041 (Average: 0.4041)\tPrecision: 84.375 (Average: 84.375)\n",
      "Epoch: No: [76] Batches: [50/782]\tLoss: 0.5506 (Average: 0.5878)\tPrecision: 81.250 (Average: 79.136)\n",
      "Epoch: No: [76] Batches: [100/782]\tLoss: 0.7836 (Average: 0.5747)\tPrecision: 75.000 (Average: 79.641)\n",
      "Epoch: No: [76] Batches: [150/782]\tLoss: 0.8352 (Average: 0.5844)\tPrecision: 78.125 (Average: 79.512)\n",
      "Epoch: No: [76] Batches: [200/782]\tLoss: 1.0314 (Average: 0.5948)\tPrecision: 62.500 (Average: 79.182)\n",
      "Epoch: No: [76] Batches: [250/782]\tLoss: 0.6507 (Average: 0.5976)\tPrecision: 71.875 (Average: 78.922)\n",
      "Epoch: No: [76] Batches: [300/782]\tLoss: 0.8481 (Average: 0.5941)\tPrecision: 67.188 (Average: 79.153)\n",
      "Epoch: No: [76] Batches: [350/782]\tLoss: 0.5076 (Average: 0.5906)\tPrecision: 84.375 (Average: 79.269)\n",
      "Epoch: No: [76] Batches: [400/782]\tLoss: 0.7422 (Average: 0.5899)\tPrecision: 73.438 (Average: 79.197)\n",
      "Epoch: No: [76] Batches: [450/782]\tLoss: 0.4624 (Average: 0.5947)\tPrecision: 82.812 (Average: 79.071)\n",
      "Epoch: No: [76] Batches: [500/782]\tLoss: 0.6194 (Average: 0.5991)\tPrecision: 76.562 (Average: 78.945)\n",
      "Epoch: No: [76] Batches: [550/782]\tLoss: 0.5640 (Average: 0.6002)\tPrecision: 81.250 (Average: 78.876)\n",
      "Epoch: No: [76] Batches: [600/782]\tLoss: 0.8321 (Average: 0.5994)\tPrecision: 70.312 (Average: 78.928)\n",
      "Epoch: No: [76] Batches: [650/782]\tLoss: 0.5645 (Average: 0.5976)\tPrecision: 78.125 (Average: 78.991)\n",
      "Epoch: No: [76] Batches: [700/782]\tLoss: 0.7387 (Average: 0.5989)\tPrecision: 70.312 (Average: 78.952)\n",
      "Epoch: No: [76] Batches: [750/782]\tLoss: 0.4847 (Average: 0.5978)\tPrecision: 84.375 (Average: 79.020)\n",
      "Test Accuracy\t  Top Precision: 72.920 (Error: 27.080 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [77] Batches: [0/782]\tLoss: 0.6091 (Average: 0.6091)\tPrecision: 78.125 (Average: 78.125)\n",
      "Epoch: No: [77] Batches: [50/782]\tLoss: 0.4695 (Average: 0.5880)\tPrecision: 84.375 (Average: 79.749)\n",
      "Epoch: No: [77] Batches: [100/782]\tLoss: 0.2775 (Average: 0.5864)\tPrecision: 90.625 (Average: 80.198)\n",
      "Epoch: No: [77] Batches: [150/782]\tLoss: 0.6767 (Average: 0.5911)\tPrecision: 73.438 (Average: 79.832)\n",
      "Epoch: No: [77] Batches: [200/782]\tLoss: 0.5431 (Average: 0.5943)\tPrecision: 75.000 (Average: 79.532)\n",
      "Epoch: No: [77] Batches: [250/782]\tLoss: 0.6312 (Average: 0.6012)\tPrecision: 79.688 (Average: 79.314)\n",
      "Epoch: No: [77] Batches: [300/782]\tLoss: 0.5349 (Average: 0.6021)\tPrecision: 81.250 (Average: 79.158)\n",
      "Epoch: No: [77] Batches: [350/782]\tLoss: 0.4732 (Average: 0.6001)\tPrecision: 81.250 (Average: 79.242)\n",
      "Epoch: No: [77] Batches: [400/782]\tLoss: 0.7993 (Average: 0.5970)\tPrecision: 78.125 (Average: 79.333)\n",
      "Epoch: No: [77] Batches: [450/782]\tLoss: 0.5316 (Average: 0.5971)\tPrecision: 81.250 (Average: 79.417)\n",
      "Epoch: No: [77] Batches: [500/782]\tLoss: 0.5468 (Average: 0.5957)\tPrecision: 84.375 (Average: 79.441)\n",
      "Epoch: No: [77] Batches: [550/782]\tLoss: 0.5763 (Average: 0.5938)\tPrecision: 79.688 (Average: 79.435)\n",
      "Epoch: No: [77] Batches: [600/782]\tLoss: 0.6644 (Average: 0.5938)\tPrecision: 75.000 (Average: 79.420)\n",
      "Epoch: No: [77] Batches: [650/782]\tLoss: 0.8332 (Average: 0.5943)\tPrecision: 78.125 (Average: 79.387)\n",
      "Epoch: No: [77] Batches: [700/782]\tLoss: 0.6089 (Average: 0.5946)\tPrecision: 76.562 (Average: 79.309)\n",
      "Epoch: No: [77] Batches: [750/782]\tLoss: 0.5310 (Average: 0.5951)\tPrecision: 81.250 (Average: 79.348)\n",
      "Test Accuracy\t  Top Precision: 76.880 (Error: 23.120 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [78] Batches: [0/782]\tLoss: 0.6839 (Average: 0.6839)\tPrecision: 81.250 (Average: 81.250)\n",
      "Epoch: No: [78] Batches: [50/782]\tLoss: 0.5121 (Average: 0.5594)\tPrecision: 78.125 (Average: 79.963)\n",
      "Epoch: No: [78] Batches: [100/782]\tLoss: 0.4442 (Average: 0.5682)\tPrecision: 87.500 (Average: 79.904)\n",
      "Epoch: No: [78] Batches: [150/782]\tLoss: 0.5800 (Average: 0.5818)\tPrecision: 79.688 (Average: 79.853)\n",
      "Epoch: No: [78] Batches: [200/782]\tLoss: 0.5215 (Average: 0.5790)\tPrecision: 79.688 (Average: 79.936)\n",
      "Epoch: No: [78] Batches: [250/782]\tLoss: 0.5490 (Average: 0.5831)\tPrecision: 81.250 (Average: 79.725)\n",
      "Epoch: No: [78] Batches: [300/782]\tLoss: 0.7435 (Average: 0.5880)\tPrecision: 76.562 (Average: 79.563)\n",
      "Epoch: No: [78] Batches: [350/782]\tLoss: 0.5509 (Average: 0.5803)\tPrecision: 75.000 (Average: 79.710)\n",
      "Epoch: No: [78] Batches: [400/782]\tLoss: 0.5366 (Average: 0.5845)\tPrecision: 79.688 (Average: 79.575)\n",
      "Epoch: No: [78] Batches: [450/782]\tLoss: 0.5619 (Average: 0.5829)\tPrecision: 81.250 (Average: 79.594)\n",
      "Epoch: No: [78] Batches: [500/782]\tLoss: 0.5365 (Average: 0.5843)\tPrecision: 81.250 (Average: 79.532)\n",
      "Epoch: No: [78] Batches: [550/782]\tLoss: 0.6932 (Average: 0.5865)\tPrecision: 76.562 (Average: 79.537)\n",
      "Epoch: No: [78] Batches: [600/782]\tLoss: 0.6506 (Average: 0.5875)\tPrecision: 71.875 (Average: 79.500)\n",
      "Epoch: No: [78] Batches: [650/782]\tLoss: 0.7325 (Average: 0.5892)\tPrecision: 78.125 (Average: 79.467)\n",
      "Epoch: No: [78] Batches: [700/782]\tLoss: 0.5530 (Average: 0.5905)\tPrecision: 81.250 (Average: 79.378)\n",
      "Epoch: No: [78] Batches: [750/782]\tLoss: 0.6135 (Average: 0.5893)\tPrecision: 79.688 (Average: 79.446)\n",
      "Test Accuracy\t  Top Precision: 76.400 (Error: 23.600 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [79] Batches: [0/782]\tLoss: 0.5067 (Average: 0.5067)\tPrecision: 84.375 (Average: 84.375)\n",
      "Epoch: No: [79] Batches: [50/782]\tLoss: 0.4770 (Average: 0.5532)\tPrecision: 82.812 (Average: 80.699)\n",
      "Epoch: No: [79] Batches: [100/782]\tLoss: 0.3949 (Average: 0.5716)\tPrecision: 87.500 (Average: 80.244)\n",
      "Epoch: No: [79] Batches: [150/782]\tLoss: 0.5301 (Average: 0.5841)\tPrecision: 79.688 (Average: 79.688)\n",
      "Epoch: No: [79] Batches: [200/782]\tLoss: 0.3748 (Average: 0.5873)\tPrecision: 87.500 (Average: 79.594)\n",
      "Epoch: No: [79] Batches: [250/782]\tLoss: 0.4943 (Average: 0.5840)\tPrecision: 78.125 (Average: 79.582)\n",
      "Epoch: No: [79] Batches: [300/782]\tLoss: 0.6818 (Average: 0.5811)\tPrecision: 70.312 (Average: 79.755)\n",
      "Epoch: No: [79] Batches: [350/782]\tLoss: 0.5026 (Average: 0.5781)\tPrecision: 81.250 (Average: 79.710)\n",
      "Epoch: No: [79] Batches: [400/782]\tLoss: 0.6379 (Average: 0.5797)\tPrecision: 76.562 (Average: 79.789)\n",
      "Epoch: No: [79] Batches: [450/782]\tLoss: 0.3985 (Average: 0.5828)\tPrecision: 84.375 (Average: 79.774)\n",
      "Epoch: No: [79] Batches: [500/782]\tLoss: 0.3998 (Average: 0.5823)\tPrecision: 81.250 (Average: 79.856)\n",
      "Epoch: No: [79] Batches: [550/782]\tLoss: 0.7808 (Average: 0.5841)\tPrecision: 70.312 (Average: 79.812)\n",
      "Epoch: No: [79] Batches: [600/782]\tLoss: 0.8311 (Average: 0.5837)\tPrecision: 75.000 (Average: 79.838)\n",
      "Epoch: No: [79] Batches: [650/782]\tLoss: 0.4506 (Average: 0.5814)\tPrecision: 82.812 (Average: 79.901)\n",
      "Epoch: No: [79] Batches: [700/782]\tLoss: 0.7900 (Average: 0.5827)\tPrecision: 68.750 (Average: 79.830)\n",
      "Epoch: No: [79] Batches: [750/782]\tLoss: 0.4947 (Average: 0.5839)\tPrecision: 79.688 (Average: 79.760)\n",
      "Test Accuracy\t  Top Precision: 75.020 (Error: 24.980 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [80] Batches: [0/782]\tLoss: 0.5505 (Average: 0.5505)\tPrecision: 78.125 (Average: 78.125)\n",
      "Epoch: No: [80] Batches: [50/782]\tLoss: 0.5051 (Average: 0.6043)\tPrecision: 85.938 (Average: 79.259)\n",
      "Epoch: No: [80] Batches: [100/782]\tLoss: 0.5213 (Average: 0.5991)\tPrecision: 76.562 (Average: 79.146)\n",
      "Epoch: No: [80] Batches: [150/782]\tLoss: 0.7419 (Average: 0.5982)\tPrecision: 75.000 (Average: 79.056)\n",
      "Epoch: No: [80] Batches: [200/782]\tLoss: 0.5849 (Average: 0.5783)\tPrecision: 79.688 (Average: 79.726)\n",
      "Epoch: No: [80] Batches: [250/782]\tLoss: 0.6275 (Average: 0.5811)\tPrecision: 76.562 (Average: 79.644)\n",
      "Epoch: No: [80] Batches: [300/782]\tLoss: 0.7228 (Average: 0.5773)\tPrecision: 75.000 (Average: 79.817)\n",
      "Epoch: No: [80] Batches: [350/782]\tLoss: 0.5173 (Average: 0.5842)\tPrecision: 84.375 (Average: 79.607)\n",
      "Epoch: No: [80] Batches: [400/782]\tLoss: 0.5134 (Average: 0.5901)\tPrecision: 81.250 (Average: 79.380)\n",
      "Epoch: No: [80] Batches: [450/782]\tLoss: 0.4140 (Average: 0.5857)\tPrecision: 84.375 (Average: 79.518)\n",
      "Epoch: No: [80] Batches: [500/782]\tLoss: 0.9230 (Average: 0.5855)\tPrecision: 75.000 (Average: 79.628)\n",
      "Epoch: No: [80] Batches: [550/782]\tLoss: 0.5759 (Average: 0.5884)\tPrecision: 78.125 (Average: 79.540)\n",
      "Epoch: No: [80] Batches: [600/782]\tLoss: 0.5170 (Average: 0.5872)\tPrecision: 79.688 (Average: 79.578)\n",
      "Epoch: No: [80] Batches: [650/782]\tLoss: 0.8825 (Average: 0.5876)\tPrecision: 73.438 (Average: 79.594)\n",
      "Epoch: No: [80] Batches: [700/782]\tLoss: 0.6132 (Average: 0.5887)\tPrecision: 76.562 (Average: 79.558)\n",
      "Epoch: No: [80] Batches: [750/782]\tLoss: 0.6370 (Average: 0.5892)\tPrecision: 73.438 (Average: 79.536)\n",
      "Test Accuracy\t  Top Precision: 68.100 (Error: 31.900 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [81] Batches: [0/782]\tLoss: 0.5130 (Average: 0.5130)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [81] Batches: [50/782]\tLoss: 0.5791 (Average: 0.5877)\tPrecision: 79.688 (Average: 79.381)\n",
      "Epoch: No: [81] Batches: [100/782]\tLoss: 0.7573 (Average: 0.5860)\tPrecision: 73.438 (Average: 79.564)\n",
      "Epoch: No: [81] Batches: [150/782]\tLoss: 0.7964 (Average: 0.5718)\tPrecision: 75.000 (Average: 80.081)\n",
      "Epoch: No: [81] Batches: [200/782]\tLoss: 0.6285 (Average: 0.5760)\tPrecision: 79.688 (Average: 80.037)\n",
      "Epoch: No: [81] Batches: [250/782]\tLoss: 0.5917 (Average: 0.5795)\tPrecision: 76.562 (Average: 79.843)\n",
      "Epoch: No: [81] Batches: [300/782]\tLoss: 0.7573 (Average: 0.5776)\tPrecision: 78.125 (Average: 79.916)\n",
      "Epoch: No: [81] Batches: [350/782]\tLoss: 0.4964 (Average: 0.5778)\tPrecision: 87.500 (Average: 79.950)\n",
      "Epoch: No: [81] Batches: [400/782]\tLoss: 0.6013 (Average: 0.5763)\tPrecision: 85.938 (Average: 80.073)\n",
      "Epoch: No: [81] Batches: [450/782]\tLoss: 0.7320 (Average: 0.5783)\tPrecision: 76.562 (Average: 80.093)\n",
      "Epoch: No: [81] Batches: [500/782]\tLoss: 0.7654 (Average: 0.5801)\tPrecision: 71.875 (Average: 79.903)\n",
      "Epoch: No: [81] Batches: [550/782]\tLoss: 0.3875 (Average: 0.5804)\tPrecision: 92.188 (Average: 79.940)\n",
      "Epoch: No: [81] Batches: [600/782]\tLoss: 0.3768 (Average: 0.5808)\tPrecision: 85.938 (Average: 79.898)\n",
      "Epoch: No: [81] Batches: [650/782]\tLoss: 0.6897 (Average: 0.5793)\tPrecision: 78.125 (Average: 80.014)\n",
      "Epoch: No: [81] Batches: [700/782]\tLoss: 0.4787 (Average: 0.5800)\tPrecision: 84.375 (Average: 79.971)\n",
      "Epoch: No: [81] Batches: [750/782]\tLoss: 0.6563 (Average: 0.5818)\tPrecision: 78.125 (Average: 79.858)\n",
      "Test Accuracy\t  Top Precision: 74.710 (Error: 25.290 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [82] Batches: [0/782]\tLoss: 0.4451 (Average: 0.4451)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [82] Batches: [50/782]\tLoss: 0.4846 (Average: 0.5766)\tPrecision: 82.812 (Average: 79.749)\n",
      "Epoch: No: [82] Batches: [100/782]\tLoss: 0.6694 (Average: 0.5744)\tPrecision: 75.000 (Average: 79.889)\n",
      "Epoch: No: [82] Batches: [150/782]\tLoss: 0.5537 (Average: 0.5626)\tPrecision: 75.000 (Average: 80.381)\n",
      "Epoch: No: [82] Batches: [200/782]\tLoss: 0.4786 (Average: 0.5602)\tPrecision: 85.938 (Average: 80.348)\n",
      "Epoch: No: [82] Batches: [250/782]\tLoss: 0.4455 (Average: 0.5559)\tPrecision: 85.938 (Average: 80.515)\n",
      "Epoch: No: [82] Batches: [300/782]\tLoss: 0.4508 (Average: 0.5623)\tPrecision: 85.938 (Average: 80.259)\n",
      "Epoch: No: [82] Batches: [350/782]\tLoss: 0.7504 (Average: 0.5650)\tPrecision: 78.125 (Average: 80.293)\n",
      "Epoch: No: [82] Batches: [400/782]\tLoss: 0.5072 (Average: 0.5691)\tPrecision: 84.375 (Average: 80.097)\n",
      "Epoch: No: [82] Batches: [450/782]\tLoss: 0.5928 (Average: 0.5736)\tPrecision: 75.000 (Average: 79.927)\n",
      "Epoch: No: [82] Batches: [500/782]\tLoss: 0.3780 (Average: 0.5759)\tPrecision: 90.625 (Average: 79.937)\n",
      "Epoch: No: [82] Batches: [550/782]\tLoss: 0.7991 (Average: 0.5785)\tPrecision: 73.438 (Average: 79.931)\n",
      "Epoch: No: [82] Batches: [600/782]\tLoss: 0.6961 (Average: 0.5787)\tPrecision: 76.562 (Average: 79.903)\n",
      "Epoch: No: [82] Batches: [650/782]\tLoss: 0.5581 (Average: 0.5782)\tPrecision: 79.688 (Average: 79.908)\n",
      "Epoch: No: [82] Batches: [700/782]\tLoss: 0.5015 (Average: 0.5761)\tPrecision: 81.250 (Average: 79.957)\n",
      "Epoch: No: [82] Batches: [750/782]\tLoss: 0.5437 (Average: 0.5752)\tPrecision: 82.812 (Average: 80.008)\n",
      "Test Accuracy\t  Top Precision: 71.640 (Error: 28.360 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [83] Batches: [0/782]\tLoss: 0.5620 (Average: 0.5620)\tPrecision: 81.250 (Average: 81.250)\n",
      "Epoch: No: [83] Batches: [50/782]\tLoss: 0.7266 (Average: 0.5597)\tPrecision: 75.000 (Average: 80.637)\n",
      "Epoch: No: [83] Batches: [100/782]\tLoss: 0.6406 (Average: 0.5674)\tPrecision: 79.688 (Average: 80.631)\n",
      "Epoch: No: [83] Batches: [150/782]\tLoss: 0.4279 (Average: 0.5602)\tPrecision: 87.500 (Average: 80.743)\n",
      "Epoch: No: [83] Batches: [200/782]\tLoss: 0.8725 (Average: 0.5717)\tPrecision: 70.312 (Average: 80.333)\n",
      "Epoch: No: [83] Batches: [250/782]\tLoss: 0.3871 (Average: 0.5788)\tPrecision: 87.500 (Average: 80.123)\n",
      "Epoch: No: [83] Batches: [300/782]\tLoss: 0.6045 (Average: 0.5773)\tPrecision: 78.125 (Average: 80.181)\n",
      "Epoch: No: [83] Batches: [350/782]\tLoss: 0.3275 (Average: 0.5758)\tPrecision: 85.938 (Average: 80.182)\n",
      "Epoch: No: [83] Batches: [400/782]\tLoss: 0.5059 (Average: 0.5745)\tPrecision: 78.125 (Average: 80.210)\n",
      "Epoch: No: [83] Batches: [450/782]\tLoss: 0.6383 (Average: 0.5729)\tPrecision: 79.688 (Average: 80.311)\n",
      "Epoch: No: [83] Batches: [500/782]\tLoss: 0.6051 (Average: 0.5712)\tPrecision: 79.688 (Average: 80.405)\n",
      "Epoch: No: [83] Batches: [550/782]\tLoss: 0.4762 (Average: 0.5710)\tPrecision: 87.500 (Average: 80.357)\n",
      "Epoch: No: [83] Batches: [600/782]\tLoss: 0.4515 (Average: 0.5735)\tPrecision: 85.938 (Average: 80.249)\n",
      "Epoch: No: [83] Batches: [650/782]\tLoss: 0.8241 (Average: 0.5738)\tPrecision: 81.250 (Average: 80.247)\n",
      "Epoch: No: [83] Batches: [700/782]\tLoss: 0.6485 (Average: 0.5738)\tPrecision: 76.562 (Average: 80.200)\n",
      "Epoch: No: [83] Batches: [750/782]\tLoss: 0.8234 (Average: 0.5761)\tPrecision: 71.875 (Average: 80.133)\n",
      "Test Accuracy\t  Top Precision: 77.320 (Error: 22.680 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [84] Batches: [0/782]\tLoss: 0.6304 (Average: 0.6304)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [84] Batches: [50/782]\tLoss: 0.2720 (Average: 0.5291)\tPrecision: 89.062 (Average: 81.740)\n",
      "Epoch: No: [84] Batches: [100/782]\tLoss: 0.5797 (Average: 0.5471)\tPrecision: 79.688 (Average: 81.250)\n",
      "Epoch: No: [84] Batches: [150/782]\tLoss: 0.6308 (Average: 0.5560)\tPrecision: 79.688 (Average: 80.857)\n",
      "Epoch: No: [84] Batches: [200/782]\tLoss: 0.4025 (Average: 0.5583)\tPrecision: 84.375 (Average: 80.667)\n",
      "Epoch: No: [84] Batches: [250/782]\tLoss: 0.5101 (Average: 0.5581)\tPrecision: 85.938 (Average: 80.547)\n",
      "Epoch: No: [84] Batches: [300/782]\tLoss: 0.3667 (Average: 0.5521)\tPrecision: 84.375 (Average: 80.788)\n",
      "Epoch: No: [84] Batches: [350/782]\tLoss: 0.4113 (Average: 0.5504)\tPrecision: 82.812 (Average: 80.787)\n",
      "Epoch: No: [84] Batches: [400/782]\tLoss: 0.6372 (Average: 0.5574)\tPrecision: 81.250 (Average: 80.642)\n",
      "Epoch: No: [84] Batches: [450/782]\tLoss: 0.6069 (Average: 0.5566)\tPrecision: 79.688 (Average: 80.654)\n",
      "Epoch: No: [84] Batches: [500/782]\tLoss: 0.8228 (Average: 0.5596)\tPrecision: 78.125 (Average: 80.629)\n",
      "Epoch: No: [84] Batches: [550/782]\tLoss: 0.8383 (Average: 0.5643)\tPrecision: 71.875 (Average: 80.470)\n",
      "Epoch: No: [84] Batches: [600/782]\tLoss: 0.6713 (Average: 0.5672)\tPrecision: 78.125 (Average: 80.332)\n",
      "Epoch: No: [84] Batches: [650/782]\tLoss: 0.5134 (Average: 0.5687)\tPrecision: 81.250 (Average: 80.288)\n",
      "Epoch: No: [84] Batches: [700/782]\tLoss: 0.7679 (Average: 0.5695)\tPrecision: 68.750 (Average: 80.287)\n",
      "Epoch: No: [84] Batches: [750/782]\tLoss: 0.4191 (Average: 0.5702)\tPrecision: 84.375 (Average: 80.299)\n",
      "Test Accuracy\t  Top Precision: 74.260 (Error: 25.740 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [85] Batches: [0/782]\tLoss: 0.6097 (Average: 0.6097)\tPrecision: 79.688 (Average: 79.688)\n",
      "Epoch: No: [85] Batches: [50/782]\tLoss: 0.4078 (Average: 0.5297)\tPrecision: 87.500 (Average: 81.556)\n",
      "Epoch: No: [85] Batches: [100/782]\tLoss: 0.3628 (Average: 0.5460)\tPrecision: 87.500 (Average: 80.755)\n",
      "Epoch: No: [85] Batches: [150/782]\tLoss: 0.6240 (Average: 0.5509)\tPrecision: 75.000 (Average: 80.608)\n",
      "Epoch: No: [85] Batches: [200/782]\tLoss: 0.4938 (Average: 0.5569)\tPrecision: 76.562 (Average: 80.410)\n",
      "Epoch: No: [85] Batches: [250/782]\tLoss: 0.3853 (Average: 0.5581)\tPrecision: 85.938 (Average: 80.385)\n",
      "Epoch: No: [85] Batches: [300/782]\tLoss: 0.6067 (Average: 0.5621)\tPrecision: 76.562 (Average: 80.378)\n",
      "Epoch: No: [85] Batches: [350/782]\tLoss: 0.5199 (Average: 0.5613)\tPrecision: 81.250 (Average: 80.377)\n",
      "Epoch: No: [85] Batches: [400/782]\tLoss: 0.6154 (Average: 0.5639)\tPrecision: 79.688 (Average: 80.346)\n",
      "Epoch: No: [85] Batches: [450/782]\tLoss: 0.2902 (Average: 0.5633)\tPrecision: 87.500 (Average: 80.419)\n",
      "Epoch: No: [85] Batches: [500/782]\tLoss: 0.6344 (Average: 0.5668)\tPrecision: 78.125 (Average: 80.339)\n",
      "Epoch: No: [85] Batches: [550/782]\tLoss: 0.6960 (Average: 0.5690)\tPrecision: 79.688 (Average: 80.348)\n",
      "Epoch: No: [85] Batches: [600/782]\tLoss: 0.7193 (Average: 0.5683)\tPrecision: 76.562 (Average: 80.371)\n",
      "Epoch: No: [85] Batches: [650/782]\tLoss: 0.7245 (Average: 0.5691)\tPrecision: 76.562 (Average: 80.391)\n",
      "Epoch: No: [85] Batches: [700/782]\tLoss: 0.5793 (Average: 0.5693)\tPrecision: 82.812 (Average: 80.392)\n",
      "Epoch: No: [85] Batches: [750/782]\tLoss: 0.7427 (Average: 0.5701)\tPrecision: 76.562 (Average: 80.382)\n",
      "Test Accuracy\t  Top Precision: 73.510 (Error: 26.490 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [86] Batches: [0/782]\tLoss: 0.4075 (Average: 0.4075)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [86] Batches: [50/782]\tLoss: 0.4295 (Average: 0.5653)\tPrecision: 89.062 (Average: 80.668)\n",
      "Epoch: No: [86] Batches: [100/782]\tLoss: 0.3632 (Average: 0.5518)\tPrecision: 87.500 (Average: 81.033)\n",
      "Epoch: No: [86] Batches: [150/782]\tLoss: 0.7759 (Average: 0.5485)\tPrecision: 73.438 (Average: 81.043)\n",
      "Epoch: No: [86] Batches: [200/782]\tLoss: 0.3657 (Average: 0.5514)\tPrecision: 87.500 (Average: 80.846)\n",
      "Epoch: No: [86] Batches: [250/782]\tLoss: 0.6409 (Average: 0.5510)\tPrecision: 81.250 (Average: 80.852)\n",
      "Epoch: No: [86] Batches: [300/782]\tLoss: 0.4470 (Average: 0.5499)\tPrecision: 81.250 (Average: 80.788)\n",
      "Epoch: No: [86] Batches: [350/782]\tLoss: 0.5833 (Average: 0.5494)\tPrecision: 76.562 (Average: 80.823)\n",
      "Epoch: No: [86] Batches: [400/782]\tLoss: 0.4366 (Average: 0.5517)\tPrecision: 87.500 (Average: 80.712)\n",
      "Epoch: No: [86] Batches: [450/782]\tLoss: 0.6192 (Average: 0.5497)\tPrecision: 73.438 (Average: 80.827)\n",
      "Epoch: No: [86] Batches: [500/782]\tLoss: 0.4905 (Average: 0.5548)\tPrecision: 82.812 (Average: 80.776)\n",
      "Epoch: No: [86] Batches: [550/782]\tLoss: 0.5029 (Average: 0.5580)\tPrecision: 84.375 (Average: 80.759)\n",
      "Epoch: No: [86] Batches: [600/782]\tLoss: 0.5537 (Average: 0.5589)\tPrecision: 84.375 (Average: 80.746)\n",
      "Epoch: No: [86] Batches: [650/782]\tLoss: 0.5023 (Average: 0.5597)\tPrecision: 82.812 (Average: 80.688)\n",
      "Epoch: No: [86] Batches: [700/782]\tLoss: 0.9150 (Average: 0.5605)\tPrecision: 68.750 (Average: 80.702)\n",
      "Epoch: No: [86] Batches: [750/782]\tLoss: 0.7287 (Average: 0.5616)\tPrecision: 76.562 (Average: 80.630)\n",
      "Test Accuracy\t  Top Precision: 72.930 (Error: 27.070 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [87] Batches: [0/782]\tLoss: 0.5394 (Average: 0.5394)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [87] Batches: [50/782]\tLoss: 0.7611 (Average: 0.5686)\tPrecision: 71.875 (Average: 80.055)\n",
      "Epoch: No: [87] Batches: [100/782]\tLoss: 0.4372 (Average: 0.5733)\tPrecision: 81.250 (Average: 80.275)\n",
      "Epoch: No: [87] Batches: [150/782]\tLoss: 0.4443 (Average: 0.5688)\tPrecision: 82.812 (Average: 80.453)\n",
      "Epoch: No: [87] Batches: [200/782]\tLoss: 0.3481 (Average: 0.5596)\tPrecision: 87.500 (Average: 80.737)\n",
      "Epoch: No: [87] Batches: [250/782]\tLoss: 0.5846 (Average: 0.5554)\tPrecision: 78.125 (Average: 80.845)\n",
      "Epoch: No: [87] Batches: [300/782]\tLoss: 0.5628 (Average: 0.5544)\tPrecision: 79.688 (Average: 80.804)\n",
      "Epoch: No: [87] Batches: [350/782]\tLoss: 0.4451 (Average: 0.5576)\tPrecision: 84.375 (Average: 80.707)\n",
      "Epoch: No: [87] Batches: [400/782]\tLoss: 0.4570 (Average: 0.5584)\tPrecision: 84.375 (Average: 80.619)\n",
      "Epoch: No: [87] Batches: [450/782]\tLoss: 0.6158 (Average: 0.5577)\tPrecision: 76.562 (Average: 80.696)\n",
      "Epoch: No: [87] Batches: [500/782]\tLoss: 0.6106 (Average: 0.5596)\tPrecision: 81.250 (Average: 80.623)\n",
      "Epoch: No: [87] Batches: [550/782]\tLoss: 0.4942 (Average: 0.5608)\tPrecision: 81.250 (Average: 80.603)\n",
      "Epoch: No: [87] Batches: [600/782]\tLoss: 0.6618 (Average: 0.5583)\tPrecision: 75.000 (Average: 80.623)\n",
      "Epoch: No: [87] Batches: [650/782]\tLoss: 0.4920 (Average: 0.5585)\tPrecision: 84.375 (Average: 80.650)\n",
      "Epoch: No: [87] Batches: [700/782]\tLoss: 0.3768 (Average: 0.5604)\tPrecision: 82.812 (Average: 80.617)\n",
      "Epoch: No: [87] Batches: [750/782]\tLoss: 0.4060 (Average: 0.5612)\tPrecision: 85.938 (Average: 80.578)\n",
      "Test Accuracy\t  Top Precision: 73.940 (Error: 26.060 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [88] Batches: [0/782]\tLoss: 0.5257 (Average: 0.5257)\tPrecision: 79.688 (Average: 79.688)\n",
      "Epoch: No: [88] Batches: [50/782]\tLoss: 0.6371 (Average: 0.5687)\tPrecision: 81.250 (Average: 80.331)\n",
      "Epoch: No: [88] Batches: [100/782]\tLoss: 0.2763 (Average: 0.5607)\tPrecision: 90.625 (Average: 80.585)\n",
      "Epoch: No: [88] Batches: [150/782]\tLoss: 0.5482 (Average: 0.5704)\tPrecision: 84.375 (Average: 80.370)\n",
      "Epoch: No: [88] Batches: [200/782]\tLoss: 0.6670 (Average: 0.5716)\tPrecision: 70.312 (Average: 80.340)\n",
      "Epoch: No: [88] Batches: [250/782]\tLoss: 0.4268 (Average: 0.5658)\tPrecision: 84.375 (Average: 80.459)\n",
      "Epoch: No: [88] Batches: [300/782]\tLoss: 0.5388 (Average: 0.5655)\tPrecision: 84.375 (Average: 80.419)\n",
      "Epoch: No: [88] Batches: [350/782]\tLoss: 0.6216 (Average: 0.5655)\tPrecision: 76.562 (Average: 80.355)\n",
      "Epoch: No: [88] Batches: [400/782]\tLoss: 0.4819 (Average: 0.5664)\tPrecision: 82.812 (Average: 80.311)\n",
      "Epoch: No: [88] Batches: [450/782]\tLoss: 0.4393 (Average: 0.5672)\tPrecision: 82.812 (Average: 80.346)\n",
      "Epoch: No: [88] Batches: [500/782]\tLoss: 0.6174 (Average: 0.5676)\tPrecision: 81.250 (Average: 80.342)\n",
      "Epoch: No: [88] Batches: [550/782]\tLoss: 0.5188 (Average: 0.5660)\tPrecision: 84.375 (Average: 80.394)\n",
      "Epoch: No: [88] Batches: [600/782]\tLoss: 0.7127 (Average: 0.5657)\tPrecision: 79.688 (Average: 80.421)\n",
      "Epoch: No: [88] Batches: [650/782]\tLoss: 0.5230 (Average: 0.5645)\tPrecision: 76.562 (Average: 80.410)\n",
      "Epoch: No: [88] Batches: [700/782]\tLoss: 0.6890 (Average: 0.5639)\tPrecision: 78.125 (Average: 80.494)\n",
      "Epoch: No: [88] Batches: [750/782]\tLoss: 0.3852 (Average: 0.5622)\tPrecision: 89.062 (Average: 80.549)\n",
      "Test Accuracy\t  Top Precision: 69.830 (Error: 30.170 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [89] Batches: [0/782]\tLoss: 0.5841 (Average: 0.5841)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [89] Batches: [50/782]\tLoss: 0.6405 (Average: 0.5856)\tPrecision: 78.125 (Average: 79.289)\n",
      "Epoch: No: [89] Batches: [100/782]\tLoss: 0.3501 (Average: 0.5466)\tPrecision: 87.500 (Average: 81.018)\n",
      "Epoch: No: [89] Batches: [150/782]\tLoss: 0.6148 (Average: 0.5463)\tPrecision: 78.125 (Average: 81.126)\n",
      "Epoch: No: [89] Batches: [200/782]\tLoss: 0.4799 (Average: 0.5409)\tPrecision: 81.250 (Average: 81.281)\n",
      "Epoch: No: [89] Batches: [250/782]\tLoss: 0.6212 (Average: 0.5447)\tPrecision: 76.562 (Average: 81.113)\n",
      "Epoch: No: [89] Batches: [300/782]\tLoss: 0.5401 (Average: 0.5464)\tPrecision: 75.000 (Average: 81.094)\n",
      "Epoch: No: [89] Batches: [350/782]\tLoss: 0.6583 (Average: 0.5489)\tPrecision: 82.812 (Average: 81.050)\n",
      "Epoch: No: [89] Batches: [400/782]\tLoss: 0.6762 (Average: 0.5555)\tPrecision: 71.875 (Average: 80.860)\n",
      "Epoch: No: [89] Batches: [450/782]\tLoss: 0.6325 (Average: 0.5556)\tPrecision: 76.562 (Average: 80.800)\n",
      "Epoch: No: [89] Batches: [500/782]\tLoss: 0.6799 (Average: 0.5567)\tPrecision: 75.000 (Average: 80.773)\n",
      "Epoch: No: [89] Batches: [550/782]\tLoss: 0.4231 (Average: 0.5562)\tPrecision: 82.812 (Average: 80.734)\n",
      "Epoch: No: [89] Batches: [600/782]\tLoss: 0.4522 (Average: 0.5564)\tPrecision: 85.938 (Average: 80.694)\n",
      "Epoch: No: [89] Batches: [650/782]\tLoss: 0.5583 (Average: 0.5614)\tPrecision: 81.250 (Average: 80.528)\n",
      "Epoch: No: [89] Batches: [700/782]\tLoss: 0.5749 (Average: 0.5592)\tPrecision: 73.438 (Average: 80.613)\n",
      "Epoch: No: [89] Batches: [750/782]\tLoss: 0.5309 (Average: 0.5587)\tPrecision: 84.375 (Average: 80.624)\n",
      "Test Accuracy\t  Top Precision: 67.310 (Error: 32.690 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [90] Batches: [0/782]\tLoss: 0.5531 (Average: 0.5531)\tPrecision: 78.125 (Average: 78.125)\n",
      "Epoch: No: [90] Batches: [50/782]\tLoss: 0.5930 (Average: 0.5769)\tPrecision: 79.688 (Average: 80.025)\n",
      "Epoch: No: [90] Batches: [100/782]\tLoss: 0.7500 (Average: 0.5602)\tPrecision: 75.000 (Average: 80.461)\n",
      "Epoch: No: [90] Batches: [150/782]\tLoss: 0.4484 (Average: 0.5532)\tPrecision: 89.062 (Average: 80.826)\n",
      "Epoch: No: [90] Batches: [200/782]\tLoss: 0.8530 (Average: 0.5540)\tPrecision: 71.875 (Average: 80.900)\n",
      "Epoch: No: [90] Batches: [250/782]\tLoss: 0.5685 (Average: 0.5493)\tPrecision: 82.812 (Average: 81.082)\n",
      "Epoch: No: [90] Batches: [300/782]\tLoss: 0.5205 (Average: 0.5471)\tPrecision: 79.688 (Average: 81.022)\n",
      "Epoch: No: [90] Batches: [350/782]\tLoss: 0.3474 (Average: 0.5439)\tPrecision: 90.625 (Average: 81.179)\n",
      "Epoch: No: [90] Batches: [400/782]\tLoss: 0.4167 (Average: 0.5408)\tPrecision: 89.062 (Average: 81.231)\n",
      "Epoch: No: [90] Batches: [450/782]\tLoss: 0.4845 (Average: 0.5420)\tPrecision: 85.938 (Average: 81.150)\n",
      "Epoch: No: [90] Batches: [500/782]\tLoss: 0.4599 (Average: 0.5429)\tPrecision: 82.812 (Average: 81.160)\n",
      "Epoch: No: [90] Batches: [550/782]\tLoss: 0.4761 (Average: 0.5430)\tPrecision: 82.812 (Average: 81.190)\n",
      "Epoch: No: [90] Batches: [600/782]\tLoss: 0.6080 (Average: 0.5439)\tPrecision: 82.812 (Average: 81.156)\n",
      "Epoch: No: [90] Batches: [650/782]\tLoss: 0.7308 (Average: 0.5459)\tPrecision: 76.562 (Average: 81.140)\n",
      "Epoch: No: [90] Batches: [700/782]\tLoss: 0.7245 (Average: 0.5462)\tPrecision: 78.125 (Average: 81.172)\n",
      "Epoch: No: [90] Batches: [750/782]\tLoss: 0.7837 (Average: 0.5486)\tPrecision: 75.000 (Average: 81.042)\n",
      "Test Accuracy\t  Top Precision: 76.290 (Error: 23.710 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [91] Batches: [0/782]\tLoss: 0.5547 (Average: 0.5547)\tPrecision: 79.688 (Average: 79.688)\n",
      "Epoch: No: [91] Batches: [50/782]\tLoss: 0.5927 (Average: 0.5616)\tPrecision: 78.125 (Average: 81.127)\n",
      "Epoch: No: [91] Batches: [100/782]\tLoss: 0.5438 (Average: 0.5554)\tPrecision: 79.688 (Average: 80.987)\n",
      "Epoch: No: [91] Batches: [150/782]\tLoss: 0.5962 (Average: 0.5566)\tPrecision: 82.812 (Average: 80.815)\n",
      "Epoch: No: [91] Batches: [200/782]\tLoss: 0.8108 (Average: 0.5571)\tPrecision: 73.438 (Average: 80.745)\n",
      "Epoch: No: [91] Batches: [250/782]\tLoss: 0.4335 (Average: 0.5556)\tPrecision: 87.500 (Average: 80.926)\n",
      "Epoch: No: [91] Batches: [300/782]\tLoss: 0.4137 (Average: 0.5489)\tPrecision: 84.375 (Average: 81.079)\n",
      "Epoch: No: [91] Batches: [350/782]\tLoss: 0.7216 (Average: 0.5490)\tPrecision: 75.000 (Average: 80.996)\n",
      "Epoch: No: [91] Batches: [400/782]\tLoss: 0.8290 (Average: 0.5528)\tPrecision: 71.875 (Average: 80.938)\n",
      "Epoch: No: [91] Batches: [450/782]\tLoss: 0.5240 (Average: 0.5538)\tPrecision: 79.688 (Average: 80.938)\n",
      "Epoch: No: [91] Batches: [500/782]\tLoss: 0.6250 (Average: 0.5516)\tPrecision: 75.000 (Average: 81.038)\n",
      "Epoch: No: [91] Batches: [550/782]\tLoss: 0.6585 (Average: 0.5487)\tPrecision: 75.000 (Average: 81.165)\n",
      "Epoch: No: [91] Batches: [600/782]\tLoss: 0.4942 (Average: 0.5475)\tPrecision: 84.375 (Average: 81.203)\n",
      "Epoch: No: [91] Batches: [650/782]\tLoss: 0.4966 (Average: 0.5490)\tPrecision: 84.375 (Average: 81.152)\n",
      "Epoch: No: [91] Batches: [700/782]\tLoss: 0.4450 (Average: 0.5496)\tPrecision: 81.250 (Average: 81.090)\n",
      "Epoch: No: [91] Batches: [750/782]\tLoss: 0.6566 (Average: 0.5486)\tPrecision: 73.438 (Average: 81.127)\n",
      "Test Accuracy\t  Top Precision: 78.200 (Error: 21.800 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [92] Batches: [0/782]\tLoss: 0.5781 (Average: 0.5781)\tPrecision: 81.250 (Average: 81.250)\n",
      "Epoch: No: [92] Batches: [50/782]\tLoss: 0.5897 (Average: 0.5322)\tPrecision: 75.000 (Average: 82.200)\n",
      "Epoch: No: [92] Batches: [100/782]\tLoss: 0.7020 (Average: 0.5380)\tPrecision: 70.312 (Average: 81.683)\n",
      "Epoch: No: [92] Batches: [150/782]\tLoss: 0.6190 (Average: 0.5488)\tPrecision: 76.562 (Average: 81.167)\n",
      "Epoch: No: [92] Batches: [200/782]\tLoss: 0.5519 (Average: 0.5562)\tPrecision: 78.125 (Average: 80.698)\n",
      "Epoch: No: [92] Batches: [250/782]\tLoss: 0.5560 (Average: 0.5504)\tPrecision: 82.812 (Average: 80.920)\n",
      "Epoch: No: [92] Batches: [300/782]\tLoss: 0.7340 (Average: 0.5465)\tPrecision: 73.438 (Average: 81.058)\n",
      "Epoch: No: [92] Batches: [350/782]\tLoss: 0.5890 (Average: 0.5506)\tPrecision: 78.125 (Average: 81.005)\n",
      "Epoch: No: [92] Batches: [400/782]\tLoss: 0.4451 (Average: 0.5477)\tPrecision: 90.625 (Average: 81.047)\n",
      "Epoch: No: [92] Batches: [450/782]\tLoss: 0.4803 (Average: 0.5453)\tPrecision: 84.375 (Average: 81.073)\n",
      "Epoch: No: [92] Batches: [500/782]\tLoss: 0.5094 (Average: 0.5435)\tPrecision: 81.250 (Average: 81.122)\n",
      "Epoch: No: [92] Batches: [550/782]\tLoss: 0.3678 (Average: 0.5449)\tPrecision: 89.062 (Average: 81.074)\n",
      "Epoch: No: [92] Batches: [600/782]\tLoss: 0.5428 (Average: 0.5456)\tPrecision: 79.688 (Average: 81.032)\n",
      "Epoch: No: [92] Batches: [650/782]\tLoss: 0.4853 (Average: 0.5461)\tPrecision: 84.375 (Average: 81.036)\n",
      "Epoch: No: [92] Batches: [700/782]\tLoss: 0.5890 (Average: 0.5474)\tPrecision: 81.250 (Average: 80.994)\n",
      "Epoch: No: [92] Batches: [750/782]\tLoss: 0.7185 (Average: 0.5495)\tPrecision: 79.688 (Average: 80.977)\n",
      "Test Accuracy\t  Top Precision: 78.280 (Error: 21.720 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [93] Batches: [0/782]\tLoss: 0.2734 (Average: 0.2734)\tPrecision: 89.062 (Average: 89.062)\n",
      "Epoch: No: [93] Batches: [50/782]\tLoss: 0.4243 (Average: 0.5017)\tPrecision: 85.938 (Average: 83.211)\n",
      "Epoch: No: [93] Batches: [100/782]\tLoss: 0.4311 (Average: 0.5072)\tPrecision: 81.250 (Average: 82.658)\n",
      "Epoch: No: [93] Batches: [150/782]\tLoss: 0.4738 (Average: 0.5184)\tPrecision: 78.125 (Average: 81.995)\n",
      "Epoch: No: [93] Batches: [200/782]\tLoss: 0.3964 (Average: 0.5210)\tPrecision: 89.062 (Average: 81.911)\n",
      "Epoch: No: [93] Batches: [250/782]\tLoss: 0.3979 (Average: 0.5246)\tPrecision: 85.938 (Average: 81.916)\n",
      "Epoch: No: [93] Batches: [300/782]\tLoss: 0.4497 (Average: 0.5239)\tPrecision: 84.375 (Average: 81.868)\n",
      "Epoch: No: [93] Batches: [350/782]\tLoss: 0.5867 (Average: 0.5277)\tPrecision: 84.375 (Average: 81.753)\n",
      "Epoch: No: [93] Batches: [400/782]\tLoss: 0.4126 (Average: 0.5260)\tPrecision: 82.812 (Average: 81.870)\n",
      "Epoch: No: [93] Batches: [450/782]\tLoss: 0.5463 (Average: 0.5291)\tPrecision: 82.812 (Average: 81.770)\n",
      "Epoch: No: [93] Batches: [500/782]\tLoss: 0.7297 (Average: 0.5286)\tPrecision: 75.000 (Average: 81.790)\n",
      "Epoch: No: [93] Batches: [550/782]\tLoss: 0.6576 (Average: 0.5296)\tPrecision: 76.562 (Average: 81.792)\n",
      "Epoch: No: [93] Batches: [600/782]\tLoss: 0.4207 (Average: 0.5312)\tPrecision: 85.938 (Average: 81.726)\n",
      "Epoch: No: [93] Batches: [650/782]\tLoss: 0.5840 (Average: 0.5339)\tPrecision: 79.688 (Average: 81.593)\n",
      "Epoch: No: [93] Batches: [700/782]\tLoss: 0.2399 (Average: 0.5339)\tPrecision: 95.312 (Average: 81.573)\n",
      "Epoch: No: [93] Batches: [750/782]\tLoss: 0.4448 (Average: 0.5347)\tPrecision: 85.938 (Average: 81.535)\n",
      "Test Accuracy\t  Top Precision: 75.440 (Error: 24.560 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [94] Batches: [0/782]\tLoss: 0.3199 (Average: 0.3199)\tPrecision: 87.500 (Average: 87.500)\n",
      "Epoch: No: [94] Batches: [50/782]\tLoss: 0.5111 (Average: 0.5361)\tPrecision: 78.125 (Average: 81.526)\n",
      "Epoch: No: [94] Batches: [100/782]\tLoss: 0.5719 (Average: 0.5170)\tPrecision: 79.688 (Average: 81.822)\n",
      "Epoch: No: [94] Batches: [150/782]\tLoss: 0.5576 (Average: 0.5228)\tPrecision: 81.250 (Average: 81.643)\n",
      "Epoch: No: [94] Batches: [200/782]\tLoss: 0.5862 (Average: 0.5250)\tPrecision: 75.000 (Average: 81.802)\n",
      "Epoch: No: [94] Batches: [250/782]\tLoss: 0.5855 (Average: 0.5321)\tPrecision: 82.812 (Average: 81.536)\n",
      "Epoch: No: [94] Batches: [300/782]\tLoss: 0.7534 (Average: 0.5336)\tPrecision: 79.688 (Average: 81.536)\n",
      "Epoch: No: [94] Batches: [350/782]\tLoss: 0.6402 (Average: 0.5354)\tPrecision: 79.688 (Average: 81.419)\n",
      "Epoch: No: [94] Batches: [400/782]\tLoss: 0.4212 (Average: 0.5396)\tPrecision: 89.062 (Average: 81.242)\n",
      "Epoch: No: [94] Batches: [450/782]\tLoss: 0.3686 (Average: 0.5411)\tPrecision: 89.062 (Average: 81.111)\n",
      "Epoch: No: [94] Batches: [500/782]\tLoss: 0.4361 (Average: 0.5433)\tPrecision: 84.375 (Average: 81.078)\n",
      "Epoch: No: [94] Batches: [550/782]\tLoss: 0.6782 (Average: 0.5421)\tPrecision: 76.562 (Average: 81.145)\n",
      "Epoch: No: [94] Batches: [600/782]\tLoss: 0.4497 (Average: 0.5443)\tPrecision: 84.375 (Average: 81.037)\n",
      "Epoch: No: [94] Batches: [650/782]\tLoss: 0.4857 (Average: 0.5454)\tPrecision: 81.250 (Average: 81.034)\n",
      "Epoch: No: [94] Batches: [700/782]\tLoss: 0.7345 (Average: 0.5463)\tPrecision: 76.562 (Average: 81.014)\n",
      "Epoch: No: [94] Batches: [750/782]\tLoss: 0.7274 (Average: 0.5458)\tPrecision: 78.125 (Average: 81.075)\n",
      "Test Accuracy\t  Top Precision: 77.090 (Error: 22.910 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [95] Batches: [0/782]\tLoss: 0.6560 (Average: 0.6560)\tPrecision: 81.250 (Average: 81.250)\n",
      "Epoch: No: [95] Batches: [50/782]\tLoss: 0.5155 (Average: 0.5470)\tPrecision: 81.250 (Average: 81.066)\n",
      "Epoch: No: [95] Batches: [100/782]\tLoss: 0.8182 (Average: 0.5415)\tPrecision: 76.562 (Average: 81.374)\n",
      "Epoch: No: [95] Batches: [150/782]\tLoss: 0.4701 (Average: 0.5322)\tPrecision: 84.375 (Average: 81.695)\n",
      "Epoch: No: [95] Batches: [200/782]\tLoss: 0.4973 (Average: 0.5319)\tPrecision: 85.938 (Average: 81.670)\n",
      "Epoch: No: [95] Batches: [250/782]\tLoss: 0.3263 (Average: 0.5353)\tPrecision: 89.062 (Average: 81.698)\n",
      "Epoch: No: [95] Batches: [300/782]\tLoss: 0.7440 (Average: 0.5288)\tPrecision: 71.875 (Average: 81.816)\n",
      "Epoch: No: [95] Batches: [350/782]\tLoss: 0.5404 (Average: 0.5315)\tPrecision: 81.250 (Average: 81.740)\n",
      "Epoch: No: [95] Batches: [400/782]\tLoss: 0.6188 (Average: 0.5339)\tPrecision: 78.125 (Average: 81.749)\n",
      "Epoch: No: [95] Batches: [450/782]\tLoss: 0.5714 (Average: 0.5346)\tPrecision: 75.000 (Average: 81.614)\n",
      "Epoch: No: [95] Batches: [500/782]\tLoss: 0.6826 (Average: 0.5323)\tPrecision: 78.125 (Average: 81.655)\n",
      "Epoch: No: [95] Batches: [550/782]\tLoss: 0.5802 (Average: 0.5319)\tPrecision: 79.688 (Average: 81.650)\n",
      "Epoch: No: [95] Batches: [600/782]\tLoss: 0.6339 (Average: 0.5326)\tPrecision: 82.812 (Average: 81.679)\n",
      "Epoch: No: [95] Batches: [650/782]\tLoss: 0.5979 (Average: 0.5326)\tPrecision: 78.125 (Average: 81.646)\n",
      "Epoch: No: [95] Batches: [700/782]\tLoss: 0.6048 (Average: 0.5317)\tPrecision: 70.312 (Average: 81.653)\n",
      "Epoch: No: [95] Batches: [750/782]\tLoss: 0.4791 (Average: 0.5330)\tPrecision: 87.500 (Average: 81.637)\n",
      "Test Accuracy\t  Top Precision: 79.150 (Error: 20.850 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [96] Batches: [0/782]\tLoss: 0.6199 (Average: 0.6199)\tPrecision: 79.688 (Average: 79.688)\n",
      "Epoch: No: [96] Batches: [50/782]\tLoss: 0.5557 (Average: 0.5524)\tPrecision: 76.562 (Average: 81.526)\n",
      "Epoch: No: [96] Batches: [100/782]\tLoss: 0.6301 (Average: 0.5585)\tPrecision: 89.062 (Average: 81.002)\n",
      "Epoch: No: [96] Batches: [150/782]\tLoss: 0.4857 (Average: 0.5421)\tPrecision: 81.250 (Average: 81.540)\n",
      "Epoch: No: [96] Batches: [200/782]\tLoss: 0.5193 (Average: 0.5420)\tPrecision: 79.688 (Average: 81.483)\n",
      "Epoch: No: [96] Batches: [250/782]\tLoss: 0.4832 (Average: 0.5367)\tPrecision: 81.250 (Average: 81.617)\n",
      "Epoch: No: [96] Batches: [300/782]\tLoss: 0.5587 (Average: 0.5405)\tPrecision: 81.250 (Average: 81.499)\n",
      "Epoch: No: [96] Batches: [350/782]\tLoss: 0.3835 (Average: 0.5399)\tPrecision: 90.625 (Average: 81.424)\n",
      "Epoch: No: [96] Batches: [400/782]\tLoss: 0.3940 (Average: 0.5389)\tPrecision: 89.062 (Average: 81.460)\n",
      "Epoch: No: [96] Batches: [450/782]\tLoss: 0.3863 (Average: 0.5368)\tPrecision: 87.500 (Average: 81.496)\n",
      "Epoch: No: [96] Batches: [500/782]\tLoss: 0.4996 (Average: 0.5380)\tPrecision: 84.375 (Average: 81.487)\n",
      "Epoch: No: [96] Batches: [550/782]\tLoss: 0.3497 (Average: 0.5405)\tPrecision: 89.062 (Average: 81.426)\n",
      "Epoch: No: [96] Batches: [600/782]\tLoss: 0.5623 (Average: 0.5395)\tPrecision: 81.250 (Average: 81.411)\n",
      "Epoch: No: [96] Batches: [650/782]\tLoss: 0.3458 (Average: 0.5384)\tPrecision: 89.062 (Average: 81.430)\n",
      "Epoch: No: [96] Batches: [700/782]\tLoss: 0.7798 (Average: 0.5392)\tPrecision: 76.562 (Average: 81.384)\n",
      "Epoch: No: [96] Batches: [750/782]\tLoss: 0.5342 (Average: 0.5394)\tPrecision: 76.562 (Average: 81.344)\n",
      "Test Accuracy\t  Top Precision: 74.100 (Error: 25.900 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [97] Batches: [0/782]\tLoss: 0.6045 (Average: 0.6045)\tPrecision: 84.375 (Average: 84.375)\n",
      "Epoch: No: [97] Batches: [50/782]\tLoss: 0.6528 (Average: 0.5340)\tPrecision: 82.812 (Average: 81.556)\n",
      "Epoch: No: [97] Batches: [100/782]\tLoss: 0.5952 (Average: 0.5350)\tPrecision: 78.125 (Average: 81.745)\n",
      "Epoch: No: [97] Batches: [150/782]\tLoss: 0.6008 (Average: 0.5359)\tPrecision: 78.125 (Average: 81.374)\n",
      "Epoch: No: [97] Batches: [200/782]\tLoss: 0.8209 (Average: 0.5356)\tPrecision: 68.750 (Average: 81.421)\n",
      "Epoch: No: [97] Batches: [250/782]\tLoss: 0.6004 (Average: 0.5419)\tPrecision: 84.375 (Average: 81.424)\n",
      "Epoch: No: [97] Batches: [300/782]\tLoss: 0.8126 (Average: 0.5386)\tPrecision: 71.875 (Average: 81.567)\n",
      "Epoch: No: [97] Batches: [350/782]\tLoss: 0.4094 (Average: 0.5312)\tPrecision: 82.812 (Average: 81.695)\n",
      "Epoch: No: [97] Batches: [400/782]\tLoss: 0.5428 (Average: 0.5313)\tPrecision: 87.500 (Average: 81.725)\n",
      "Epoch: No: [97] Batches: [450/782]\tLoss: 0.4153 (Average: 0.5341)\tPrecision: 85.938 (Average: 81.659)\n",
      "Epoch: No: [97] Batches: [500/782]\tLoss: 0.4517 (Average: 0.5316)\tPrecision: 84.375 (Average: 81.755)\n",
      "Epoch: No: [97] Batches: [550/782]\tLoss: 0.3337 (Average: 0.5317)\tPrecision: 85.938 (Average: 81.735)\n",
      "Epoch: No: [97] Batches: [600/782]\tLoss: 0.3369 (Average: 0.5324)\tPrecision: 84.375 (Average: 81.752)\n",
      "Epoch: No: [97] Batches: [650/782]\tLoss: 0.5062 (Average: 0.5321)\tPrecision: 79.688 (Average: 81.730)\n",
      "Epoch: No: [97] Batches: [700/782]\tLoss: 0.4477 (Average: 0.5311)\tPrecision: 82.812 (Average: 81.707)\n",
      "Epoch: No: [97] Batches: [750/782]\tLoss: 0.6163 (Average: 0.5324)\tPrecision: 76.562 (Average: 81.633)\n",
      "Test Accuracy\t  Top Precision: 75.160 (Error: 24.840 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [98] Batches: [0/782]\tLoss: 0.4918 (Average: 0.4918)\tPrecision: 84.375 (Average: 84.375)\n",
      "Epoch: No: [98] Batches: [50/782]\tLoss: 0.5020 (Average: 0.5406)\tPrecision: 79.688 (Average: 80.760)\n",
      "Epoch: No: [98] Batches: [100/782]\tLoss: 0.5769 (Average: 0.5333)\tPrecision: 85.938 (Average: 81.374)\n",
      "Epoch: No: [98] Batches: [150/782]\tLoss: 0.8023 (Average: 0.5402)\tPrecision: 76.562 (Average: 81.115)\n",
      "Epoch: No: [98] Batches: [200/782]\tLoss: 0.4012 (Average: 0.5360)\tPrecision: 87.500 (Average: 81.328)\n",
      "Epoch: No: [98] Batches: [250/782]\tLoss: 0.5872 (Average: 0.5421)\tPrecision: 85.938 (Average: 81.175)\n",
      "Epoch: No: [98] Batches: [300/782]\tLoss: 0.5225 (Average: 0.5350)\tPrecision: 78.125 (Average: 81.416)\n",
      "Epoch: No: [98] Batches: [350/782]\tLoss: 0.3619 (Average: 0.5299)\tPrecision: 89.062 (Average: 81.557)\n",
      "Epoch: No: [98] Batches: [400/782]\tLoss: 0.5863 (Average: 0.5292)\tPrecision: 75.000 (Average: 81.480)\n",
      "Epoch: No: [98] Batches: [450/782]\tLoss: 0.4981 (Average: 0.5344)\tPrecision: 85.938 (Average: 81.281)\n",
      "Epoch: No: [98] Batches: [500/782]\tLoss: 0.5354 (Average: 0.5351)\tPrecision: 79.688 (Average: 81.247)\n",
      "Epoch: No: [98] Batches: [550/782]\tLoss: 0.8988 (Average: 0.5367)\tPrecision: 68.750 (Average: 81.205)\n",
      "Epoch: No: [98] Batches: [600/782]\tLoss: 0.4182 (Average: 0.5382)\tPrecision: 85.938 (Average: 81.195)\n",
      "Epoch: No: [98] Batches: [650/782]\tLoss: 0.2776 (Average: 0.5365)\tPrecision: 90.625 (Average: 81.291)\n",
      "Epoch: No: [98] Batches: [700/782]\tLoss: 0.7332 (Average: 0.5396)\tPrecision: 75.000 (Average: 81.228)\n",
      "Epoch: No: [98] Batches: [750/782]\tLoss: 0.4570 (Average: 0.5367)\tPrecision: 84.375 (Average: 81.308)\n",
      "Test Accuracy\t  Top Precision: 74.690 (Error: 25.310 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [99] Batches: [0/782]\tLoss: 0.7357 (Average: 0.7357)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [99] Batches: [50/782]\tLoss: 0.5828 (Average: 0.5017)\tPrecision: 73.438 (Average: 82.629)\n",
      "Epoch: No: [99] Batches: [100/782]\tLoss: 0.6349 (Average: 0.5105)\tPrecision: 79.688 (Average: 82.642)\n",
      "Epoch: No: [99] Batches: [150/782]\tLoss: 0.7525 (Average: 0.5174)\tPrecision: 75.000 (Average: 82.368)\n",
      "Epoch: No: [99] Batches: [200/782]\tLoss: 0.6217 (Average: 0.5201)\tPrecision: 76.562 (Average: 82.128)\n",
      "Epoch: No: [99] Batches: [250/782]\tLoss: 0.5319 (Average: 0.5184)\tPrecision: 81.250 (Average: 82.128)\n",
      "Epoch: No: [99] Batches: [300/782]\tLoss: 0.6349 (Average: 0.5168)\tPrecision: 78.125 (Average: 82.164)\n",
      "Epoch: No: [99] Batches: [350/782]\tLoss: 0.4910 (Average: 0.5125)\tPrecision: 79.688 (Average: 82.256)\n",
      "Epoch: No: [99] Batches: [400/782]\tLoss: 0.4385 (Average: 0.5111)\tPrecision: 82.812 (Average: 82.294)\n",
      "Epoch: No: [99] Batches: [450/782]\tLoss: 0.5613 (Average: 0.5154)\tPrecision: 82.812 (Average: 82.068)\n",
      "Epoch: No: [99] Batches: [500/782]\tLoss: 0.8049 (Average: 0.5146)\tPrecision: 79.688 (Average: 82.095)\n",
      "Epoch: No: [99] Batches: [550/782]\tLoss: 0.5492 (Average: 0.5162)\tPrecision: 78.125 (Average: 82.036)\n",
      "Epoch: No: [99] Batches: [600/782]\tLoss: 0.8509 (Average: 0.5172)\tPrecision: 75.000 (Average: 82.017)\n",
      "Epoch: No: [99] Batches: [650/782]\tLoss: 0.4559 (Average: 0.5166)\tPrecision: 85.938 (Average: 82.044)\n",
      "Epoch: No: [99] Batches: [700/782]\tLoss: 0.3616 (Average: 0.5169)\tPrecision: 82.812 (Average: 82.030)\n",
      "Epoch: No: [99] Batches: [750/782]\tLoss: 0.7520 (Average: 0.5181)\tPrecision: 78.125 (Average: 81.978)\n",
      "Test Accuracy\t  Top Precision: 74.710 (Error: 25.290 )\n",
      "\n",
      "The lowest error from model: Model 1 after 100 epochs is 20.850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "args=ResNetParams()\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "best_precision = 0\n",
    "best_precision = run_epochs()\n",
    "print('The lowest error from model: {} after {} epochs is {error:.3f}'.format(args.arch,args.epochs, error=100-best_precision))\n",
    "model_save_name = 'project1_model.pt'\n",
    "#path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
    "\n",
    "#Saving the generated model and testing its loading\n",
    "path = model_save_name\n",
    "torch.save(model.state_dict(), path) \n",
    "model_path = path\n",
    "model.load_state_dict(torch.load(model_path, map_location=device), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e00a0c9c-eb50-475f-9682-4cd323a5ae59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the model\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.plot(val_losses,label=\"Val\")\n",
    "plt.plot(train_losses,label=\"Train\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('plot_graph.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "22cf10741174a73c16493d470940eeed718720095727e52348075ea0a9a6dd40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
