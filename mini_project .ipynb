{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1f56a5f4-3107-444c-8ef5-da497b39bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import time\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c9244a66-7eab-419f-a9ee-85e480dcac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "                 transforms.RandomHorizontalFlip(),\n",
    "                 transforms.RandomCrop(32, 4),\n",
    "                 transforms.ToTensor(),\n",
    "                 normalize,\n",
    "                 ]), download=True),\n",
    "                 batch_size=128, shuffle=True,\n",
    "                 num_workers=4, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "               datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               normalize,\n",
    "               ])),\n",
    "               batch_size=128, shuffle=False,\n",
    "               num_workers=4, pin_memory=True)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f1b5126b-64fc-4851-9363-99652bf37466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv3 = nn.Conv2d(out_channel, self.expansion *\n",
    "                               out_channel, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channel != self.expansion*out_channel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion*out_channel,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "969b3dcd-949a-4e7e-a815-1e7fef511b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 128\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        # self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        # self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(256*block.expansion*4, num_classes)\n",
    "        # self.linear = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        # out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9464dcb0-a98a-4a0d-bd14-e4326c0816ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project1_model():\n",
    "    return ResNet(Bottleneck, [2, 3, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2f35d814-7e04-43fa-a6a0-1cc434c03789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 32, 32]           3,456\n",
      "       BatchNorm2d-2          [-1, 128, 32, 32]             256\n",
      "            Conv2d-3          [-1, 128, 32, 32]          16,384\n",
      "       BatchNorm2d-4          [-1, 128, 32, 32]             256\n",
      "            Conv2d-5          [-1, 128, 16, 16]         147,456\n",
      "       BatchNorm2d-6          [-1, 128, 16, 16]             256\n",
      "            Conv2d-7          [-1, 512, 16, 16]          65,536\n",
      "       BatchNorm2d-8          [-1, 512, 16, 16]           1,024\n",
      "            Conv2d-9          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-10          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-11          [-1, 512, 16, 16]               0\n",
      "           Conv2d-12          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-13          [-1, 128, 16, 16]             256\n",
      "           Conv2d-14          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-15          [-1, 128, 16, 16]             256\n",
      "           Conv2d-16          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-17          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-18          [-1, 512, 16, 16]               0\n",
      "           Conv2d-19          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-20          [-1, 128, 16, 16]             256\n",
      "           Conv2d-21          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-22          [-1, 128, 16, 16]             256\n",
      "           Conv2d-23          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-24          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-25          [-1, 512, 16, 16]               0\n",
      "           Conv2d-26          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-27          [-1, 256, 16, 16]             512\n",
      "           Conv2d-28            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-29            [-1, 256, 8, 8]             512\n",
      "           Conv2d-30           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-31           [-1, 1024, 8, 8]           2,048\n",
      "           Conv2d-32           [-1, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-33           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-34           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-35            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-36            [-1, 256, 8, 8]             512\n",
      "           Conv2d-37            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-38            [-1, 256, 8, 8]             512\n",
      "           Conv2d-39           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-40           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-41           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-42            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-43            [-1, 256, 8, 8]             512\n",
      "           Conv2d-44            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 8, 8]             512\n",
      "           Conv2d-46           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-47           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-48           [-1, 1024, 8, 8]               0\n",
      "           Linear-49                   [-1, 10]          40,970\n",
      "================================================================\n",
      "Total params: 4,649,098\n",
      "Trainable params: 4,649,098\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 25.25\n",
      "Params size (MB): 17.73\n",
      "Estimated Total Size (MB): 43.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = project1_model().to(device)\n",
    "summary(model, (3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d711ca1c-34bf-4249-9293-2bca3a2ec9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetParams:\n",
    "   \"\"\"\n",
    "    A class to pass the hyperparameters to the model\n",
    "   \"\"\"\n",
    "   def __init__(self, arch='Model 1' ,epochs=100, start_epoch=0, batch_size=128, lr=0.1, momentum=0.9, weight_decay=1e-4, print_freq=50,\n",
    "                save_dir='save_temporary_checkpoints', save_every=10):\n",
    "        self.save_every = save_every #Saves checkpoints at every specified number of epochs\n",
    "        self.save_dir = save_dir #The directory used to save the trained models\n",
    "        self.print_freq = print_freq #print frequency \n",
    "        self.weight_decay = weight_decay #Weight decay for SGD\n",
    "        self.momentum = momentum #Momentum for SGD\n",
    "        self.lr = lr #Learning Rate\n",
    "        self.batch_size = batch_size #Batch Size for each epoch \n",
    "        self.start_epoch = start_epoch #Starting Epoch\n",
    "        self.epochs = epochs #Total Epochs\n",
    "        self.arch = arch #ResNet model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fd2d70bc-b3c1-4687-bbf4-971a161b586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs():\n",
    "    global args, best_precision\n",
    "    #Check if the save_dir exists or not\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    #Loading the model \n",
    "    model = project1_model()\n",
    "    model.cuda()\n",
    "\n",
    "    #Defining the Loss Function\n",
    "    loss_func = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    #Defining the Optimizer\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "    #                             momentum=args.momentum,\n",
    "    #                             weight_decay=args.weight_decay)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), args.lr, betas=(0.9, 0.99))\n",
    "    \n",
    "    #Defining the Learning Rate Scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                        milestones=[100, 150], last_epoch=args.start_epoch - 1)\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        #Train for one epoch\n",
    "        print('Training model: {}'.format(args.arch))\n",
    "        print('Current Learning Rate {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
    "        train(train_loader, model, loss_func, optimizer, epoch)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        #Test for one epoch\n",
    "        precision = validate(val_loader, model, loss_func)\n",
    "\n",
    "        #Save the best precision and make a checkpoint\n",
    "        is_best = precision > best_precision\n",
    "        best_precision = max(precision, best_precision)\n",
    "        if epoch > 0 and epoch % args.save_every == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(args.save_dir, 'project1_model_checkpoint.th'))\n",
    "        if is_best:\n",
    "            torch.save(model.state_dict(), os.path.join(args.save_dir, 'project1_model.th'))\n",
    "    return best_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9c6ca600-7ed6-491a-bb1c-c80fc1446897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeepAverages(object):\n",
    "    #Computes and stores the average along with the current value\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "19f4ab67-10a8-4f19-a7b8-b7dddeca5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    #Computes the top 1 precision\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def validate(val_loader, model, loss_func):\n",
    "    #Run an Evaluation\n",
    "    batch_time = KeepAverages()\n",
    "    losses = KeepAverages()\n",
    "    top1 = KeepAverages()\n",
    "\n",
    "    #Switch to Evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            input_var = input.cuda()\n",
    "            target_var = target.cuda()\n",
    "\n",
    "            #Compute the output of the Model and calculate the Loss\n",
    "            output = model(input_var)\n",
    "            loss = loss_func(output, target_var)\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            #Measure the Loss and Update it \n",
    "            precision = accuracy(output.data, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(precision.item(), input.size(0))\n",
    "\n",
    "            #Measure the elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "\n",
    "    print('Test Accuracy\\t  Top Precision: {top1.avg:.3f} (Error: {error:.3f} )\\n'\n",
    "          .format(top1=top1,error=100-top1.avg))\n",
    "    val_losses.append(100-top1.avg)\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "648d79b4-acfe-4461-9592-6a8126bddf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, loss_func, optimizer, epoch):\n",
    "    #Run one training epoch\n",
    "\n",
    "    batch_time = KeepAverages()\n",
    "    data_time = KeepAverages()\n",
    "    losses = KeepAverages()\n",
    "    top1 = KeepAverages()\n",
    "\n",
    "    #Switch to Train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # Measure the data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        target = target.cuda()\n",
    "        input_var = input.cuda()\n",
    "        target_var = target\n",
    "\n",
    "        #Compute the output and the Loss\n",
    "        output = model(input_var)\n",
    "        loss = loss_func(output, target_var)\n",
    "\n",
    "        #Compute the Gradient and do an SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        \n",
    "        #Measure the accuracy and record the loss\n",
    "        precision = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(precision.item(), input.size(0))\n",
    "\n",
    "        #Measure the Elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: No: [{0}] Batches: [{1}/{2}]\\t'\n",
    "                  'Loss: {loss.val:.4f} (Average: {loss.avg:.4f})\\t'\n",
    "                  'Precision: {top1.val:.3f} (Average: {top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))\n",
    "    train_losses.append(100-top1.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7ebf3777-92f6-4855-baa8-0df7af075742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "  #Function to show an image\n",
    "  %matplotlib inline\n",
    "  %config InlineBackend.figure_format = 'retina'\n",
    "  img = img / 2 + 0.5     # un - Normalize\n",
    "  npimg = img.numpy()\n",
    "  plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "858ed804-c53a-478d-bb30-7bb9a7d397dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [0] Batches: [0/391]\tLoss: 2.5908 (Average: 2.5908)\tPrecision: 11.719 (Average: 11.719)\n",
      "Epoch: No: [0] Batches: [50/391]\tLoss: 2.2984 (Average: 22.2506)\tPrecision: 12.500 (Average: 10.218)\n",
      "Epoch: No: [0] Batches: [100/391]\tLoss: 2.3120 (Average: 12.3738)\tPrecision: 8.594 (Average: 11.146)\n",
      "Epoch: No: [0] Batches: [150/391]\tLoss: 2.2765 (Average: 9.0357)\tPrecision: 14.062 (Average: 11.320)\n",
      "Epoch: No: [0] Batches: [200/391]\tLoss: 2.1673 (Average: 7.3410)\tPrecision: 22.656 (Average: 12.722)\n",
      "Epoch: No: [0] Batches: [250/391]\tLoss: 2.1365 (Average: 6.3022)\tPrecision: 25.000 (Average: 14.380)\n",
      "Epoch: No: [0] Batches: [300/391]\tLoss: 2.0583 (Average: 5.5995)\tPrecision: 19.531 (Average: 15.664)\n",
      "Epoch: No: [0] Batches: [350/391]\tLoss: 2.0114 (Average: 5.0958)\tPrecision: 19.531 (Average: 16.624)\n",
      "Test Accuracy\t  Top Precision: 22.810 (Error: 77.190 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [1] Batches: [0/391]\tLoss: 2.0658 (Average: 2.0658)\tPrecision: 28.125 (Average: 28.125)\n",
      "Epoch: No: [1] Batches: [50/391]\tLoss: 2.0649 (Average: 2.0222)\tPrecision: 21.875 (Average: 25.276)\n",
      "Epoch: No: [1] Batches: [100/391]\tLoss: 2.0316 (Average: 2.0130)\tPrecision: 20.312 (Average: 25.116)\n",
      "Epoch: No: [1] Batches: [150/391]\tLoss: 1.7831 (Average: 1.9870)\tPrecision: 33.594 (Average: 25.786)\n",
      "Epoch: No: [1] Batches: [200/391]\tLoss: 1.9205 (Average: 1.9591)\tPrecision: 26.562 (Average: 26.679)\n",
      "Epoch: No: [1] Batches: [250/391]\tLoss: 1.8339 (Average: 1.9416)\tPrecision: 31.250 (Average: 27.076)\n",
      "Epoch: No: [1] Batches: [300/391]\tLoss: 1.8869 (Average: 1.9230)\tPrecision: 21.875 (Average: 27.541)\n",
      "Epoch: No: [1] Batches: [350/391]\tLoss: 1.8150 (Average: 1.9065)\tPrecision: 30.469 (Average: 27.871)\n",
      "Test Accuracy\t  Top Precision: 33.240 (Error: 66.760 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [2] Batches: [0/391]\tLoss: 1.8881 (Average: 1.8881)\tPrecision: 27.344 (Average: 27.344)\n",
      "Epoch: No: [2] Batches: [50/391]\tLoss: 1.8355 (Average: 1.8204)\tPrecision: 32.031 (Average: 30.224)\n",
      "Epoch: No: [2] Batches: [100/391]\tLoss: 1.7199 (Average: 1.8209)\tPrecision: 32.031 (Average: 30.685)\n",
      "Epoch: No: [2] Batches: [150/391]\tLoss: 1.8657 (Average: 1.8080)\tPrecision: 33.594 (Average: 30.696)\n",
      "Epoch: No: [2] Batches: [200/391]\tLoss: 1.7963 (Average: 1.8025)\tPrecision: 34.375 (Average: 31.087)\n",
      "Epoch: No: [2] Batches: [250/391]\tLoss: 1.7567 (Average: 1.7946)\tPrecision: 38.281 (Average: 31.446)\n",
      "Epoch: No: [2] Batches: [300/391]\tLoss: 1.6473 (Average: 1.7861)\tPrecision: 35.938 (Average: 31.704)\n",
      "Epoch: No: [2] Batches: [350/391]\tLoss: 1.6898 (Average: 1.7805)\tPrecision: 28.125 (Average: 31.853)\n",
      "Test Accuracy\t  Top Precision: 34.400 (Error: 65.600 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [3] Batches: [0/391]\tLoss: 1.8241 (Average: 1.8241)\tPrecision: 28.125 (Average: 28.125)\n",
      "Epoch: No: [3] Batches: [50/391]\tLoss: 1.5871 (Average: 1.7254)\tPrecision: 40.625 (Average: 34.222)\n",
      "Epoch: No: [3] Batches: [100/391]\tLoss: 1.6987 (Average: 1.7456)\tPrecision: 38.281 (Average: 33.772)\n",
      "Epoch: No: [3] Batches: [150/391]\tLoss: 1.8777 (Average: 1.7346)\tPrecision: 25.000 (Average: 34.261)\n",
      "Epoch: No: [3] Batches: [200/391]\tLoss: 1.8089 (Average: 1.7297)\tPrecision: 29.688 (Average: 34.336)\n",
      "Epoch: No: [3] Batches: [250/391]\tLoss: 1.7703 (Average: 1.7271)\tPrecision: 35.938 (Average: 34.229)\n",
      "Epoch: No: [3] Batches: [300/391]\tLoss: 1.6064 (Average: 1.7237)\tPrecision: 44.531 (Average: 34.455)\n",
      "Epoch: No: [3] Batches: [350/391]\tLoss: 1.7282 (Average: 1.7171)\tPrecision: 36.719 (Average: 34.798)\n",
      "Test Accuracy\t  Top Precision: 37.450 (Error: 62.550 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [4] Batches: [0/391]\tLoss: 1.5799 (Average: 1.5799)\tPrecision: 42.188 (Average: 42.188)\n",
      "Epoch: No: [4] Batches: [50/391]\tLoss: 1.6621 (Average: 1.6726)\tPrecision: 32.812 (Average: 36.137)\n",
      "Epoch: No: [4] Batches: [100/391]\tLoss: 1.6342 (Average: 1.6836)\tPrecision: 37.500 (Average: 36.054)\n",
      "Epoch: No: [4] Batches: [150/391]\tLoss: 1.5896 (Average: 1.6741)\tPrecision: 39.844 (Average: 36.243)\n",
      "Epoch: No: [4] Batches: [200/391]\tLoss: 1.8013 (Average: 1.6678)\tPrecision: 33.594 (Average: 36.645)\n",
      "Epoch: No: [4] Batches: [250/391]\tLoss: 1.5688 (Average: 1.6577)\tPrecision: 38.281 (Average: 37.080)\n",
      "Epoch: No: [4] Batches: [300/391]\tLoss: 1.6378 (Average: 1.6572)\tPrecision: 41.406 (Average: 37.118)\n",
      "Epoch: No: [4] Batches: [350/391]\tLoss: 1.5204 (Average: 1.6500)\tPrecision: 42.969 (Average: 37.493)\n",
      "Test Accuracy\t  Top Precision: 41.840 (Error: 58.160 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [5] Batches: [0/391]\tLoss: 1.5221 (Average: 1.5221)\tPrecision: 43.750 (Average: 43.750)\n",
      "Epoch: No: [5] Batches: [50/391]\tLoss: 1.4487 (Average: 1.6044)\tPrecision: 50.000 (Average: 39.645)\n",
      "Epoch: No: [5] Batches: [100/391]\tLoss: 1.7171 (Average: 1.6147)\tPrecision: 35.938 (Average: 39.380)\n",
      "Epoch: No: [5] Batches: [150/391]\tLoss: 1.6867 (Average: 1.6052)\tPrecision: 39.844 (Average: 39.533)\n",
      "Epoch: No: [5] Batches: [200/391]\tLoss: 1.5190 (Average: 1.5954)\tPrecision: 50.781 (Average: 39.848)\n",
      "Epoch: No: [5] Batches: [250/391]\tLoss: 1.4629 (Average: 1.5895)\tPrecision: 42.188 (Average: 40.283)\n",
      "Epoch: No: [5] Batches: [300/391]\tLoss: 1.7113 (Average: 1.5814)\tPrecision: 39.844 (Average: 40.734)\n",
      "Epoch: No: [5] Batches: [350/391]\tLoss: 1.3831 (Average: 1.5720)\tPrecision: 46.875 (Average: 41.090)\n",
      "Test Accuracy\t  Top Precision: 42.310 (Error: 57.690 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [6] Batches: [0/391]\tLoss: 1.4343 (Average: 1.4343)\tPrecision: 46.875 (Average: 46.875)\n",
      "Epoch: No: [6] Batches: [50/391]\tLoss: 1.4720 (Average: 1.5395)\tPrecision: 45.312 (Average: 42.678)\n",
      "Epoch: No: [6] Batches: [100/391]\tLoss: 1.4371 (Average: 1.5291)\tPrecision: 45.312 (Average: 42.327)\n",
      "Epoch: No: [6] Batches: [150/391]\tLoss: 1.2990 (Average: 1.5075)\tPrecision: 50.000 (Average: 43.409)\n",
      "Epoch: No: [6] Batches: [200/391]\tLoss: 1.2654 (Average: 1.4961)\tPrecision: 48.438 (Average: 43.699)\n",
      "Epoch: No: [6] Batches: [250/391]\tLoss: 1.6010 (Average: 1.4900)\tPrecision: 39.062 (Average: 44.145)\n",
      "Epoch: No: [6] Batches: [300/391]\tLoss: 1.4815 (Average: 1.4805)\tPrecision: 47.656 (Average: 44.539)\n",
      "Epoch: No: [6] Batches: [350/391]\tLoss: 1.3137 (Average: 1.4726)\tPrecision: 48.438 (Average: 44.794)\n",
      "Test Accuracy\t  Top Precision: 46.920 (Error: 53.080 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [7] Batches: [0/391]\tLoss: 1.4760 (Average: 1.4760)\tPrecision: 44.531 (Average: 44.531)\n",
      "Epoch: No: [7] Batches: [50/391]\tLoss: 1.6330 (Average: 1.4123)\tPrecision: 45.312 (Average: 47.595)\n",
      "Epoch: No: [7] Batches: [100/391]\tLoss: 1.4786 (Average: 1.4224)\tPrecision: 50.000 (Average: 47.277)\n",
      "Epoch: No: [7] Batches: [150/391]\tLoss: 1.4007 (Average: 1.4186)\tPrecision: 50.000 (Average: 47.341)\n",
      "Epoch: No: [7] Batches: [200/391]\tLoss: 1.4970 (Average: 1.4141)\tPrecision: 47.656 (Average: 47.544)\n",
      "Epoch: No: [7] Batches: [250/391]\tLoss: 1.3884 (Average: 1.4100)\tPrecision: 48.438 (Average: 47.697)\n",
      "Epoch: No: [7] Batches: [300/391]\tLoss: 1.4707 (Average: 1.4056)\tPrecision: 46.094 (Average: 47.848)\n",
      "Epoch: No: [7] Batches: [350/391]\tLoss: 1.3111 (Average: 1.3998)\tPrecision: 46.875 (Average: 48.159)\n",
      "Test Accuracy\t  Top Precision: 45.350 (Error: 54.650 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [8] Batches: [0/391]\tLoss: 1.4154 (Average: 1.4154)\tPrecision: 52.344 (Average: 52.344)\n",
      "Epoch: No: [8] Batches: [50/391]\tLoss: 1.2445 (Average: 1.3518)\tPrecision: 48.438 (Average: 50.199)\n",
      "Epoch: No: [8] Batches: [100/391]\tLoss: 1.5483 (Average: 1.3461)\tPrecision: 49.219 (Average: 50.541)\n",
      "Epoch: No: [8] Batches: [150/391]\tLoss: 1.2874 (Average: 1.3380)\tPrecision: 48.438 (Average: 50.528)\n",
      "Epoch: No: [8] Batches: [200/391]\tLoss: 1.3335 (Average: 1.3295)\tPrecision: 47.656 (Average: 50.637)\n",
      "Epoch: No: [8] Batches: [250/391]\tLoss: 1.2740 (Average: 1.3300)\tPrecision: 53.125 (Average: 50.766)\n",
      "Epoch: No: [8] Batches: [300/391]\tLoss: 1.3162 (Average: 1.3279)\tPrecision: 52.344 (Average: 50.921)\n",
      "Epoch: No: [8] Batches: [350/391]\tLoss: 1.2732 (Average: 1.3218)\tPrecision: 49.219 (Average: 51.077)\n",
      "Test Accuracy\t  Top Precision: 52.530 (Error: 47.470 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [9] Batches: [0/391]\tLoss: 1.3803 (Average: 1.3803)\tPrecision: 51.562 (Average: 51.562)\n",
      "Epoch: No: [9] Batches: [50/391]\tLoss: 1.2474 (Average: 1.3014)\tPrecision: 55.469 (Average: 52.191)\n",
      "Epoch: No: [9] Batches: [100/391]\tLoss: 1.3481 (Average: 1.3022)\tPrecision: 51.562 (Average: 52.073)\n",
      "Epoch: No: [9] Batches: [150/391]\tLoss: 1.1401 (Average: 1.2900)\tPrecision: 64.844 (Average: 52.478)\n",
      "Epoch: No: [9] Batches: [200/391]\tLoss: 1.3836 (Average: 1.2819)\tPrecision: 48.438 (Average: 52.744)\n",
      "Epoch: No: [9] Batches: [250/391]\tLoss: 1.1191 (Average: 1.2756)\tPrecision: 60.156 (Average: 52.960)\n",
      "Epoch: No: [9] Batches: [300/391]\tLoss: 1.4629 (Average: 1.2743)\tPrecision: 48.438 (Average: 53.042)\n",
      "Epoch: No: [9] Batches: [350/391]\tLoss: 1.3572 (Average: 1.2747)\tPrecision: 49.219 (Average: 53.054)\n",
      "Test Accuracy\t  Top Precision: 55.710 (Error: 44.290 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [10] Batches: [0/391]\tLoss: 1.3040 (Average: 1.3040)\tPrecision: 49.219 (Average: 49.219)\n",
      "Epoch: No: [10] Batches: [50/391]\tLoss: 1.0720 (Average: 1.2346)\tPrecision: 57.031 (Average: 54.841)\n",
      "Epoch: No: [10] Batches: [100/391]\tLoss: 1.0475 (Average: 1.2349)\tPrecision: 65.625 (Average: 55.190)\n",
      "Epoch: No: [10] Batches: [150/391]\tLoss: 1.1522 (Average: 1.2376)\tPrecision: 54.688 (Average: 54.972)\n",
      "Epoch: No: [10] Batches: [200/391]\tLoss: 1.0521 (Average: 1.2338)\tPrecision: 60.156 (Average: 54.882)\n",
      "Epoch: No: [10] Batches: [250/391]\tLoss: 1.1761 (Average: 1.2300)\tPrecision: 62.500 (Average: 55.036)\n",
      "Epoch: No: [10] Batches: [300/391]\tLoss: 1.2136 (Average: 1.2241)\tPrecision: 53.125 (Average: 55.344)\n",
      "Epoch: No: [10] Batches: [350/391]\tLoss: 1.5227 (Average: 1.2213)\tPrecision: 46.875 (Average: 55.480)\n",
      "Test Accuracy\t  Top Precision: 57.830 (Error: 42.170 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [11] Batches: [0/391]\tLoss: 1.1063 (Average: 1.1063)\tPrecision: 65.625 (Average: 65.625)\n",
      "Epoch: No: [11] Batches: [50/391]\tLoss: 1.3841 (Average: 1.1735)\tPrecision: 51.562 (Average: 56.694)\n",
      "Epoch: No: [11] Batches: [100/391]\tLoss: 1.1865 (Average: 1.1699)\tPrecision: 55.469 (Average: 56.993)\n",
      "Epoch: No: [11] Batches: [150/391]\tLoss: 1.0767 (Average: 1.1649)\tPrecision: 60.938 (Average: 57.176)\n",
      "Epoch: No: [11] Batches: [200/391]\tLoss: 1.1777 (Average: 1.1702)\tPrecision: 57.812 (Average: 57.136)\n",
      "Epoch: No: [11] Batches: [250/391]\tLoss: 1.1716 (Average: 1.1714)\tPrecision: 55.469 (Average: 57.224)\n",
      "Epoch: No: [11] Batches: [300/391]\tLoss: 0.9796 (Average: 1.1667)\tPrecision: 65.625 (Average: 57.550)\n",
      "Epoch: No: [11] Batches: [350/391]\tLoss: 1.2492 (Average: 1.1690)\tPrecision: 51.562 (Average: 57.401)\n",
      "Test Accuracy\t  Top Precision: 58.840 (Error: 41.160 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [12] Batches: [0/391]\tLoss: 1.2018 (Average: 1.2018)\tPrecision: 58.594 (Average: 58.594)\n",
      "Epoch: No: [12] Batches: [50/391]\tLoss: 1.0802 (Average: 1.1383)\tPrecision: 62.500 (Average: 58.946)\n",
      "Epoch: No: [12] Batches: [100/391]\tLoss: 1.0649 (Average: 1.1291)\tPrecision: 64.844 (Average: 59.035)\n",
      "Epoch: No: [12] Batches: [150/391]\tLoss: 1.0457 (Average: 1.1255)\tPrecision: 57.031 (Average: 59.059)\n",
      "Epoch: No: [12] Batches: [200/391]\tLoss: 1.1906 (Average: 1.1266)\tPrecision: 59.375 (Average: 59.060)\n",
      "Epoch: No: [12] Batches: [250/391]\tLoss: 1.1350 (Average: 1.1283)\tPrecision: 60.938 (Average: 58.886)\n",
      "Epoch: No: [12] Batches: [300/391]\tLoss: 0.9119 (Average: 1.1253)\tPrecision: 69.531 (Average: 59.077)\n",
      "Epoch: No: [12] Batches: [350/391]\tLoss: 1.2266 (Average: 1.1248)\tPrecision: 59.375 (Average: 59.092)\n",
      "Test Accuracy\t  Top Precision: 56.620 (Error: 43.380 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [13] Batches: [0/391]\tLoss: 1.2513 (Average: 1.2513)\tPrecision: 51.562 (Average: 51.562)\n",
      "Epoch: No: [13] Batches: [50/391]\tLoss: 1.2118 (Average: 1.1091)\tPrecision: 54.688 (Average: 59.727)\n",
      "Epoch: No: [13] Batches: [100/391]\tLoss: 0.9440 (Average: 1.0934)\tPrecision: 64.844 (Average: 60.388)\n",
      "Epoch: No: [13] Batches: [150/391]\tLoss: 1.0208 (Average: 1.0823)\tPrecision: 62.500 (Average: 60.865)\n",
      "Epoch: No: [13] Batches: [200/391]\tLoss: 0.9972 (Average: 1.0837)\tPrecision: 64.844 (Average: 60.782)\n",
      "Epoch: No: [13] Batches: [250/391]\tLoss: 1.0513 (Average: 1.0807)\tPrecision: 64.062 (Average: 60.962)\n",
      "Epoch: No: [13] Batches: [300/391]\tLoss: 0.8992 (Average: 1.0758)\tPrecision: 64.844 (Average: 61.093)\n",
      "Epoch: No: [13] Batches: [350/391]\tLoss: 0.9096 (Average: 1.0747)\tPrecision: 63.281 (Average: 61.104)\n",
      "Test Accuracy\t  Top Precision: 60.870 (Error: 39.130 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [14] Batches: [0/391]\tLoss: 1.1653 (Average: 1.1653)\tPrecision: 62.500 (Average: 62.500)\n",
      "Epoch: No: [14] Batches: [50/391]\tLoss: 1.1578 (Average: 1.0396)\tPrecision: 56.250 (Average: 62.944)\n",
      "Epoch: No: [14] Batches: [100/391]\tLoss: 1.0614 (Average: 1.0405)\tPrecision: 60.938 (Average: 62.717)\n",
      "Epoch: No: [14] Batches: [150/391]\tLoss: 1.0143 (Average: 1.0504)\tPrecision: 58.594 (Average: 62.516)\n",
      "Epoch: No: [14] Batches: [200/391]\tLoss: 0.9777 (Average: 1.0462)\tPrecision: 63.281 (Average: 62.473)\n",
      "Epoch: No: [14] Batches: [250/391]\tLoss: 0.9272 (Average: 1.0495)\tPrecision: 64.062 (Average: 62.313)\n",
      "Epoch: No: [14] Batches: [300/391]\tLoss: 1.0965 (Average: 1.0524)\tPrecision: 60.938 (Average: 62.220)\n",
      "Epoch: No: [14] Batches: [350/391]\tLoss: 1.0249 (Average: 1.0510)\tPrecision: 61.719 (Average: 62.200)\n",
      "Test Accuracy\t  Top Precision: 65.290 (Error: 34.710 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [15] Batches: [0/391]\tLoss: 1.0908 (Average: 1.0908)\tPrecision: 60.938 (Average: 60.938)\n",
      "Epoch: No: [15] Batches: [50/391]\tLoss: 1.0554 (Average: 1.0069)\tPrecision: 62.500 (Average: 64.108)\n",
      "Epoch: No: [15] Batches: [100/391]\tLoss: 0.9260 (Average: 1.0135)\tPrecision: 68.750 (Average: 63.219)\n",
      "Epoch: No: [15] Batches: [150/391]\tLoss: 1.0874 (Average: 1.0135)\tPrecision: 56.250 (Average: 63.369)\n",
      "Epoch: No: [15] Batches: [200/391]\tLoss: 0.9031 (Average: 1.0122)\tPrecision: 61.719 (Average: 63.390)\n",
      "Epoch: No: [15] Batches: [250/391]\tLoss: 0.8012 (Average: 1.0076)\tPrecision: 68.750 (Average: 63.611)\n",
      "Epoch: No: [15] Batches: [300/391]\tLoss: 1.0097 (Average: 1.0067)\tPrecision: 66.406 (Average: 63.782)\n",
      "Epoch: No: [15] Batches: [350/391]\tLoss: 0.8407 (Average: 1.0088)\tPrecision: 71.875 (Average: 63.640)\n",
      "Test Accuracy\t  Top Precision: 64.090 (Error: 35.910 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [16] Batches: [0/391]\tLoss: 1.1482 (Average: 1.1482)\tPrecision: 65.625 (Average: 65.625)\n",
      "Epoch: No: [16] Batches: [50/391]\tLoss: 0.9837 (Average: 1.0067)\tPrecision: 63.281 (Average: 63.802)\n",
      "Epoch: No: [16] Batches: [100/391]\tLoss: 1.0222 (Average: 1.0000)\tPrecision: 67.969 (Average: 64.124)\n",
      "Epoch: No: [16] Batches: [150/391]\tLoss: 1.0421 (Average: 0.9900)\tPrecision: 64.844 (Average: 64.678)\n",
      "Epoch: No: [16] Batches: [200/391]\tLoss: 0.9638 (Average: 0.9892)\tPrecision: 62.500 (Average: 64.638)\n",
      "Epoch: No: [16] Batches: [250/391]\tLoss: 1.0217 (Average: 0.9884)\tPrecision: 66.406 (Average: 64.732)\n",
      "Epoch: No: [16] Batches: [300/391]\tLoss: 1.0724 (Average: 0.9904)\tPrecision: 65.625 (Average: 64.678)\n",
      "Epoch: No: [16] Batches: [350/391]\tLoss: 1.0626 (Average: 0.9858)\tPrecision: 65.625 (Average: 64.895)\n",
      "Test Accuracy\t  Top Precision: 61.780 (Error: 38.220 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [17] Batches: [0/391]\tLoss: 1.0044 (Average: 1.0044)\tPrecision: 63.281 (Average: 63.281)\n",
      "Epoch: No: [17] Batches: [50/391]\tLoss: 0.8885 (Average: 0.9525)\tPrecision: 69.531 (Average: 66.437)\n",
      "Epoch: No: [17] Batches: [100/391]\tLoss: 0.8490 (Average: 0.9437)\tPrecision: 70.312 (Average: 66.723)\n",
      "Epoch: No: [17] Batches: [150/391]\tLoss: 1.0392 (Average: 0.9579)\tPrecision: 58.594 (Average: 65.987)\n",
      "Epoch: No: [17] Batches: [200/391]\tLoss: 0.9186 (Average: 0.9584)\tPrecision: 68.750 (Average: 66.084)\n",
      "Epoch: No: [17] Batches: [250/391]\tLoss: 0.8386 (Average: 0.9553)\tPrecision: 72.656 (Average: 66.173)\n",
      "Epoch: No: [17] Batches: [300/391]\tLoss: 0.9316 (Average: 0.9565)\tPrecision: 67.188 (Average: 66.084)\n",
      "Epoch: No: [17] Batches: [350/391]\tLoss: 0.8894 (Average: 0.9545)\tPrecision: 67.188 (Average: 66.104)\n",
      "Test Accuracy\t  Top Precision: 59.540 (Error: 40.460 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [18] Batches: [0/391]\tLoss: 1.0217 (Average: 1.0217)\tPrecision: 65.625 (Average: 65.625)\n",
      "Epoch: No: [18] Batches: [50/391]\tLoss: 1.0862 (Average: 0.9643)\tPrecision: 57.812 (Average: 65.303)\n",
      "Epoch: No: [18] Batches: [100/391]\tLoss: 0.9058 (Average: 0.9421)\tPrecision: 71.875 (Average: 66.282)\n",
      "Epoch: No: [18] Batches: [150/391]\tLoss: 0.9051 (Average: 0.9380)\tPrecision: 68.750 (Average: 66.479)\n",
      "Epoch: No: [18] Batches: [200/391]\tLoss: 0.9587 (Average: 0.9331)\tPrecision: 65.625 (Average: 66.511)\n",
      "Epoch: No: [18] Batches: [250/391]\tLoss: 1.0022 (Average: 0.9350)\tPrecision: 60.938 (Average: 66.394)\n",
      "Epoch: No: [18] Batches: [300/391]\tLoss: 0.7875 (Average: 0.9356)\tPrecision: 71.875 (Average: 66.533)\n",
      "Epoch: No: [18] Batches: [350/391]\tLoss: 0.9781 (Average: 0.9343)\tPrecision: 65.625 (Average: 66.513)\n",
      "Test Accuracy\t  Top Precision: 62.710 (Error: 37.290 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [19] Batches: [0/391]\tLoss: 0.7140 (Average: 0.7140)\tPrecision: 72.656 (Average: 72.656)\n",
      "Epoch: No: [19] Batches: [50/391]\tLoss: 0.8104 (Average: 0.9107)\tPrecision: 73.438 (Average: 67.111)\n",
      "Epoch: No: [19] Batches: [100/391]\tLoss: 0.9985 (Average: 0.9249)\tPrecision: 60.938 (Average: 66.917)\n",
      "Epoch: No: [19] Batches: [150/391]\tLoss: 0.9631 (Average: 0.9111)\tPrecision: 65.625 (Average: 67.255)\n",
      "Epoch: No: [19] Batches: [200/391]\tLoss: 0.9808 (Average: 0.9121)\tPrecision: 66.406 (Average: 67.254)\n",
      "Epoch: No: [19] Batches: [250/391]\tLoss: 0.9903 (Average: 0.9097)\tPrecision: 62.500 (Average: 67.250)\n",
      "Epoch: No: [19] Batches: [300/391]\tLoss: 0.7352 (Average: 0.9133)\tPrecision: 69.531 (Average: 67.091)\n",
      "Epoch: No: [19] Batches: [350/391]\tLoss: 0.8156 (Average: 0.9168)\tPrecision: 77.344 (Average: 67.136)\n",
      "Test Accuracy\t  Top Precision: 66.780 (Error: 33.220 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [20] Batches: [0/391]\tLoss: 0.9641 (Average: 0.9641)\tPrecision: 71.094 (Average: 71.094)\n",
      "Epoch: No: [20] Batches: [50/391]\tLoss: 0.8981 (Average: 0.8860)\tPrecision: 67.969 (Average: 68.857)\n",
      "Epoch: No: [20] Batches: [100/391]\tLoss: 0.7902 (Average: 0.8827)\tPrecision: 72.656 (Average: 68.897)\n",
      "Epoch: No: [20] Batches: [150/391]\tLoss: 0.8649 (Average: 0.8903)\tPrecision: 71.875 (Average: 68.522)\n",
      "Epoch: No: [20] Batches: [200/391]\tLoss: 0.9533 (Average: 0.8829)\tPrecision: 67.969 (Average: 68.661)\n",
      "Epoch: No: [20] Batches: [250/391]\tLoss: 1.0278 (Average: 0.8862)\tPrecision: 70.312 (Average: 68.616)\n",
      "Epoch: No: [20] Batches: [300/391]\tLoss: 0.8042 (Average: 0.8883)\tPrecision: 74.219 (Average: 68.449)\n",
      "Epoch: No: [20] Batches: [350/391]\tLoss: 0.7846 (Average: 0.8838)\tPrecision: 70.312 (Average: 68.532)\n",
      "Test Accuracy\t  Top Precision: 67.850 (Error: 32.150 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [21] Batches: [0/391]\tLoss: 0.8473 (Average: 0.8473)\tPrecision: 72.656 (Average: 72.656)\n",
      "Epoch: No: [21] Batches: [50/391]\tLoss: 0.7832 (Average: 0.8790)\tPrecision: 67.188 (Average: 68.949)\n",
      "Epoch: No: [21] Batches: [100/391]\tLoss: 0.9237 (Average: 0.8573)\tPrecision: 69.531 (Average: 69.570)\n",
      "Epoch: No: [21] Batches: [150/391]\tLoss: 0.7158 (Average: 0.8550)\tPrecision: 75.781 (Average: 69.878)\n",
      "Epoch: No: [21] Batches: [200/391]\tLoss: 0.8424 (Average: 0.8564)\tPrecision: 67.969 (Average: 69.675)\n",
      "Epoch: No: [21] Batches: [250/391]\tLoss: 0.9031 (Average: 0.8553)\tPrecision: 68.750 (Average: 69.609)\n",
      "Epoch: No: [21] Batches: [300/391]\tLoss: 0.7799 (Average: 0.8567)\tPrecision: 73.438 (Average: 69.536)\n",
      "Epoch: No: [21] Batches: [350/391]\tLoss: 0.9637 (Average: 0.8629)\tPrecision: 63.281 (Average: 69.280)\n",
      "Test Accuracy\t  Top Precision: 69.040 (Error: 30.960 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [22] Batches: [0/391]\tLoss: 0.8497 (Average: 0.8497)\tPrecision: 70.312 (Average: 70.312)\n",
      "Epoch: No: [22] Batches: [50/391]\tLoss: 0.7653 (Average: 0.8428)\tPrecision: 69.531 (Average: 69.378)\n",
      "Epoch: No: [22] Batches: [100/391]\tLoss: 0.8996 (Average: 0.8677)\tPrecision: 67.188 (Average: 68.912)\n",
      "Epoch: No: [22] Batches: [150/391]\tLoss: 0.8478 (Average: 0.8667)\tPrecision: 67.969 (Average: 68.869)\n",
      "Epoch: No: [22] Batches: [200/391]\tLoss: 0.9237 (Average: 0.8621)\tPrecision: 67.969 (Average: 69.321)\n",
      "Epoch: No: [22] Batches: [250/391]\tLoss: 0.9182 (Average: 0.8561)\tPrecision: 64.844 (Average: 69.628)\n",
      "Epoch: No: [22] Batches: [300/391]\tLoss: 0.9438 (Average: 0.8571)\tPrecision: 67.188 (Average: 69.407)\n",
      "Epoch: No: [22] Batches: [350/391]\tLoss: 0.7075 (Average: 0.8576)\tPrecision: 73.438 (Average: 69.355)\n",
      "Test Accuracy\t  Top Precision: 63.360 (Error: 36.640 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [23] Batches: [0/391]\tLoss: 0.8184 (Average: 0.8184)\tPrecision: 71.094 (Average: 71.094)\n",
      "Epoch: No: [23] Batches: [50/391]\tLoss: 0.7633 (Average: 0.8274)\tPrecision: 76.562 (Average: 70.450)\n",
      "Epoch: No: [23] Batches: [100/391]\tLoss: 0.9529 (Average: 0.8331)\tPrecision: 67.188 (Average: 70.173)\n",
      "Epoch: No: [23] Batches: [150/391]\tLoss: 0.7772 (Average: 0.8358)\tPrecision: 75.781 (Average: 70.266)\n",
      "Epoch: No: [23] Batches: [200/391]\tLoss: 0.8201 (Average: 0.8320)\tPrecision: 70.312 (Average: 70.340)\n",
      "Epoch: No: [23] Batches: [250/391]\tLoss: 0.9444 (Average: 0.8353)\tPrecision: 70.312 (Average: 70.278)\n",
      "Epoch: No: [23] Batches: [300/391]\tLoss: 0.7372 (Average: 0.8336)\tPrecision: 75.000 (Average: 70.359)\n",
      "Epoch: No: [23] Batches: [350/391]\tLoss: 0.7745 (Average: 0.8306)\tPrecision: 70.312 (Average: 70.477)\n",
      "Test Accuracy\t  Top Precision: 59.970 (Error: 40.030 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [24] Batches: [0/391]\tLoss: 0.9011 (Average: 0.9011)\tPrecision: 64.062 (Average: 64.062)\n",
      "Epoch: No: [24] Batches: [50/391]\tLoss: 0.9719 (Average: 0.8669)\tPrecision: 65.625 (Average: 69.164)\n",
      "Epoch: No: [24] Batches: [100/391]\tLoss: 0.7968 (Average: 0.8511)\tPrecision: 70.312 (Average: 69.841)\n",
      "Epoch: No: [24] Batches: [150/391]\tLoss: 0.8567 (Average: 0.8451)\tPrecision: 68.750 (Average: 70.090)\n",
      "Epoch: No: [24] Batches: [200/391]\tLoss: 0.8499 (Average: 0.8415)\tPrecision: 70.312 (Average: 70.087)\n",
      "Epoch: No: [24] Batches: [250/391]\tLoss: 0.8388 (Average: 0.8313)\tPrecision: 69.531 (Average: 70.456)\n",
      "Epoch: No: [24] Batches: [300/391]\tLoss: 0.8909 (Average: 0.8289)\tPrecision: 69.531 (Average: 70.590)\n",
      "Epoch: No: [24] Batches: [350/391]\tLoss: 0.7546 (Average: 0.8235)\tPrecision: 72.656 (Average: 70.829)\n",
      "Test Accuracy\t  Top Precision: 69.760 (Error: 30.240 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [25] Batches: [0/391]\tLoss: 0.6898 (Average: 0.6898)\tPrecision: 75.000 (Average: 75.000)\n",
      "Epoch: No: [25] Batches: [50/391]\tLoss: 0.6366 (Average: 0.8269)\tPrecision: 75.000 (Average: 70.052)\n",
      "Epoch: No: [25] Batches: [100/391]\tLoss: 0.8878 (Average: 0.8257)\tPrecision: 69.531 (Average: 70.630)\n",
      "Epoch: No: [25] Batches: [150/391]\tLoss: 0.7754 (Average: 0.8243)\tPrecision: 72.656 (Average: 70.711)\n",
      "Epoch: No: [25] Batches: [200/391]\tLoss: 0.8237 (Average: 0.8229)\tPrecision: 66.406 (Average: 70.623)\n",
      "Epoch: No: [25] Batches: [250/391]\tLoss: 1.0418 (Average: 0.8159)\tPrecision: 68.750 (Average: 70.708)\n",
      "Epoch: No: [25] Batches: [300/391]\tLoss: 0.9259 (Average: 0.8124)\tPrecision: 68.750 (Average: 70.889)\n",
      "Epoch: No: [25] Batches: [350/391]\tLoss: 1.0320 (Average: 0.8036)\tPrecision: 66.406 (Average: 71.232)\n",
      "Test Accuracy\t  Top Precision: 68.550 (Error: 31.450 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [26] Batches: [0/391]\tLoss: 0.8201 (Average: 0.8201)\tPrecision: 75.000 (Average: 75.000)\n",
      "Epoch: No: [26] Batches: [50/391]\tLoss: 0.8527 (Average: 0.7764)\tPrecision: 66.406 (Average: 72.258)\n",
      "Epoch: No: [26] Batches: [100/391]\tLoss: 0.7721 (Average: 0.7903)\tPrecision: 69.531 (Average: 71.651)\n",
      "Epoch: No: [26] Batches: [150/391]\tLoss: 0.9465 (Average: 0.7846)\tPrecision: 68.750 (Average: 71.870)\n",
      "Epoch: No: [26] Batches: [200/391]\tLoss: 0.9106 (Average: 0.7857)\tPrecision: 71.875 (Average: 71.859)\n",
      "Epoch: No: [26] Batches: [250/391]\tLoss: 0.8834 (Average: 0.7879)\tPrecision: 69.531 (Average: 71.831)\n",
      "Epoch: No: [26] Batches: [300/391]\tLoss: 0.9534 (Average: 0.7936)\tPrecision: 67.188 (Average: 71.662)\n",
      "Epoch: No: [26] Batches: [350/391]\tLoss: 0.7201 (Average: 0.7940)\tPrecision: 73.438 (Average: 71.670)\n",
      "Test Accuracy\t  Top Precision: 72.060 (Error: 27.940 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [27] Batches: [0/391]\tLoss: 0.8403 (Average: 0.8403)\tPrecision: 68.750 (Average: 68.750)\n",
      "Epoch: No: [27] Batches: [50/391]\tLoss: 0.8923 (Average: 0.7469)\tPrecision: 69.531 (Average: 73.683)\n",
      "Epoch: No: [27] Batches: [100/391]\tLoss: 0.6586 (Average: 0.7589)\tPrecision: 77.344 (Average: 73.213)\n",
      "Epoch: No: [27] Batches: [150/391]\tLoss: 0.6550 (Average: 0.7640)\tPrecision: 78.125 (Average: 73.086)\n",
      "Epoch: No: [27] Batches: [200/391]\tLoss: 0.8365 (Average: 0.7771)\tPrecision: 65.625 (Average: 72.750)\n",
      "Epoch: No: [27] Batches: [250/391]\tLoss: 0.6538 (Average: 0.7793)\tPrecision: 75.781 (Average: 72.578)\n",
      "Epoch: No: [27] Batches: [300/391]\tLoss: 0.7141 (Average: 0.7783)\tPrecision: 70.312 (Average: 72.599)\n",
      "Epoch: No: [27] Batches: [350/391]\tLoss: 0.7385 (Average: 0.7753)\tPrecision: 74.219 (Average: 72.712)\n",
      "Test Accuracy\t  Top Precision: 72.650 (Error: 27.350 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [28] Batches: [0/391]\tLoss: 0.5557 (Average: 0.5557)\tPrecision: 81.250 (Average: 81.250)\n",
      "Epoch: No: [28] Batches: [50/391]\tLoss: 0.6352 (Average: 0.7528)\tPrecision: 77.344 (Average: 72.626)\n",
      "Epoch: No: [28] Batches: [100/391]\tLoss: 0.7591 (Average: 0.7657)\tPrecision: 70.312 (Average: 72.540)\n",
      "Epoch: No: [28] Batches: [150/391]\tLoss: 0.8785 (Average: 0.7600)\tPrecision: 68.750 (Average: 72.987)\n",
      "Epoch: No: [28] Batches: [200/391]\tLoss: 0.8894 (Average: 0.7633)\tPrecision: 67.188 (Average: 72.901)\n",
      "Epoch: No: [28] Batches: [250/391]\tLoss: 0.8428 (Average: 0.7624)\tPrecision: 67.969 (Average: 73.011)\n",
      "Epoch: No: [28] Batches: [300/391]\tLoss: 0.6859 (Average: 0.7632)\tPrecision: 71.875 (Average: 72.981)\n",
      "Epoch: No: [28] Batches: [350/391]\tLoss: 0.8108 (Average: 0.7645)\tPrecision: 67.969 (Average: 72.888)\n",
      "Test Accuracy\t  Top Precision: 72.100 (Error: 27.900 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [29] Batches: [0/391]\tLoss: 0.4929 (Average: 0.4929)\tPrecision: 84.375 (Average: 84.375)\n",
      "Epoch: No: [29] Batches: [50/391]\tLoss: 0.6952 (Average: 0.7325)\tPrecision: 75.781 (Average: 73.882)\n",
      "Epoch: No: [29] Batches: [100/391]\tLoss: 0.6248 (Average: 0.7393)\tPrecision: 76.562 (Average: 73.646)\n",
      "Epoch: No: [29] Batches: [150/391]\tLoss: 0.7967 (Average: 0.7432)\tPrecision: 67.969 (Average: 73.665)\n",
      "Epoch: No: [29] Batches: [200/391]\tLoss: 0.7225 (Average: 0.7496)\tPrecision: 73.438 (Average: 73.469)\n",
      "Epoch: No: [29] Batches: [250/391]\tLoss: 0.7422 (Average: 0.7512)\tPrecision: 71.875 (Average: 73.428)\n",
      "Epoch: No: [29] Batches: [300/391]\tLoss: 0.7965 (Average: 0.7558)\tPrecision: 70.312 (Average: 73.253)\n",
      "Epoch: No: [29] Batches: [350/391]\tLoss: 0.7621 (Average: 0.7529)\tPrecision: 72.656 (Average: 73.455)\n",
      "Test Accuracy\t  Top Precision: 70.040 (Error: 29.960 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [30] Batches: [0/391]\tLoss: 0.9814 (Average: 0.9814)\tPrecision: 71.094 (Average: 71.094)\n",
      "Epoch: No: [30] Batches: [50/391]\tLoss: 0.9142 (Average: 0.7487)\tPrecision: 69.531 (Average: 73.315)\n",
      "Epoch: No: [30] Batches: [100/391]\tLoss: 0.6663 (Average: 0.7344)\tPrecision: 74.219 (Average: 73.693)\n",
      "Epoch: No: [30] Batches: [150/391]\tLoss: 0.9567 (Average: 0.7349)\tPrecision: 64.844 (Average: 73.815)\n",
      "Epoch: No: [30] Batches: [200/391]\tLoss: 0.7313 (Average: 0.7414)\tPrecision: 70.312 (Average: 73.585)\n",
      "Epoch: No: [30] Batches: [250/391]\tLoss: 0.6456 (Average: 0.7419)\tPrecision: 76.562 (Average: 73.618)\n",
      "Epoch: No: [30] Batches: [300/391]\tLoss: 0.8097 (Average: 0.7435)\tPrecision: 71.094 (Average: 73.469)\n",
      "Epoch: No: [30] Batches: [350/391]\tLoss: 0.6881 (Average: 0.7404)\tPrecision: 77.344 (Average: 73.611)\n",
      "Test Accuracy\t  Top Precision: 71.440 (Error: 28.560 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [31] Batches: [0/391]\tLoss: 0.7553 (Average: 0.7553)\tPrecision: 73.438 (Average: 73.438)\n",
      "Epoch: No: [31] Batches: [50/391]\tLoss: 0.6722 (Average: 0.6988)\tPrecision: 73.438 (Average: 75.138)\n",
      "Epoch: No: [31] Batches: [100/391]\tLoss: 0.8335 (Average: 0.7275)\tPrecision: 71.875 (Average: 74.242)\n",
      "Epoch: No: [31] Batches: [150/391]\tLoss: 0.6134 (Average: 0.7280)\tPrecision: 80.469 (Average: 74.384)\n",
      "Epoch: No: [31] Batches: [200/391]\tLoss: 0.7731 (Average: 0.7254)\tPrecision: 75.781 (Average: 74.530)\n",
      "Epoch: No: [31] Batches: [250/391]\tLoss: 0.6381 (Average: 0.7210)\tPrecision: 78.906 (Average: 74.742)\n",
      "Epoch: No: [31] Batches: [300/391]\tLoss: 0.7948 (Average: 0.7245)\tPrecision: 70.312 (Average: 74.629)\n",
      "Epoch: No: [31] Batches: [350/391]\tLoss: 0.8127 (Average: 0.7246)\tPrecision: 66.406 (Average: 74.564)\n",
      "Test Accuracy\t  Top Precision: 73.520 (Error: 26.480 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [32] Batches: [0/391]\tLoss: 0.7875 (Average: 0.7875)\tPrecision: 72.656 (Average: 72.656)\n",
      "Epoch: No: [32] Batches: [50/391]\tLoss: 0.6468 (Average: 0.7151)\tPrecision: 74.219 (Average: 74.663)\n",
      "Epoch: No: [32] Batches: [100/391]\tLoss: 0.6948 (Average: 0.7118)\tPrecision: 75.000 (Average: 75.023)\n",
      "Epoch: No: [32] Batches: [150/391]\tLoss: 0.8725 (Average: 0.7137)\tPrecision: 75.000 (Average: 75.062)\n",
      "Epoch: No: [32] Batches: [200/391]\tLoss: 0.7306 (Average: 0.7144)\tPrecision: 70.312 (Average: 74.961)\n",
      "Epoch: No: [32] Batches: [250/391]\tLoss: 0.7431 (Average: 0.7152)\tPrecision: 71.875 (Average: 74.854)\n",
      "Epoch: No: [32] Batches: [300/391]\tLoss: 0.6321 (Average: 0.7178)\tPrecision: 80.469 (Average: 74.707)\n",
      "Epoch: No: [32] Batches: [350/391]\tLoss: 0.7604 (Average: 0.7189)\tPrecision: 74.219 (Average: 74.697)\n",
      "Test Accuracy\t  Top Precision: 74.740 (Error: 25.260 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [33] Batches: [0/391]\tLoss: 0.6399 (Average: 0.6399)\tPrecision: 79.688 (Average: 79.688)\n",
      "Epoch: No: [33] Batches: [50/391]\tLoss: 0.7905 (Average: 0.6948)\tPrecision: 75.000 (Average: 76.348)\n",
      "Epoch: No: [33] Batches: [100/391]\tLoss: 0.7228 (Average: 0.6841)\tPrecision: 76.562 (Average: 76.338)\n",
      "Epoch: No: [33] Batches: [150/391]\tLoss: 0.6589 (Average: 0.6907)\tPrecision: 75.000 (Average: 75.962)\n",
      "Epoch: No: [33] Batches: [200/391]\tLoss: 0.6588 (Average: 0.6943)\tPrecision: 80.469 (Average: 75.859)\n",
      "Epoch: No: [33] Batches: [250/391]\tLoss: 0.7150 (Average: 0.6949)\tPrecision: 73.438 (Average: 75.738)\n",
      "Epoch: No: [33] Batches: [300/391]\tLoss: 0.7132 (Average: 0.6982)\tPrecision: 77.344 (Average: 75.696)\n",
      "Epoch: No: [33] Batches: [350/391]\tLoss: 0.5469 (Average: 0.6992)\tPrecision: 82.031 (Average: 75.648)\n",
      "Test Accuracy\t  Top Precision: 72.640 (Error: 27.360 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [34] Batches: [0/391]\tLoss: 0.6210 (Average: 0.6210)\tPrecision: 78.906 (Average: 78.906)\n",
      "Epoch: No: [34] Batches: [50/391]\tLoss: 0.7737 (Average: 0.7123)\tPrecision: 70.312 (Average: 75.475)\n",
      "Epoch: No: [34] Batches: [100/391]\tLoss: 0.8428 (Average: 0.7076)\tPrecision: 72.656 (Average: 75.611)\n",
      "Epoch: No: [34] Batches: [150/391]\tLoss: 0.6541 (Average: 0.7064)\tPrecision: 73.438 (Average: 75.502)\n",
      "Epoch: No: [34] Batches: [200/391]\tLoss: 0.6856 (Average: 0.6990)\tPrecision: 75.000 (Average: 75.641)\n",
      "Epoch: No: [34] Batches: [250/391]\tLoss: 0.7067 (Average: 0.6921)\tPrecision: 78.125 (Average: 75.834)\n",
      "Epoch: No: [34] Batches: [300/391]\tLoss: 0.5920 (Average: 0.6899)\tPrecision: 78.906 (Average: 76.007)\n",
      "Epoch: No: [34] Batches: [350/391]\tLoss: 0.7181 (Average: 0.6880)\tPrecision: 78.906 (Average: 76.039)\n",
      "Test Accuracy\t  Top Precision: 72.300 (Error: 27.700 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [35] Batches: [0/391]\tLoss: 0.7063 (Average: 0.7063)\tPrecision: 77.344 (Average: 77.344)\n",
      "Epoch: No: [35] Batches: [50/391]\tLoss: 0.6592 (Average: 0.6416)\tPrecision: 75.781 (Average: 78.370)\n",
      "Epoch: No: [35] Batches: [100/391]\tLoss: 0.5888 (Average: 0.6686)\tPrecision: 79.688 (Average: 77.112)\n",
      "Epoch: No: [35] Batches: [150/391]\tLoss: 0.6373 (Average: 0.6840)\tPrecision: 79.688 (Average: 76.475)\n",
      "Epoch: No: [35] Batches: [200/391]\tLoss: 0.5294 (Average: 0.6805)\tPrecision: 78.906 (Average: 76.333)\n",
      "Epoch: No: [35] Batches: [250/391]\tLoss: 0.7637 (Average: 0.6780)\tPrecision: 70.312 (Average: 76.351)\n",
      "Epoch: No: [35] Batches: [300/391]\tLoss: 0.7270 (Average: 0.6745)\tPrecision: 73.438 (Average: 76.518)\n",
      "Epoch: No: [35] Batches: [350/391]\tLoss: 0.6398 (Average: 0.6737)\tPrecision: 76.562 (Average: 76.507)\n",
      "Test Accuracy\t  Top Precision: 72.670 (Error: 27.330 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [36] Batches: [0/391]\tLoss: 0.6017 (Average: 0.6017)\tPrecision: 77.344 (Average: 77.344)\n",
      "Epoch: No: [36] Batches: [50/391]\tLoss: 0.6129 (Average: 0.6762)\tPrecision: 79.688 (Average: 76.210)\n",
      "Epoch: No: [36] Batches: [100/391]\tLoss: 0.7766 (Average: 0.6741)\tPrecision: 72.656 (Average: 76.423)\n",
      "Epoch: No: [36] Batches: [150/391]\tLoss: 0.7106 (Average: 0.6866)\tPrecision: 72.656 (Average: 76.092)\n",
      "Epoch: No: [36] Batches: [200/391]\tLoss: 0.8110 (Average: 0.6831)\tPrecision: 75.000 (Average: 76.034)\n",
      "Epoch: No: [36] Batches: [250/391]\tLoss: 0.6112 (Average: 0.6817)\tPrecision: 79.688 (Average: 76.124)\n",
      "Epoch: No: [36] Batches: [300/391]\tLoss: 0.8350 (Average: 0.6787)\tPrecision: 78.125 (Average: 76.251)\n",
      "Epoch: No: [36] Batches: [350/391]\tLoss: 0.6437 (Average: 0.6764)\tPrecision: 75.000 (Average: 76.329)\n",
      "Test Accuracy\t  Top Precision: 76.740 (Error: 23.260 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [37] Batches: [0/391]\tLoss: 0.7009 (Average: 0.7009)\tPrecision: 74.219 (Average: 74.219)\n",
      "Epoch: No: [37] Batches: [50/391]\tLoss: 0.7671 (Average: 0.6525)\tPrecision: 69.531 (Average: 76.991)\n",
      "Epoch: No: [37] Batches: [100/391]\tLoss: 0.5764 (Average: 0.6526)\tPrecision: 81.250 (Average: 77.243)\n",
      "Epoch: No: [37] Batches: [150/391]\tLoss: 0.7080 (Average: 0.6614)\tPrecision: 74.219 (Average: 77.038)\n",
      "Epoch: No: [37] Batches: [200/391]\tLoss: 0.7861 (Average: 0.6498)\tPrecision: 76.562 (Average: 77.635)\n",
      "Epoch: No: [37] Batches: [250/391]\tLoss: 0.8188 (Average: 0.6508)\tPrecision: 71.875 (Average: 77.527)\n",
      "Epoch: No: [37] Batches: [300/391]\tLoss: 0.7408 (Average: 0.6502)\tPrecision: 76.562 (Average: 77.432)\n",
      "Epoch: No: [37] Batches: [350/391]\tLoss: 0.7128 (Average: 0.6532)\tPrecision: 75.781 (Average: 77.346)\n",
      "Test Accuracy\t  Top Precision: 75.970 (Error: 24.030 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [38] Batches: [0/391]\tLoss: 0.5688 (Average: 0.5688)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [38] Batches: [50/391]\tLoss: 0.6702 (Average: 0.6741)\tPrecision: 75.781 (Average: 76.823)\n",
      "Epoch: No: [38] Batches: [100/391]\tLoss: 0.5167 (Average: 0.6750)\tPrecision: 76.562 (Average: 76.617)\n",
      "Epoch: No: [38] Batches: [150/391]\tLoss: 0.7158 (Average: 0.6675)\tPrecision: 71.094 (Average: 76.930)\n",
      "Epoch: No: [38] Batches: [200/391]\tLoss: 0.6443 (Average: 0.6674)\tPrecision: 78.125 (Average: 77.025)\n",
      "Epoch: No: [38] Batches: [250/391]\tLoss: 0.5774 (Average: 0.6622)\tPrecision: 78.125 (Average: 77.213)\n",
      "Epoch: No: [38] Batches: [300/391]\tLoss: 0.6180 (Average: 0.6661)\tPrecision: 78.906 (Average: 77.139)\n",
      "Epoch: No: [38] Batches: [350/391]\tLoss: 0.6235 (Average: 0.6667)\tPrecision: 80.469 (Average: 77.045)\n",
      "Test Accuracy\t  Top Precision: 73.880 (Error: 26.120 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [39] Batches: [0/391]\tLoss: 0.6168 (Average: 0.6168)\tPrecision: 76.562 (Average: 76.562)\n",
      "Epoch: No: [39] Batches: [50/391]\tLoss: 0.5632 (Average: 0.6502)\tPrecision: 81.250 (Average: 77.911)\n",
      "Epoch: No: [39] Batches: [100/391]\tLoss: 0.6730 (Average: 0.6327)\tPrecision: 75.000 (Average: 78.311)\n",
      "Epoch: No: [39] Batches: [150/391]\tLoss: 0.7059 (Average: 0.6404)\tPrecision: 76.562 (Average: 78.156)\n",
      "Epoch: No: [39] Batches: [200/391]\tLoss: 0.5579 (Average: 0.6433)\tPrecision: 78.125 (Average: 77.907)\n",
      "Epoch: No: [39] Batches: [250/391]\tLoss: 0.6196 (Average: 0.6510)\tPrecision: 80.469 (Average: 77.559)\n",
      "Epoch: No: [39] Batches: [300/391]\tLoss: 0.5875 (Average: 0.6459)\tPrecision: 79.688 (Average: 77.689)\n",
      "Epoch: No: [39] Batches: [350/391]\tLoss: 0.6137 (Average: 0.6397)\tPrecision: 80.469 (Average: 77.896)\n",
      "Test Accuracy\t  Top Precision: 75.500 (Error: 24.500 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [40] Batches: [0/391]\tLoss: 0.5499 (Average: 0.5499)\tPrecision: 78.906 (Average: 78.906)\n",
      "Epoch: No: [40] Batches: [50/391]\tLoss: 0.7373 (Average: 0.6476)\tPrecision: 72.656 (Average: 77.834)\n",
      "Epoch: No: [40] Batches: [100/391]\tLoss: 0.7201 (Average: 0.6395)\tPrecision: 75.000 (Average: 77.800)\n",
      "Epoch: No: [40] Batches: [150/391]\tLoss: 0.8159 (Average: 0.6430)\tPrecision: 74.219 (Average: 77.752)\n",
      "Epoch: No: [40] Batches: [200/391]\tLoss: 0.9590 (Average: 0.6516)\tPrecision: 70.312 (Average: 77.348)\n",
      "Epoch: No: [40] Batches: [250/391]\tLoss: 0.7628 (Average: 0.6491)\tPrecision: 71.875 (Average: 77.490)\n",
      "Epoch: No: [40] Batches: [300/391]\tLoss: 0.6860 (Average: 0.6473)\tPrecision: 76.562 (Average: 77.471)\n",
      "Epoch: No: [40] Batches: [350/391]\tLoss: 0.7724 (Average: 0.6427)\tPrecision: 75.000 (Average: 77.731)\n",
      "Test Accuracy\t  Top Precision: 76.410 (Error: 23.590 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [41] Batches: [0/391]\tLoss: 0.7129 (Average: 0.7129)\tPrecision: 73.438 (Average: 73.438)\n",
      "Epoch: No: [41] Batches: [50/391]\tLoss: 0.7205 (Average: 0.6092)\tPrecision: 71.094 (Average: 78.937)\n",
      "Epoch: No: [41] Batches: [100/391]\tLoss: 0.6561 (Average: 0.6070)\tPrecision: 79.688 (Average: 79.045)\n",
      "Epoch: No: [41] Batches: [150/391]\tLoss: 0.5581 (Average: 0.6176)\tPrecision: 78.906 (Average: 78.761)\n",
      "Epoch: No: [41] Batches: [200/391]\tLoss: 0.5759 (Average: 0.6150)\tPrecision: 81.250 (Average: 78.770)\n",
      "Epoch: No: [41] Batches: [250/391]\tLoss: 0.5580 (Average: 0.6176)\tPrecision: 85.938 (Average: 78.595)\n",
      "Epoch: No: [41] Batches: [300/391]\tLoss: 0.7378 (Average: 0.6255)\tPrecision: 71.875 (Average: 78.390)\n",
      "Epoch: No: [41] Batches: [350/391]\tLoss: 0.6120 (Average: 0.6266)\tPrecision: 78.906 (Average: 78.328)\n",
      "Test Accuracy\t  Top Precision: 74.220 (Error: 25.780 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [42] Batches: [0/391]\tLoss: 0.6029 (Average: 0.6029)\tPrecision: 80.469 (Average: 80.469)\n",
      "Epoch: No: [42] Batches: [50/391]\tLoss: 0.5808 (Average: 0.6145)\tPrecision: 76.562 (Average: 78.539)\n",
      "Epoch: No: [42] Batches: [100/391]\tLoss: 0.5349 (Average: 0.6056)\tPrecision: 82.812 (Average: 78.906)\n",
      "Epoch: No: [42] Batches: [150/391]\tLoss: 0.4808 (Average: 0.6030)\tPrecision: 83.594 (Average: 79.067)\n",
      "Epoch: No: [42] Batches: [200/391]\tLoss: 0.7045 (Average: 0.6060)\tPrecision: 77.344 (Average: 79.101)\n",
      "Epoch: No: [42] Batches: [250/391]\tLoss: 0.7673 (Average: 0.6167)\tPrecision: 73.438 (Average: 78.797)\n",
      "Epoch: No: [42] Batches: [300/391]\tLoss: 0.4821 (Average: 0.6192)\tPrecision: 81.250 (Average: 78.634)\n",
      "Epoch: No: [42] Batches: [350/391]\tLoss: 0.5973 (Average: 0.6157)\tPrecision: 83.594 (Average: 78.773)\n",
      "Test Accuracy\t  Top Precision: 74.300 (Error: 25.700 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [43] Batches: [0/391]\tLoss: 0.5784 (Average: 0.5784)\tPrecision: 74.219 (Average: 74.219)\n",
      "Epoch: No: [43] Batches: [50/391]\tLoss: 0.6614 (Average: 0.6146)\tPrecision: 76.562 (Average: 77.926)\n",
      "Epoch: No: [43] Batches: [100/391]\tLoss: 0.6863 (Average: 0.6084)\tPrecision: 79.688 (Average: 78.628)\n",
      "Epoch: No: [43] Batches: [150/391]\tLoss: 0.8070 (Average: 0.6245)\tPrecision: 75.000 (Average: 78.301)\n",
      "Epoch: No: [43] Batches: [200/391]\tLoss: 0.7467 (Average: 0.6287)\tPrecision: 71.094 (Average: 78.109)\n",
      "Epoch: No: [43] Batches: [250/391]\tLoss: 0.4450 (Average: 0.6261)\tPrecision: 87.500 (Average: 78.259)\n",
      "Epoch: No: [43] Batches: [300/391]\tLoss: 0.6026 (Average: 0.6201)\tPrecision: 82.812 (Average: 78.561)\n",
      "Epoch: No: [43] Batches: [350/391]\tLoss: 0.7028 (Average: 0.6170)\tPrecision: 76.562 (Average: 78.704)\n",
      "Test Accuracy\t  Top Precision: 76.820 (Error: 23.180 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [44] Batches: [0/391]\tLoss: 0.5153 (Average: 0.5153)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [44] Batches: [50/391]\tLoss: 0.6497 (Average: 0.6037)\tPrecision: 78.125 (Average: 79.473)\n",
      "Epoch: No: [44] Batches: [100/391]\tLoss: 0.7055 (Average: 0.5938)\tPrecision: 75.000 (Average: 79.556)\n",
      "Epoch: No: [44] Batches: [150/391]\tLoss: 0.4604 (Average: 0.6005)\tPrecision: 85.938 (Average: 79.243)\n",
      "Epoch: No: [44] Batches: [200/391]\tLoss: 0.6185 (Average: 0.6049)\tPrecision: 79.688 (Average: 78.996)\n",
      "Epoch: No: [44] Batches: [250/391]\tLoss: 0.5553 (Average: 0.5978)\tPrecision: 81.250 (Average: 79.205)\n",
      "Epoch: No: [44] Batches: [300/391]\tLoss: 0.6215 (Average: 0.6007)\tPrecision: 78.125 (Average: 79.192)\n",
      "Epoch: No: [44] Batches: [350/391]\tLoss: 0.6956 (Average: 0.6037)\tPrecision: 75.781 (Average: 79.073)\n",
      "Test Accuracy\t  Top Precision: 73.260 (Error: 26.740 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [45] Batches: [0/391]\tLoss: 0.4927 (Average: 0.4927)\tPrecision: 84.375 (Average: 84.375)\n",
      "Epoch: No: [45] Batches: [50/391]\tLoss: 0.4437 (Average: 0.5681)\tPrecision: 84.375 (Average: 80.469)\n",
      "Epoch: No: [45] Batches: [100/391]\tLoss: 0.5889 (Average: 0.5859)\tPrecision: 79.688 (Average: 80.152)\n",
      "Epoch: No: [45] Batches: [150/391]\tLoss: 0.5491 (Average: 0.5862)\tPrecision: 84.375 (Average: 80.096)\n",
      "Epoch: No: [45] Batches: [200/391]\tLoss: 0.4780 (Average: 0.5953)\tPrecision: 85.938 (Average: 79.707)\n",
      "Epoch: No: [45] Batches: [250/391]\tLoss: 0.6239 (Average: 0.5921)\tPrecision: 78.906 (Average: 79.846)\n",
      "Epoch: No: [45] Batches: [300/391]\tLoss: 0.6542 (Average: 0.5917)\tPrecision: 75.000 (Average: 79.752)\n",
      "Epoch: No: [45] Batches: [350/391]\tLoss: 0.6243 (Average: 0.5943)\tPrecision: 84.375 (Average: 79.634)\n",
      "Test Accuracy\t  Top Precision: 78.320 (Error: 21.680 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [46] Batches: [0/391]\tLoss: 0.5198 (Average: 0.5198)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [46] Batches: [50/391]\tLoss: 0.5682 (Average: 0.5659)\tPrecision: 81.250 (Average: 80.040)\n",
      "Epoch: No: [46] Batches: [100/391]\tLoss: 0.5394 (Average: 0.5866)\tPrecision: 80.469 (Average: 79.293)\n",
      "Epoch: No: [46] Batches: [150/391]\tLoss: 0.6693 (Average: 0.5929)\tPrecision: 76.562 (Average: 79.222)\n",
      "Epoch: No: [46] Batches: [200/391]\tLoss: 0.5396 (Average: 0.5911)\tPrecision: 81.250 (Average: 79.345)\n",
      "Epoch: No: [46] Batches: [250/391]\tLoss: 0.5481 (Average: 0.5904)\tPrecision: 82.031 (Average: 79.407)\n",
      "Epoch: No: [46] Batches: [300/391]\tLoss: 0.5988 (Average: 0.5885)\tPrecision: 81.250 (Average: 79.472)\n",
      "Epoch: No: [46] Batches: [350/391]\tLoss: 0.7102 (Average: 0.5907)\tPrecision: 75.781 (Average: 79.483)\n",
      "Test Accuracy\t  Top Precision: 64.320 (Error: 35.680 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [47] Batches: [0/391]\tLoss: 0.6496 (Average: 0.6496)\tPrecision: 78.906 (Average: 78.906)\n",
      "Epoch: No: [47] Batches: [50/391]\tLoss: 0.8800 (Average: 0.6162)\tPrecision: 72.656 (Average: 78.661)\n",
      "Epoch: No: [47] Batches: [100/391]\tLoss: 0.4736 (Average: 0.5940)\tPrecision: 83.594 (Average: 79.479)\n",
      "Epoch: No: [47] Batches: [150/391]\tLoss: 0.7376 (Average: 0.5954)\tPrecision: 77.344 (Average: 79.434)\n",
      "Epoch: No: [47] Batches: [200/391]\tLoss: 0.5860 (Average: 0.5918)\tPrecision: 78.906 (Average: 79.559)\n",
      "Epoch: No: [47] Batches: [250/391]\tLoss: 0.4611 (Average: 0.5909)\tPrecision: 86.719 (Average: 79.625)\n",
      "Epoch: No: [47] Batches: [300/391]\tLoss: 0.5155 (Average: 0.5881)\tPrecision: 84.375 (Average: 79.682)\n",
      "Epoch: No: [47] Batches: [350/391]\tLoss: 0.5958 (Average: 0.5891)\tPrecision: 78.125 (Average: 79.692)\n",
      "Test Accuracy\t  Top Precision: 75.750 (Error: 24.250 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [48] Batches: [0/391]\tLoss: 0.7134 (Average: 0.7134)\tPrecision: 74.219 (Average: 74.219)\n",
      "Epoch: No: [48] Batches: [50/391]\tLoss: 0.7582 (Average: 0.5878)\tPrecision: 71.875 (Average: 79.994)\n",
      "Epoch: No: [48] Batches: [100/391]\tLoss: 0.5005 (Average: 0.5734)\tPrecision: 82.031 (Average: 80.500)\n",
      "Epoch: No: [48] Batches: [150/391]\tLoss: 0.6295 (Average: 0.5663)\tPrecision: 77.344 (Average: 80.588)\n",
      "Epoch: No: [48] Batches: [200/391]\tLoss: 0.6385 (Average: 0.5743)\tPrecision: 78.906 (Average: 80.356)\n",
      "Epoch: No: [48] Batches: [250/391]\tLoss: 0.4139 (Average: 0.5721)\tPrecision: 86.719 (Average: 80.534)\n",
      "Epoch: No: [48] Batches: [300/391]\tLoss: 0.6245 (Average: 0.5693)\tPrecision: 79.688 (Average: 80.469)\n",
      "Epoch: No: [48] Batches: [350/391]\tLoss: 0.5970 (Average: 0.5720)\tPrecision: 76.562 (Average: 80.308)\n",
      "Test Accuracy\t  Top Precision: 77.600 (Error: 22.400 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [49] Batches: [0/391]\tLoss: 0.4016 (Average: 0.4016)\tPrecision: 86.719 (Average: 86.719)\n",
      "Epoch: No: [49] Batches: [50/391]\tLoss: 0.4446 (Average: 0.5443)\tPrecision: 85.156 (Average: 81.434)\n",
      "Epoch: No: [49] Batches: [100/391]\tLoss: 0.5122 (Average: 0.5501)\tPrecision: 79.688 (Average: 81.196)\n",
      "Epoch: No: [49] Batches: [150/391]\tLoss: 0.5000 (Average: 0.5497)\tPrecision: 83.594 (Average: 81.152)\n",
      "Epoch: No: [49] Batches: [200/391]\tLoss: 0.7167 (Average: 0.5455)\tPrecision: 82.031 (Average: 81.180)\n",
      "Epoch: No: [49] Batches: [250/391]\tLoss: 0.6161 (Average: 0.5520)\tPrecision: 75.781 (Average: 80.942)\n",
      "Epoch: No: [49] Batches: [300/391]\tLoss: 0.4946 (Average: 0.5573)\tPrecision: 79.688 (Average: 80.733)\n",
      "Epoch: No: [49] Batches: [350/391]\tLoss: 0.5968 (Average: 0.5600)\tPrecision: 78.125 (Average: 80.640)\n",
      "Test Accuracy\t  Top Precision: 74.260 (Error: 25.740 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [50] Batches: [0/391]\tLoss: 0.5190 (Average: 0.5190)\tPrecision: 78.125 (Average: 78.125)\n",
      "Epoch: No: [50] Batches: [50/391]\tLoss: 0.6488 (Average: 0.5600)\tPrecision: 75.000 (Average: 80.668)\n",
      "Epoch: No: [50] Batches: [100/391]\tLoss: 0.5438 (Average: 0.5653)\tPrecision: 86.719 (Average: 80.515)\n",
      "Epoch: No: [50] Batches: [150/391]\tLoss: 0.5591 (Average: 0.5619)\tPrecision: 82.031 (Average: 80.484)\n",
      "Epoch: No: [50] Batches: [200/391]\tLoss: 0.4541 (Average: 0.5657)\tPrecision: 81.250 (Average: 80.434)\n",
      "Epoch: No: [50] Batches: [250/391]\tLoss: 0.6374 (Average: 0.5662)\tPrecision: 81.250 (Average: 80.528)\n",
      "Epoch: No: [50] Batches: [300/391]\tLoss: 0.4819 (Average: 0.5642)\tPrecision: 82.031 (Average: 80.588)\n",
      "Epoch: No: [50] Batches: [350/391]\tLoss: 0.6861 (Average: 0.5687)\tPrecision: 78.125 (Average: 80.384)\n",
      "Test Accuracy\t  Top Precision: 78.430 (Error: 21.570 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [51] Batches: [0/391]\tLoss: 0.5585 (Average: 0.5585)\tPrecision: 79.688 (Average: 79.688)\n",
      "Epoch: No: [51] Batches: [50/391]\tLoss: 0.5485 (Average: 0.5510)\tPrecision: 81.250 (Average: 81.143)\n",
      "Epoch: No: [51] Batches: [100/391]\tLoss: 0.5449 (Average: 0.5637)\tPrecision: 80.469 (Average: 80.608)\n",
      "Epoch: No: [51] Batches: [150/391]\tLoss: 0.5832 (Average: 0.5537)\tPrecision: 75.781 (Average: 81.053)\n",
      "Epoch: No: [51] Batches: [200/391]\tLoss: 0.5743 (Average: 0.5536)\tPrecision: 80.469 (Average: 80.993)\n",
      "Epoch: No: [51] Batches: [250/391]\tLoss: 0.6376 (Average: 0.5540)\tPrecision: 78.906 (Average: 80.979)\n",
      "Epoch: No: [51] Batches: [300/391]\tLoss: 0.4297 (Average: 0.5584)\tPrecision: 86.719 (Average: 80.830)\n",
      "Epoch: No: [51] Batches: [350/391]\tLoss: 0.4248 (Average: 0.5585)\tPrecision: 86.719 (Average: 80.849)\n",
      "Test Accuracy\t  Top Precision: 79.410 (Error: 20.590 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [52] Batches: [0/391]\tLoss: 0.5249 (Average: 0.5249)\tPrecision: 79.688 (Average: 79.688)\n",
      "Epoch: No: [52] Batches: [50/391]\tLoss: 0.5500 (Average: 0.5707)\tPrecision: 81.250 (Average: 79.810)\n",
      "Epoch: No: [52] Batches: [100/391]\tLoss: 0.5538 (Average: 0.5622)\tPrecision: 80.469 (Average: 80.391)\n",
      "Epoch: No: [52] Batches: [150/391]\tLoss: 0.6950 (Average: 0.5537)\tPrecision: 73.438 (Average: 80.758)\n",
      "Epoch: No: [52] Batches: [200/391]\tLoss: 0.5602 (Average: 0.5502)\tPrecision: 82.031 (Average: 80.760)\n",
      "Epoch: No: [52] Batches: [250/391]\tLoss: 0.6783 (Average: 0.5492)\tPrecision: 77.344 (Average: 80.836)\n",
      "Epoch: No: [52] Batches: [300/391]\tLoss: 0.4638 (Average: 0.5494)\tPrecision: 82.812 (Average: 80.809)\n",
      "Epoch: No: [52] Batches: [350/391]\tLoss: 0.4941 (Average: 0.5498)\tPrecision: 84.375 (Average: 80.840)\n",
      "Test Accuracy\t  Top Precision: 77.760 (Error: 22.240 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [53] Batches: [0/391]\tLoss: 0.4843 (Average: 0.4843)\tPrecision: 82.031 (Average: 82.031)\n",
      "Epoch: No: [53] Batches: [50/391]\tLoss: 0.4678 (Average: 0.5474)\tPrecision: 79.688 (Average: 80.790)\n",
      "Epoch: No: [53] Batches: [100/391]\tLoss: 0.6026 (Average: 0.5386)\tPrecision: 81.250 (Average: 81.010)\n",
      "Epoch: No: [53] Batches: [150/391]\tLoss: 0.6196 (Average: 0.5411)\tPrecision: 83.594 (Average: 81.115)\n",
      "Epoch: No: [53] Batches: [200/391]\tLoss: 0.5813 (Average: 0.5424)\tPrecision: 79.688 (Average: 81.199)\n",
      "Epoch: No: [53] Batches: [250/391]\tLoss: 0.6138 (Average: 0.5430)\tPrecision: 78.906 (Average: 81.157)\n",
      "Epoch: No: [53] Batches: [300/391]\tLoss: 0.4328 (Average: 0.5463)\tPrecision: 85.156 (Average: 81.149)\n",
      "Epoch: No: [53] Batches: [350/391]\tLoss: 0.5317 (Average: 0.5476)\tPrecision: 83.594 (Average: 81.217)\n",
      "Test Accuracy\t  Top Precision: 74.530 (Error: 25.470 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [54] Batches: [0/391]\tLoss: 0.6006 (Average: 0.6006)\tPrecision: 78.125 (Average: 78.125)\n",
      "Epoch: No: [54] Batches: [50/391]\tLoss: 0.3572 (Average: 0.5569)\tPrecision: 87.500 (Average: 81.158)\n",
      "Epoch: No: [54] Batches: [100/391]\tLoss: 0.6501 (Average: 0.5566)\tPrecision: 76.562 (Average: 81.103)\n",
      "Epoch: No: [54] Batches: [150/391]\tLoss: 0.5737 (Average: 0.5470)\tPrecision: 82.031 (Average: 81.648)\n",
      "Epoch: No: [54] Batches: [200/391]\tLoss: 0.6329 (Average: 0.5461)\tPrecision: 77.344 (Average: 81.522)\n",
      "Epoch: No: [54] Batches: [250/391]\tLoss: 0.4302 (Average: 0.5428)\tPrecision: 85.156 (Average: 81.580)\n",
      "Epoch: No: [54] Batches: [300/391]\tLoss: 0.6501 (Average: 0.5434)\tPrecision: 79.688 (Average: 81.528)\n",
      "Epoch: No: [54] Batches: [350/391]\tLoss: 0.5370 (Average: 0.5430)\tPrecision: 82.812 (Average: 81.473)\n",
      "Test Accuracy\t  Top Precision: 76.280 (Error: 23.720 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [55] Batches: [0/391]\tLoss: 0.6284 (Average: 0.6284)\tPrecision: 78.125 (Average: 78.125)\n",
      "Epoch: No: [55] Batches: [50/391]\tLoss: 0.6979 (Average: 0.5237)\tPrecision: 78.125 (Average: 81.909)\n",
      "Epoch: No: [55] Batches: [100/391]\tLoss: 0.6692 (Average: 0.5284)\tPrecision: 78.906 (Average: 81.938)\n",
      "Epoch: No: [55] Batches: [150/391]\tLoss: 0.5570 (Average: 0.5418)\tPrecision: 82.812 (Average: 81.390)\n",
      "Epoch: No: [55] Batches: [200/391]\tLoss: 0.5514 (Average: 0.5371)\tPrecision: 79.688 (Average: 81.413)\n",
      "Epoch: No: [55] Batches: [250/391]\tLoss: 0.6931 (Average: 0.5352)\tPrecision: 80.469 (Average: 81.536)\n",
      "Epoch: No: [55] Batches: [300/391]\tLoss: 0.6446 (Average: 0.5364)\tPrecision: 79.688 (Average: 81.523)\n",
      "Epoch: No: [55] Batches: [350/391]\tLoss: 0.5180 (Average: 0.5338)\tPrecision: 84.375 (Average: 81.655)\n",
      "Test Accuracy\t  Top Precision: 79.810 (Error: 20.190 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [56] Batches: [0/391]\tLoss: 0.4960 (Average: 0.4960)\tPrecision: 77.344 (Average: 77.344)\n",
      "Epoch: No: [56] Batches: [50/391]\tLoss: 0.7777 (Average: 0.5183)\tPrecision: 74.219 (Average: 82.338)\n",
      "Epoch: No: [56] Batches: [100/391]\tLoss: 0.4292 (Average: 0.5358)\tPrecision: 85.156 (Average: 81.691)\n",
      "Epoch: No: [56] Batches: [150/391]\tLoss: 0.4896 (Average: 0.5190)\tPrecision: 82.812 (Average: 82.316)\n",
      "Epoch: No: [56] Batches: [200/391]\tLoss: 0.5517 (Average: 0.5238)\tPrecision: 78.906 (Average: 82.121)\n",
      "Epoch: No: [56] Batches: [250/391]\tLoss: 0.5684 (Average: 0.5321)\tPrecision: 83.594 (Average: 81.680)\n",
      "Epoch: No: [56] Batches: [300/391]\tLoss: 0.4955 (Average: 0.5348)\tPrecision: 80.469 (Average: 81.533)\n",
      "Epoch: No: [56] Batches: [350/391]\tLoss: 0.4443 (Average: 0.5387)\tPrecision: 84.375 (Average: 81.372)\n",
      "Test Accuracy\t  Top Precision: 77.000 (Error: 23.000 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [57] Batches: [0/391]\tLoss: 0.3993 (Average: 0.3993)\tPrecision: 86.719 (Average: 86.719)\n",
      "Epoch: No: [57] Batches: [50/391]\tLoss: 0.2844 (Average: 0.4968)\tPrecision: 86.719 (Average: 82.904)\n",
      "Epoch: No: [57] Batches: [100/391]\tLoss: 0.3095 (Average: 0.5017)\tPrecision: 88.281 (Average: 82.565)\n",
      "Epoch: No: [57] Batches: [150/391]\tLoss: 0.7171 (Average: 0.5187)\tPrecision: 78.125 (Average: 82.047)\n",
      "Epoch: No: [57] Batches: [200/391]\tLoss: 0.6063 (Average: 0.5217)\tPrecision: 78.125 (Average: 81.977)\n",
      "Epoch: No: [57] Batches: [250/391]\tLoss: 0.5641 (Average: 0.5207)\tPrecision: 84.375 (Average: 82.037)\n",
      "Epoch: No: [57] Batches: [300/391]\tLoss: 0.4549 (Average: 0.5263)\tPrecision: 81.250 (Average: 81.818)\n",
      "Epoch: No: [57] Batches: [350/391]\tLoss: 0.4741 (Average: 0.5285)\tPrecision: 85.156 (Average: 81.751)\n",
      "Test Accuracy\t  Top Precision: 80.100 (Error: 19.900 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [58] Batches: [0/391]\tLoss: 0.5657 (Average: 0.5657)\tPrecision: 80.469 (Average: 80.469)\n",
      "Epoch: No: [58] Batches: [50/391]\tLoss: 0.5563 (Average: 0.5110)\tPrecision: 82.031 (Average: 82.246)\n",
      "Epoch: No: [58] Batches: [100/391]\tLoss: 0.3554 (Average: 0.5109)\tPrecision: 86.719 (Average: 82.140)\n",
      "Epoch: No: [58] Batches: [150/391]\tLoss: 0.4885 (Average: 0.5087)\tPrecision: 85.156 (Average: 82.311)\n",
      "Epoch: No: [58] Batches: [200/391]\tLoss: 0.5223 (Average: 0.5116)\tPrecision: 82.812 (Average: 82.315)\n",
      "Epoch: No: [58] Batches: [250/391]\tLoss: 0.4769 (Average: 0.5155)\tPrecision: 78.125 (Average: 82.134)\n",
      "Epoch: No: [58] Batches: [300/391]\tLoss: 0.6227 (Average: 0.5200)\tPrecision: 75.781 (Average: 81.899)\n",
      "Epoch: No: [58] Batches: [350/391]\tLoss: 0.4423 (Average: 0.5211)\tPrecision: 85.156 (Average: 81.884)\n",
      "Test Accuracy\t  Top Precision: 71.440 (Error: 28.560 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [59] Batches: [0/391]\tLoss: 0.5076 (Average: 0.5076)\tPrecision: 83.594 (Average: 83.594)\n",
      "Epoch: No: [59] Batches: [50/391]\tLoss: 0.4127 (Average: 0.5007)\tPrecision: 84.375 (Average: 83.395)\n",
      "Epoch: No: [59] Batches: [100/391]\tLoss: 0.3906 (Average: 0.5004)\tPrecision: 85.938 (Average: 83.338)\n",
      "Epoch: No: [59] Batches: [150/391]\tLoss: 0.5280 (Average: 0.5015)\tPrecision: 81.250 (Average: 83.247)\n",
      "Epoch: No: [59] Batches: [200/391]\tLoss: 0.5478 (Average: 0.5056)\tPrecision: 78.906 (Average: 83.034)\n",
      "Epoch: No: [59] Batches: [250/391]\tLoss: 0.3911 (Average: 0.5055)\tPrecision: 89.062 (Average: 83.033)\n",
      "Epoch: No: [59] Batches: [300/391]\tLoss: 0.4617 (Average: 0.5077)\tPrecision: 80.469 (Average: 82.831)\n",
      "Epoch: No: [59] Batches: [350/391]\tLoss: 0.6277 (Average: 0.5157)\tPrecision: 80.469 (Average: 82.561)\n",
      "Test Accuracy\t  Top Precision: 75.080 (Error: 24.920 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [60] Batches: [0/391]\tLoss: 0.4556 (Average: 0.4556)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [60] Batches: [50/391]\tLoss: 0.4726 (Average: 0.5020)\tPrecision: 81.250 (Average: 82.659)\n",
      "Epoch: No: [60] Batches: [100/391]\tLoss: 0.4634 (Average: 0.5143)\tPrecision: 85.938 (Average: 82.256)\n",
      "Epoch: No: [60] Batches: [150/391]\tLoss: 0.6427 (Average: 0.5214)\tPrecision: 78.906 (Average: 82.119)\n",
      "Epoch: No: [60] Batches: [200/391]\tLoss: 0.4623 (Average: 0.5184)\tPrecision: 82.031 (Average: 82.276)\n",
      "Epoch: No: [60] Batches: [250/391]\tLoss: 0.5574 (Average: 0.5166)\tPrecision: 79.688 (Average: 82.355)\n",
      "Epoch: No: [60] Batches: [300/391]\tLoss: 0.6684 (Average: 0.5178)\tPrecision: 73.438 (Average: 82.275)\n",
      "Epoch: No: [60] Batches: [350/391]\tLoss: 0.4944 (Average: 0.5154)\tPrecision: 82.812 (Average: 82.314)\n",
      "Test Accuracy\t  Top Precision: 80.060 (Error: 19.940 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [61] Batches: [0/391]\tLoss: 0.5976 (Average: 0.5976)\tPrecision: 79.688 (Average: 79.688)\n",
      "Epoch: No: [61] Batches: [50/391]\tLoss: 0.5424 (Average: 0.5026)\tPrecision: 78.125 (Average: 83.027)\n",
      "Epoch: No: [61] Batches: [100/391]\tLoss: 0.4714 (Average: 0.5096)\tPrecision: 84.375 (Average: 82.766)\n",
      "Epoch: No: [61] Batches: [150/391]\tLoss: 0.6618 (Average: 0.5065)\tPrecision: 77.344 (Average: 82.714)\n",
      "Epoch: No: [61] Batches: [200/391]\tLoss: 0.5716 (Average: 0.5066)\tPrecision: 81.250 (Average: 82.715)\n",
      "Epoch: No: [61] Batches: [250/391]\tLoss: 0.5331 (Average: 0.5107)\tPrecision: 86.719 (Average: 82.613)\n",
      "Epoch: No: [61] Batches: [300/391]\tLoss: 0.4507 (Average: 0.5090)\tPrecision: 82.812 (Average: 82.594)\n",
      "Epoch: No: [61] Batches: [350/391]\tLoss: 0.5611 (Average: 0.5084)\tPrecision: 77.344 (Average: 82.637)\n",
      "Test Accuracy\t  Top Precision: 78.770 (Error: 21.230 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [62] Batches: [0/391]\tLoss: 0.7144 (Average: 0.7144)\tPrecision: 78.125 (Average: 78.125)\n",
      "Epoch: No: [62] Batches: [50/391]\tLoss: 0.6008 (Average: 0.5123)\tPrecision: 74.219 (Average: 81.939)\n",
      "Epoch: No: [62] Batches: [100/391]\tLoss: 0.4622 (Average: 0.5035)\tPrecision: 82.812 (Average: 82.333)\n",
      "Epoch: No: [62] Batches: [150/391]\tLoss: 0.4930 (Average: 0.5043)\tPrecision: 83.594 (Average: 82.295)\n",
      "Epoch: No: [62] Batches: [200/391]\tLoss: 0.7755 (Average: 0.5023)\tPrecision: 76.562 (Average: 82.544)\n",
      "Epoch: No: [62] Batches: [250/391]\tLoss: 0.3708 (Average: 0.5020)\tPrecision: 86.719 (Average: 82.585)\n",
      "Epoch: No: [62] Batches: [300/391]\tLoss: 0.4883 (Average: 0.4986)\tPrecision: 85.938 (Average: 82.706)\n",
      "Epoch: No: [62] Batches: [350/391]\tLoss: 0.5163 (Average: 0.4992)\tPrecision: 85.156 (Average: 82.752)\n",
      "Test Accuracy\t  Top Precision: 79.110 (Error: 20.890 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [63] Batches: [0/391]\tLoss: 0.5446 (Average: 0.5446)\tPrecision: 80.469 (Average: 80.469)\n",
      "Epoch: No: [63] Batches: [50/391]\tLoss: 0.4517 (Average: 0.4787)\tPrecision: 82.812 (Average: 83.900)\n",
      "Epoch: No: [63] Batches: [100/391]\tLoss: 0.6163 (Average: 0.4903)\tPrecision: 82.031 (Average: 83.354)\n",
      "Epoch: No: [63] Batches: [150/391]\tLoss: 0.5183 (Average: 0.4904)\tPrecision: 83.594 (Average: 83.102)\n",
      "Epoch: No: [63] Batches: [200/391]\tLoss: 0.6077 (Average: 0.4939)\tPrecision: 78.906 (Average: 82.937)\n",
      "Epoch: No: [63] Batches: [250/391]\tLoss: 0.5234 (Average: 0.5002)\tPrecision: 79.688 (Average: 82.788)\n",
      "Epoch: No: [63] Batches: [300/391]\tLoss: 0.5884 (Average: 0.5005)\tPrecision: 82.031 (Average: 82.818)\n",
      "Epoch: No: [63] Batches: [350/391]\tLoss: 0.5127 (Average: 0.5043)\tPrecision: 85.156 (Average: 82.748)\n",
      "Test Accuracy\t  Top Precision: 80.330 (Error: 19.670 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [64] Batches: [0/391]\tLoss: 0.5215 (Average: 0.5215)\tPrecision: 78.906 (Average: 78.906)\n",
      "Epoch: No: [64] Batches: [50/391]\tLoss: 0.3495 (Average: 0.4747)\tPrecision: 87.500 (Average: 83.701)\n",
      "Epoch: No: [64] Batches: [100/391]\tLoss: 0.5503 (Average: 0.4850)\tPrecision: 81.250 (Average: 83.331)\n",
      "Epoch: No: [64] Batches: [150/391]\tLoss: 0.5989 (Average: 0.4858)\tPrecision: 79.688 (Average: 83.237)\n",
      "Epoch: No: [64] Batches: [200/391]\tLoss: 0.5121 (Average: 0.4892)\tPrecision: 85.156 (Average: 83.240)\n",
      "Epoch: No: [64] Batches: [250/391]\tLoss: 0.4947 (Average: 0.4952)\tPrecision: 83.594 (Average: 83.052)\n",
      "Epoch: No: [64] Batches: [300/391]\tLoss: 0.4436 (Average: 0.4985)\tPrecision: 85.938 (Average: 82.947)\n",
      "Epoch: No: [64] Batches: [350/391]\tLoss: 0.4835 (Average: 0.4968)\tPrecision: 81.250 (Average: 82.939)\n",
      "Test Accuracy\t  Top Precision: 79.380 (Error: 20.620 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [65] Batches: [0/391]\tLoss: 0.3584 (Average: 0.3584)\tPrecision: 89.844 (Average: 89.844)\n",
      "Epoch: No: [65] Batches: [50/391]\tLoss: 0.4093 (Average: 0.4572)\tPrecision: 84.375 (Average: 84.467)\n",
      "Epoch: No: [65] Batches: [100/391]\tLoss: 0.5810 (Average: 0.4732)\tPrecision: 80.469 (Average: 84.066)\n",
      "Epoch: No: [65] Batches: [150/391]\tLoss: 0.3835 (Average: 0.4880)\tPrecision: 87.500 (Average: 83.480)\n",
      "Epoch: No: [65] Batches: [200/391]\tLoss: 0.4080 (Average: 0.4842)\tPrecision: 84.375 (Average: 83.652)\n",
      "Epoch: No: [65] Batches: [250/391]\tLoss: 0.4542 (Average: 0.4837)\tPrecision: 84.375 (Average: 83.606)\n",
      "Epoch: No: [65] Batches: [300/391]\tLoss: 0.4529 (Average: 0.4853)\tPrecision: 80.469 (Average: 83.544)\n",
      "Epoch: No: [65] Batches: [350/391]\tLoss: 0.4389 (Average: 0.4863)\tPrecision: 84.375 (Average: 83.471)\n",
      "Test Accuracy\t  Top Precision: 80.060 (Error: 19.940 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [66] Batches: [0/391]\tLoss: 0.5322 (Average: 0.5322)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [66] Batches: [50/391]\tLoss: 0.4282 (Average: 0.4967)\tPrecision: 86.719 (Average: 83.073)\n",
      "Epoch: No: [66] Batches: [100/391]\tLoss: 0.4137 (Average: 0.4870)\tPrecision: 85.938 (Average: 83.161)\n",
      "Epoch: No: [66] Batches: [150/391]\tLoss: 0.5682 (Average: 0.4901)\tPrecision: 82.031 (Average: 83.138)\n",
      "Epoch: No: [66] Batches: [200/391]\tLoss: 0.5006 (Average: 0.4925)\tPrecision: 85.156 (Average: 83.077)\n",
      "Epoch: No: [66] Batches: [250/391]\tLoss: 0.5261 (Average: 0.4909)\tPrecision: 78.906 (Average: 83.198)\n",
      "Epoch: No: [66] Batches: [300/391]\tLoss: 0.5150 (Average: 0.4921)\tPrecision: 82.031 (Average: 83.191)\n",
      "Epoch: No: [66] Batches: [350/391]\tLoss: 0.6054 (Average: 0.4931)\tPrecision: 80.469 (Average: 83.135)\n",
      "Test Accuracy\t  Top Precision: 79.140 (Error: 20.860 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [67] Batches: [0/391]\tLoss: 0.3537 (Average: 0.3537)\tPrecision: 85.938 (Average: 85.938)\n",
      "Epoch: No: [67] Batches: [50/391]\tLoss: 0.5057 (Average: 0.4711)\tPrecision: 82.812 (Average: 83.732)\n",
      "Epoch: No: [67] Batches: [100/391]\tLoss: 0.4945 (Average: 0.4741)\tPrecision: 82.031 (Average: 83.625)\n",
      "Epoch: No: [67] Batches: [150/391]\tLoss: 0.5012 (Average: 0.4784)\tPrecision: 82.031 (Average: 83.495)\n",
      "Epoch: No: [67] Batches: [200/391]\tLoss: 0.4673 (Average: 0.4801)\tPrecision: 83.594 (Average: 83.543)\n",
      "Epoch: No: [67] Batches: [250/391]\tLoss: 0.4492 (Average: 0.4779)\tPrecision: 86.719 (Average: 83.578)\n",
      "Epoch: No: [67] Batches: [300/391]\tLoss: 0.7475 (Average: 0.4806)\tPrecision: 77.344 (Average: 83.469)\n",
      "Epoch: No: [67] Batches: [350/391]\tLoss: 0.3578 (Average: 0.4848)\tPrecision: 89.062 (Average: 83.362)\n",
      "Test Accuracy\t  Top Precision: 80.110 (Error: 19.890 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [68] Batches: [0/391]\tLoss: 0.3942 (Average: 0.3942)\tPrecision: 86.719 (Average: 86.719)\n",
      "Epoch: No: [68] Batches: [50/391]\tLoss: 0.3870 (Average: 0.4729)\tPrecision: 85.938 (Average: 84.498)\n",
      "Epoch: No: [68] Batches: [100/391]\tLoss: 0.5676 (Average: 0.4661)\tPrecision: 80.469 (Average: 84.197)\n",
      "Epoch: No: [68] Batches: [150/391]\tLoss: 0.6133 (Average: 0.4695)\tPrecision: 81.250 (Average: 84.049)\n",
      "Epoch: No: [68] Batches: [200/391]\tLoss: 0.4378 (Average: 0.4706)\tPrecision: 85.156 (Average: 83.982)\n",
      "Epoch: No: [68] Batches: [250/391]\tLoss: 0.4722 (Average: 0.4757)\tPrecision: 80.469 (Average: 83.902)\n",
      "Epoch: No: [68] Batches: [300/391]\tLoss: 0.5377 (Average: 0.4784)\tPrecision: 83.594 (Average: 83.851)\n",
      "Epoch: No: [68] Batches: [350/391]\tLoss: 0.4060 (Average: 0.4820)\tPrecision: 84.375 (Average: 83.687)\n",
      "Test Accuracy\t  Top Precision: 77.660 (Error: 22.340 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [69] Batches: [0/391]\tLoss: 0.5152 (Average: 0.5152)\tPrecision: 85.938 (Average: 85.938)\n",
      "Epoch: No: [69] Batches: [50/391]\tLoss: 0.4477 (Average: 0.4355)\tPrecision: 83.594 (Average: 85.432)\n",
      "Epoch: No: [69] Batches: [100/391]\tLoss: 0.6002 (Average: 0.4521)\tPrecision: 77.344 (Average: 84.630)\n",
      "Epoch: No: [69] Batches: [150/391]\tLoss: 0.6270 (Average: 0.4619)\tPrecision: 80.469 (Average: 84.328)\n",
      "Epoch: No: [69] Batches: [200/391]\tLoss: 0.5411 (Average: 0.4598)\tPrecision: 82.812 (Average: 84.344)\n",
      "Epoch: No: [69] Batches: [250/391]\tLoss: 0.4425 (Average: 0.4621)\tPrecision: 84.375 (Average: 84.303)\n",
      "Epoch: No: [69] Batches: [300/391]\tLoss: 0.4607 (Average: 0.4657)\tPrecision: 86.719 (Average: 84.082)\n",
      "Epoch: No: [69] Batches: [350/391]\tLoss: 0.5645 (Average: 0.4685)\tPrecision: 85.156 (Average: 84.019)\n",
      "Test Accuracy\t  Top Precision: 80.430 (Error: 19.570 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [70] Batches: [0/391]\tLoss: 0.4013 (Average: 0.4013)\tPrecision: 85.938 (Average: 85.938)\n",
      "Epoch: No: [70] Batches: [50/391]\tLoss: 0.4627 (Average: 0.4566)\tPrecision: 88.281 (Average: 84.023)\n",
      "Epoch: No: [70] Batches: [100/391]\tLoss: 0.3294 (Average: 0.4651)\tPrecision: 89.062 (Average: 84.166)\n",
      "Epoch: No: [70] Batches: [150/391]\tLoss: 0.4688 (Average: 0.4723)\tPrecision: 85.156 (Average: 83.863)\n",
      "Epoch: No: [70] Batches: [200/391]\tLoss: 0.4580 (Average: 0.4730)\tPrecision: 84.375 (Average: 83.850)\n",
      "Epoch: No: [70] Batches: [250/391]\tLoss: 0.3844 (Average: 0.4754)\tPrecision: 86.719 (Average: 83.759)\n",
      "Epoch: No: [70] Batches: [300/391]\tLoss: 0.4274 (Average: 0.4768)\tPrecision: 87.500 (Average: 83.752)\n",
      "Epoch: No: [70] Batches: [350/391]\tLoss: 0.6273 (Average: 0.4726)\tPrecision: 81.250 (Average: 83.799)\n",
      "Test Accuracy\t  Top Precision: 78.170 (Error: 21.830 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [71] Batches: [0/391]\tLoss: 0.4821 (Average: 0.4821)\tPrecision: 83.594 (Average: 83.594)\n",
      "Epoch: No: [71] Batches: [50/391]\tLoss: 0.5922 (Average: 0.4818)\tPrecision: 78.125 (Average: 83.548)\n",
      "Epoch: No: [71] Batches: [100/391]\tLoss: 0.4632 (Average: 0.4686)\tPrecision: 78.906 (Average: 84.050)\n",
      "Epoch: No: [71] Batches: [150/391]\tLoss: 0.4709 (Average: 0.4686)\tPrecision: 87.500 (Average: 83.956)\n",
      "Epoch: No: [71] Batches: [200/391]\tLoss: 0.5941 (Average: 0.4669)\tPrecision: 78.125 (Average: 83.967)\n",
      "Epoch: No: [71] Batches: [250/391]\tLoss: 0.4086 (Average: 0.4713)\tPrecision: 89.062 (Average: 83.846)\n",
      "Epoch: No: [71] Batches: [300/391]\tLoss: 0.3810 (Average: 0.4687)\tPrecision: 88.281 (Average: 83.895)\n",
      "Epoch: No: [71] Batches: [350/391]\tLoss: 0.3272 (Average: 0.4680)\tPrecision: 89.844 (Average: 83.972)\n",
      "Test Accuracy\t  Top Precision: 80.230 (Error: 19.770 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [72] Batches: [0/391]\tLoss: 0.5673 (Average: 0.5673)\tPrecision: 82.031 (Average: 82.031)\n",
      "Epoch: No: [72] Batches: [50/391]\tLoss: 0.4438 (Average: 0.4498)\tPrecision: 84.375 (Average: 84.896)\n",
      "Epoch: No: [72] Batches: [100/391]\tLoss: 0.5206 (Average: 0.4576)\tPrecision: 82.031 (Average: 84.553)\n",
      "Epoch: No: [72] Batches: [150/391]\tLoss: 0.4402 (Average: 0.4606)\tPrecision: 82.031 (Average: 84.411)\n",
      "Epoch: No: [72] Batches: [200/391]\tLoss: 0.6154 (Average: 0.4600)\tPrecision: 78.125 (Average: 84.317)\n",
      "Epoch: No: [72] Batches: [250/391]\tLoss: 0.3166 (Average: 0.4655)\tPrecision: 87.500 (Average: 84.145)\n",
      "Epoch: No: [72] Batches: [300/391]\tLoss: 0.2800 (Average: 0.4633)\tPrecision: 92.188 (Average: 84.173)\n",
      "Epoch: No: [72] Batches: [350/391]\tLoss: 0.3508 (Average: 0.4644)\tPrecision: 85.156 (Average: 84.083)\n",
      "Test Accuracy\t  Top Precision: 78.660 (Error: 21.340 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [73] Batches: [0/391]\tLoss: 0.5764 (Average: 0.5764)\tPrecision: 84.375 (Average: 84.375)\n",
      "Epoch: No: [73] Batches: [50/391]\tLoss: 0.4250 (Average: 0.4557)\tPrecision: 82.031 (Average: 84.314)\n",
      "Epoch: No: [73] Batches: [100/391]\tLoss: 0.3536 (Average: 0.4450)\tPrecision: 86.719 (Average: 84.708)\n",
      "Epoch: No: [73] Batches: [150/391]\tLoss: 0.3259 (Average: 0.4556)\tPrecision: 89.844 (Average: 84.230)\n",
      "Epoch: No: [73] Batches: [200/391]\tLoss: 0.5274 (Average: 0.4585)\tPrecision: 83.594 (Average: 84.165)\n",
      "Epoch: No: [73] Batches: [250/391]\tLoss: 0.3746 (Average: 0.4568)\tPrecision: 87.500 (Average: 84.294)\n",
      "Epoch: No: [73] Batches: [300/391]\tLoss: 0.5457 (Average: 0.4555)\tPrecision: 79.688 (Average: 84.383)\n",
      "Epoch: No: [73] Batches: [350/391]\tLoss: 0.5747 (Average: 0.4582)\tPrecision: 81.250 (Average: 84.404)\n",
      "Test Accuracy\t  Top Precision: 79.660 (Error: 20.340 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [74] Batches: [0/391]\tLoss: 0.3660 (Average: 0.3660)\tPrecision: 86.719 (Average: 86.719)\n",
      "Epoch: No: [74] Batches: [50/391]\tLoss: 0.3925 (Average: 0.4428)\tPrecision: 85.156 (Average: 84.881)\n",
      "Epoch: No: [74] Batches: [100/391]\tLoss: 0.4070 (Average: 0.4390)\tPrecision: 86.719 (Average: 85.087)\n",
      "Epoch: No: [74] Batches: [150/391]\tLoss: 0.3464 (Average: 0.4431)\tPrecision: 86.719 (Average: 84.810)\n",
      "Epoch: No: [74] Batches: [200/391]\tLoss: 0.4194 (Average: 0.4475)\tPrecision: 85.156 (Average: 84.585)\n",
      "Epoch: No: [74] Batches: [250/391]\tLoss: 0.4092 (Average: 0.4494)\tPrecision: 86.719 (Average: 84.580)\n",
      "Epoch: No: [74] Batches: [300/391]\tLoss: 0.3923 (Average: 0.4530)\tPrecision: 85.938 (Average: 84.448)\n",
      "Epoch: No: [74] Batches: [350/391]\tLoss: 0.4977 (Average: 0.4517)\tPrecision: 82.031 (Average: 84.484)\n",
      "Test Accuracy\t  Top Precision: 79.410 (Error: 20.590 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [75] Batches: [0/391]\tLoss: 0.5922 (Average: 0.5922)\tPrecision: 81.250 (Average: 81.250)\n",
      "Epoch: No: [75] Batches: [50/391]\tLoss: 0.5308 (Average: 0.4421)\tPrecision: 79.688 (Average: 84.911)\n",
      "Epoch: No: [75] Batches: [100/391]\tLoss: 0.3790 (Average: 0.4436)\tPrecision: 89.844 (Average: 84.847)\n",
      "Epoch: No: [75] Batches: [150/391]\tLoss: 0.4339 (Average: 0.4456)\tPrecision: 85.156 (Average: 84.898)\n",
      "Epoch: No: [75] Batches: [200/391]\tLoss: 0.2170 (Average: 0.4432)\tPrecision: 93.750 (Average: 84.911)\n",
      "Epoch: No: [75] Batches: [250/391]\tLoss: 0.3711 (Average: 0.4473)\tPrecision: 86.719 (Average: 84.773)\n",
      "Epoch: No: [75] Batches: [300/391]\tLoss: 0.3948 (Average: 0.4485)\tPrecision: 86.719 (Average: 84.699)\n",
      "Epoch: No: [75] Batches: [350/391]\tLoss: 0.4268 (Average: 0.4537)\tPrecision: 82.812 (Average: 84.502)\n",
      "Test Accuracy\t  Top Precision: 78.080 (Error: 21.920 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [76] Batches: [0/391]\tLoss: 0.4151 (Average: 0.4151)\tPrecision: 85.156 (Average: 85.156)\n",
      "Epoch: No: [76] Batches: [50/391]\tLoss: 0.4431 (Average: 0.4406)\tPrecision: 86.719 (Average: 84.727)\n",
      "Epoch: No: [76] Batches: [100/391]\tLoss: 0.4632 (Average: 0.4330)\tPrecision: 85.156 (Average: 84.878)\n",
      "Epoch: No: [76] Batches: [150/391]\tLoss: 0.5343 (Average: 0.4391)\tPrecision: 79.688 (Average: 84.675)\n",
      "Epoch: No: [76] Batches: [200/391]\tLoss: 0.4729 (Average: 0.4441)\tPrecision: 84.375 (Average: 84.507)\n",
      "Epoch: No: [76] Batches: [250/391]\tLoss: 0.3785 (Average: 0.4430)\tPrecision: 88.281 (Average: 84.621)\n",
      "Epoch: No: [76] Batches: [300/391]\tLoss: 0.6024 (Average: 0.4450)\tPrecision: 77.344 (Average: 84.593)\n",
      "Epoch: No: [76] Batches: [350/391]\tLoss: 0.3202 (Average: 0.4457)\tPrecision: 92.188 (Average: 84.584)\n",
      "Test Accuracy\t  Top Precision: 78.810 (Error: 21.190 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [77] Batches: [0/391]\tLoss: 0.4669 (Average: 0.4669)\tPrecision: 80.469 (Average: 80.469)\n",
      "Epoch: No: [77] Batches: [50/391]\tLoss: 0.5519 (Average: 0.4427)\tPrecision: 82.812 (Average: 84.589)\n",
      "Epoch: No: [77] Batches: [100/391]\tLoss: 0.3516 (Average: 0.4493)\tPrecision: 89.844 (Average: 84.460)\n",
      "Epoch: No: [77] Batches: [150/391]\tLoss: 0.3969 (Average: 0.4489)\tPrecision: 85.938 (Average: 84.670)\n",
      "Epoch: No: [77] Batches: [200/391]\tLoss: 0.3647 (Average: 0.4446)\tPrecision: 85.938 (Average: 84.760)\n",
      "Epoch: No: [77] Batches: [250/391]\tLoss: 0.4096 (Average: 0.4440)\tPrecision: 84.375 (Average: 84.749)\n",
      "Epoch: No: [77] Batches: [300/391]\tLoss: 0.4134 (Average: 0.4450)\tPrecision: 88.281 (Average: 84.731)\n",
      "Epoch: No: [77] Batches: [350/391]\tLoss: 0.4383 (Average: 0.4446)\tPrecision: 85.938 (Average: 84.744)\n",
      "Test Accuracy\t  Top Precision: 81.360 (Error: 18.640 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [78] Batches: [0/391]\tLoss: 0.4623 (Average: 0.4623)\tPrecision: 83.594 (Average: 83.594)\n",
      "Epoch: No: [78] Batches: [50/391]\tLoss: 0.4286 (Average: 0.4310)\tPrecision: 84.375 (Average: 85.355)\n",
      "Epoch: No: [78] Batches: [100/391]\tLoss: 0.3788 (Average: 0.4322)\tPrecision: 88.281 (Average: 85.295)\n",
      "Epoch: No: [78] Batches: [150/391]\tLoss: 0.5643 (Average: 0.4363)\tPrecision: 79.688 (Average: 85.027)\n",
      "Epoch: No: [78] Batches: [200/391]\tLoss: 0.4775 (Average: 0.4382)\tPrecision: 84.375 (Average: 84.853)\n",
      "Epoch: No: [78] Batches: [250/391]\tLoss: 0.3542 (Average: 0.4398)\tPrecision: 87.500 (Average: 84.736)\n",
      "Epoch: No: [78] Batches: [300/391]\tLoss: 0.5078 (Average: 0.4437)\tPrecision: 81.250 (Average: 84.616)\n",
      "Epoch: No: [78] Batches: [350/391]\tLoss: 0.5107 (Average: 0.4432)\tPrecision: 81.250 (Average: 84.629)\n",
      "Test Accuracy\t  Top Precision: 79.550 (Error: 20.450 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [79] Batches: [0/391]\tLoss: 0.3451 (Average: 0.3451)\tPrecision: 88.281 (Average: 88.281)\n",
      "Epoch: No: [79] Batches: [50/391]\tLoss: 0.3402 (Average: 0.4269)\tPrecision: 87.500 (Average: 85.478)\n",
      "Epoch: No: [79] Batches: [100/391]\tLoss: 0.3209 (Average: 0.4262)\tPrecision: 89.062 (Average: 85.636)\n",
      "Epoch: No: [79] Batches: [150/391]\tLoss: 0.3757 (Average: 0.4310)\tPrecision: 87.500 (Average: 85.518)\n",
      "Epoch: No: [79] Batches: [200/391]\tLoss: 0.4484 (Average: 0.4303)\tPrecision: 84.375 (Average: 85.584)\n",
      "Epoch: No: [79] Batches: [250/391]\tLoss: 0.4288 (Average: 0.4329)\tPrecision: 85.156 (Average: 85.337)\n",
      "Epoch: No: [79] Batches: [300/391]\tLoss: 0.3439 (Average: 0.4324)\tPrecision: 86.719 (Average: 85.421)\n",
      "Epoch: No: [79] Batches: [350/391]\tLoss: 0.5000 (Average: 0.4355)\tPrecision: 85.156 (Average: 85.232)\n",
      "Test Accuracy\t  Top Precision: 81.500 (Error: 18.500 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [80] Batches: [0/391]\tLoss: 0.5541 (Average: 0.5541)\tPrecision: 82.031 (Average: 82.031)\n",
      "Epoch: No: [80] Batches: [50/391]\tLoss: 0.4670 (Average: 0.4434)\tPrecision: 81.250 (Average: 84.911)\n",
      "Epoch: No: [80] Batches: [100/391]\tLoss: 0.4689 (Average: 0.4366)\tPrecision: 85.938 (Average: 85.079)\n",
      "Epoch: No: [80] Batches: [150/391]\tLoss: 0.3736 (Average: 0.4422)\tPrecision: 87.500 (Average: 84.980)\n",
      "Epoch: No: [80] Batches: [200/391]\tLoss: 0.4503 (Average: 0.4442)\tPrecision: 85.938 (Average: 84.818)\n",
      "Epoch: No: [80] Batches: [250/391]\tLoss: 0.4282 (Average: 0.4428)\tPrecision: 86.719 (Average: 84.811)\n",
      "Epoch: No: [80] Batches: [300/391]\tLoss: 0.3305 (Average: 0.4392)\tPrecision: 85.156 (Average: 84.985)\n",
      "Epoch: No: [80] Batches: [350/391]\tLoss: 0.2708 (Average: 0.4410)\tPrecision: 89.062 (Average: 84.903)\n",
      "Test Accuracy\t  Top Precision: 80.750 (Error: 19.250 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [81] Batches: [0/391]\tLoss: 0.3628 (Average: 0.3628)\tPrecision: 88.281 (Average: 88.281)\n",
      "Epoch: No: [81] Batches: [50/391]\tLoss: 0.4679 (Average: 0.4192)\tPrecision: 83.594 (Average: 85.447)\n",
      "Epoch: No: [81] Batches: [100/391]\tLoss: 0.2645 (Average: 0.4271)\tPrecision: 90.625 (Average: 85.218)\n",
      "Epoch: No: [81] Batches: [150/391]\tLoss: 0.4686 (Average: 0.4389)\tPrecision: 82.031 (Average: 84.711)\n",
      "Epoch: No: [81] Batches: [200/391]\tLoss: 0.3905 (Average: 0.4384)\tPrecision: 87.500 (Average: 84.768)\n",
      "Epoch: No: [81] Batches: [250/391]\tLoss: 0.6350 (Average: 0.4359)\tPrecision: 80.469 (Average: 84.829)\n",
      "Epoch: No: [81] Batches: [300/391]\tLoss: 0.4564 (Average: 0.4391)\tPrecision: 85.156 (Average: 84.762)\n",
      "Epoch: No: [81] Batches: [350/391]\tLoss: 0.4677 (Average: 0.4384)\tPrecision: 88.281 (Average: 84.791)\n",
      "Test Accuracy\t  Top Precision: 80.840 (Error: 19.160 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [82] Batches: [0/391]\tLoss: 0.3234 (Average: 0.3234)\tPrecision: 88.281 (Average: 88.281)\n",
      "Epoch: No: [82] Batches: [50/391]\tLoss: 0.4364 (Average: 0.4346)\tPrecision: 82.031 (Average: 85.233)\n",
      "Epoch: No: [82] Batches: [100/391]\tLoss: 0.4027 (Average: 0.4252)\tPrecision: 80.469 (Average: 85.435)\n",
      "Epoch: No: [82] Batches: [150/391]\tLoss: 0.3314 (Average: 0.4234)\tPrecision: 86.719 (Average: 85.560)\n",
      "Epoch: No: [82] Batches: [200/391]\tLoss: 0.3682 (Average: 0.4251)\tPrecision: 88.281 (Average: 85.533)\n",
      "Epoch: No: [82] Batches: [250/391]\tLoss: 0.3988 (Average: 0.4239)\tPrecision: 85.938 (Average: 85.489)\n",
      "Epoch: No: [82] Batches: [300/391]\tLoss: 0.4869 (Average: 0.4247)\tPrecision: 82.031 (Average: 85.431)\n",
      "Epoch: No: [82] Batches: [350/391]\tLoss: 0.4306 (Average: 0.4247)\tPrecision: 87.500 (Average: 85.374)\n",
      "Test Accuracy\t  Top Precision: 77.900 (Error: 22.100 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [83] Batches: [0/391]\tLoss: 0.3613 (Average: 0.3613)\tPrecision: 85.156 (Average: 85.156)\n",
      "Epoch: No: [83] Batches: [50/391]\tLoss: 0.3228 (Average: 0.4304)\tPrecision: 86.719 (Average: 85.555)\n",
      "Epoch: No: [83] Batches: [100/391]\tLoss: 0.4380 (Average: 0.4297)\tPrecision: 89.062 (Average: 85.334)\n",
      "Epoch: No: [83] Batches: [150/391]\tLoss: 0.3033 (Average: 0.4279)\tPrecision: 89.844 (Average: 85.249)\n",
      "Epoch: No: [83] Batches: [200/391]\tLoss: 0.4823 (Average: 0.4310)\tPrecision: 85.156 (Average: 85.098)\n",
      "Epoch: No: [83] Batches: [250/391]\tLoss: 0.4777 (Average: 0.4351)\tPrecision: 83.594 (Average: 84.948)\n",
      "Epoch: No: [83] Batches: [300/391]\tLoss: 0.4322 (Average: 0.4335)\tPrecision: 85.156 (Average: 85.055)\n",
      "Epoch: No: [83] Batches: [350/391]\tLoss: 0.4048 (Average: 0.4292)\tPrecision: 84.375 (Average: 85.165)\n",
      "Test Accuracy\t  Top Precision: 79.880 (Error: 20.120 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [84] Batches: [0/391]\tLoss: 0.4393 (Average: 0.4393)\tPrecision: 82.031 (Average: 82.031)\n",
      "Epoch: No: [84] Batches: [50/391]\tLoss: 0.4099 (Average: 0.4291)\tPrecision: 85.156 (Average: 85.049)\n",
      "Epoch: No: [84] Batches: [100/391]\tLoss: 0.3379 (Average: 0.4374)\tPrecision: 87.500 (Average: 84.932)\n",
      "Epoch: No: [84] Batches: [150/391]\tLoss: 0.4546 (Average: 0.4367)\tPrecision: 83.594 (Average: 84.975)\n",
      "Epoch: No: [84] Batches: [200/391]\tLoss: 0.5290 (Average: 0.4335)\tPrecision: 83.594 (Average: 85.230)\n",
      "Epoch: No: [84] Batches: [250/391]\tLoss: 0.3073 (Average: 0.4322)\tPrecision: 87.500 (Average: 85.234)\n",
      "Epoch: No: [84] Batches: [300/391]\tLoss: 0.3610 (Average: 0.4375)\tPrecision: 88.281 (Average: 85.112)\n",
      "Epoch: No: [84] Batches: [350/391]\tLoss: 0.4095 (Average: 0.4360)\tPrecision: 84.375 (Average: 85.154)\n",
      "Test Accuracy\t  Top Precision: 79.600 (Error: 20.400 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [85] Batches: [0/391]\tLoss: 0.4252 (Average: 0.4252)\tPrecision: 85.938 (Average: 85.938)\n",
      "Epoch: No: [85] Batches: [50/391]\tLoss: 0.4517 (Average: 0.4386)\tPrecision: 85.938 (Average: 84.804)\n",
      "Epoch: No: [85] Batches: [100/391]\tLoss: 0.2780 (Average: 0.4177)\tPrecision: 90.625 (Average: 85.852)\n",
      "Epoch: No: [85] Batches: [150/391]\tLoss: 0.4835 (Average: 0.4163)\tPrecision: 82.812 (Average: 85.844)\n",
      "Epoch: No: [85] Batches: [200/391]\tLoss: 0.5349 (Average: 0.4181)\tPrecision: 82.812 (Average: 85.829)\n",
      "Epoch: No: [85] Batches: [250/391]\tLoss: 0.5188 (Average: 0.4229)\tPrecision: 82.031 (Average: 85.617)\n",
      "Epoch: No: [85] Batches: [300/391]\tLoss: 0.4029 (Average: 0.4223)\tPrecision: 87.500 (Average: 85.657)\n",
      "Epoch: No: [85] Batches: [350/391]\tLoss: 0.3663 (Average: 0.4244)\tPrecision: 89.844 (Average: 85.546)\n",
      "Test Accuracy\t  Top Precision: 79.580 (Error: 20.420 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [86] Batches: [0/391]\tLoss: 0.3770 (Average: 0.3770)\tPrecision: 87.500 (Average: 87.500)\n",
      "Epoch: No: [86] Batches: [50/391]\tLoss: 0.4395 (Average: 0.4158)\tPrecision: 85.156 (Average: 85.325)\n",
      "Epoch: No: [86] Batches: [100/391]\tLoss: 0.3967 (Average: 0.4168)\tPrecision: 86.719 (Average: 85.628)\n",
      "Epoch: No: [86] Batches: [150/391]\tLoss: 0.2758 (Average: 0.4195)\tPrecision: 89.062 (Average: 85.467)\n",
      "Epoch: No: [86] Batches: [200/391]\tLoss: 0.3942 (Average: 0.4183)\tPrecision: 84.375 (Average: 85.553)\n",
      "Epoch: No: [86] Batches: [250/391]\tLoss: 0.5428 (Average: 0.4228)\tPrecision: 80.469 (Average: 85.496)\n",
      "Epoch: No: [86] Batches: [300/391]\tLoss: 0.5339 (Average: 0.4257)\tPrecision: 81.250 (Average: 85.400)\n",
      "Epoch: No: [86] Batches: [350/391]\tLoss: 0.3735 (Average: 0.4250)\tPrecision: 84.375 (Average: 85.443)\n",
      "Test Accuracy\t  Top Precision: 80.690 (Error: 19.310 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [87] Batches: [0/391]\tLoss: 0.4170 (Average: 0.4170)\tPrecision: 86.719 (Average: 86.719)\n",
      "Epoch: No: [87] Batches: [50/391]\tLoss: 0.4161 (Average: 0.4058)\tPrecision: 86.719 (Average: 86.213)\n",
      "Epoch: No: [87] Batches: [100/391]\tLoss: 0.3108 (Average: 0.4180)\tPrecision: 87.500 (Average: 85.837)\n",
      "Epoch: No: [87] Batches: [150/391]\tLoss: 0.3385 (Average: 0.4167)\tPrecision: 88.281 (Average: 85.881)\n",
      "Epoch: No: [87] Batches: [200/391]\tLoss: 0.3786 (Average: 0.4226)\tPrecision: 87.500 (Average: 85.646)\n",
      "Epoch: No: [87] Batches: [250/391]\tLoss: 0.3912 (Average: 0.4196)\tPrecision: 86.719 (Average: 85.673)\n",
      "Epoch: No: [87] Batches: [300/391]\tLoss: 0.5307 (Average: 0.4210)\tPrecision: 82.031 (Average: 85.610)\n",
      "Epoch: No: [87] Batches: [350/391]\tLoss: 0.4754 (Average: 0.4210)\tPrecision: 82.812 (Average: 85.584)\n",
      "Test Accuracy\t  Top Precision: 81.370 (Error: 18.630 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [88] Batches: [0/391]\tLoss: 0.5672 (Average: 0.5672)\tPrecision: 81.250 (Average: 81.250)\n",
      "Epoch: No: [88] Batches: [50/391]\tLoss: 0.2745 (Average: 0.3917)\tPrecision: 89.062 (Average: 86.275)\n",
      "Epoch: No: [88] Batches: [100/391]\tLoss: 0.3751 (Average: 0.3950)\tPrecision: 89.062 (Average: 86.386)\n",
      "Epoch: No: [88] Batches: [150/391]\tLoss: 0.5357 (Average: 0.4076)\tPrecision: 83.594 (Average: 85.865)\n",
      "Epoch: No: [88] Batches: [200/391]\tLoss: 0.4544 (Average: 0.4110)\tPrecision: 83.594 (Average: 85.720)\n",
      "Epoch: No: [88] Batches: [250/391]\tLoss: 0.5026 (Average: 0.4227)\tPrecision: 79.688 (Average: 85.492)\n",
      "Epoch: No: [88] Batches: [300/391]\tLoss: 0.4415 (Average: 0.4231)\tPrecision: 83.594 (Average: 85.455)\n",
      "Epoch: No: [88] Batches: [350/391]\tLoss: 0.4069 (Average: 0.4205)\tPrecision: 85.938 (Average: 85.506)\n",
      "Test Accuracy\t  Top Precision: 81.360 (Error: 18.640 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [89] Batches: [0/391]\tLoss: 0.3176 (Average: 0.3176)\tPrecision: 89.844 (Average: 89.844)\n",
      "Epoch: No: [89] Batches: [50/391]\tLoss: 0.3257 (Average: 0.4017)\tPrecision: 90.625 (Average: 86.060)\n",
      "Epoch: No: [89] Batches: [100/391]\tLoss: 0.4526 (Average: 0.4026)\tPrecision: 85.156 (Average: 86.077)\n",
      "Epoch: No: [89] Batches: [150/391]\tLoss: 0.4231 (Average: 0.4008)\tPrecision: 89.062 (Average: 86.160)\n",
      "Epoch: No: [89] Batches: [200/391]\tLoss: 0.4838 (Average: 0.4034)\tPrecision: 82.031 (Average: 86.035)\n",
      "Epoch: No: [89] Batches: [250/391]\tLoss: 0.4711 (Average: 0.4037)\tPrecision: 85.156 (Average: 86.158)\n",
      "Epoch: No: [89] Batches: [300/391]\tLoss: 0.3241 (Average: 0.4021)\tPrecision: 89.062 (Average: 86.210)\n",
      "Epoch: No: [89] Batches: [350/391]\tLoss: 0.4026 (Average: 0.4060)\tPrecision: 85.938 (Average: 86.102)\n",
      "Test Accuracy\t  Top Precision: 81.740 (Error: 18.260 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [90] Batches: [0/391]\tLoss: 0.2643 (Average: 0.2643)\tPrecision: 90.625 (Average: 90.625)\n",
      "Epoch: No: [90] Batches: [50/391]\tLoss: 0.3985 (Average: 0.4158)\tPrecision: 87.500 (Average: 85.616)\n",
      "Epoch: No: [90] Batches: [100/391]\tLoss: 0.4780 (Average: 0.4039)\tPrecision: 85.156 (Average: 86.108)\n",
      "Epoch: No: [90] Batches: [150/391]\tLoss: 0.3285 (Average: 0.4083)\tPrecision: 89.844 (Average: 85.922)\n",
      "Epoch: No: [90] Batches: [200/391]\tLoss: 0.4092 (Average: 0.4114)\tPrecision: 89.844 (Average: 85.825)\n",
      "Epoch: No: [90] Batches: [250/391]\tLoss: 0.4480 (Average: 0.4116)\tPrecision: 79.688 (Average: 85.819)\n",
      "Epoch: No: [90] Batches: [300/391]\tLoss: 0.3643 (Average: 0.4110)\tPrecision: 86.719 (Average: 85.875)\n",
      "Epoch: No: [90] Batches: [350/391]\tLoss: 0.4043 (Average: 0.4082)\tPrecision: 83.594 (Average: 85.964)\n",
      "Test Accuracy\t  Top Precision: 79.550 (Error: 20.450 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [91] Batches: [0/391]\tLoss: 0.5206 (Average: 0.5206)\tPrecision: 81.250 (Average: 81.250)\n",
      "Epoch: No: [91] Batches: [50/391]\tLoss: 0.5894 (Average: 0.4094)\tPrecision: 78.906 (Average: 85.922)\n",
      "Epoch: No: [91] Batches: [100/391]\tLoss: 0.3823 (Average: 0.4149)\tPrecision: 86.719 (Average: 85.891)\n",
      "Epoch: No: [91] Batches: [150/391]\tLoss: 0.3291 (Average: 0.4066)\tPrecision: 89.844 (Average: 86.103)\n",
      "Epoch: No: [91] Batches: [200/391]\tLoss: 0.2834 (Average: 0.4096)\tPrecision: 89.844 (Average: 85.883)\n",
      "Epoch: No: [91] Batches: [250/391]\tLoss: 0.3185 (Average: 0.4098)\tPrecision: 91.406 (Average: 85.900)\n",
      "Epoch: No: [91] Batches: [300/391]\tLoss: 0.5281 (Average: 0.4134)\tPrecision: 79.688 (Average: 85.779)\n",
      "Epoch: No: [91] Batches: [350/391]\tLoss: 0.3552 (Average: 0.4094)\tPrecision: 89.844 (Average: 85.931)\n",
      "Test Accuracy\t  Top Precision: 77.370 (Error: 22.630 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [92] Batches: [0/391]\tLoss: 0.5126 (Average: 0.5126)\tPrecision: 83.594 (Average: 83.594)\n",
      "Epoch: No: [92] Batches: [50/391]\tLoss: 0.3130 (Average: 0.4151)\tPrecision: 90.625 (Average: 86.091)\n",
      "Epoch: No: [92] Batches: [100/391]\tLoss: 0.3573 (Average: 0.4038)\tPrecision: 87.500 (Average: 86.634)\n",
      "Epoch: No: [92] Batches: [150/391]\tLoss: 0.3648 (Average: 0.4074)\tPrecision: 91.406 (Average: 86.341)\n",
      "Epoch: No: [92] Batches: [200/391]\tLoss: 0.4155 (Average: 0.4052)\tPrecision: 87.500 (Average: 86.400)\n",
      "Epoch: No: [92] Batches: [250/391]\tLoss: 0.5353 (Average: 0.4105)\tPrecision: 80.469 (Average: 86.134)\n",
      "Epoch: No: [92] Batches: [300/391]\tLoss: 0.4592 (Average: 0.4104)\tPrecision: 85.156 (Average: 86.088)\n",
      "Epoch: No: [92] Batches: [350/391]\tLoss: 0.4006 (Average: 0.4111)\tPrecision: 85.938 (Average: 86.051)\n",
      "Test Accuracy\t  Top Precision: 82.560 (Error: 17.440 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [93] Batches: [0/391]\tLoss: 0.4604 (Average: 0.4604)\tPrecision: 83.594 (Average: 83.594)\n",
      "Epoch: No: [93] Batches: [50/391]\tLoss: 0.4019 (Average: 0.3922)\tPrecision: 87.500 (Average: 86.657)\n",
      "Epoch: No: [93] Batches: [100/391]\tLoss: 0.3840 (Average: 0.3884)\tPrecision: 85.938 (Average: 86.928)\n",
      "Epoch: No: [93] Batches: [150/391]\tLoss: 0.4377 (Average: 0.3963)\tPrecision: 85.156 (Average: 86.584)\n",
      "Epoch: No: [93] Batches: [200/391]\tLoss: 0.4009 (Average: 0.3995)\tPrecision: 87.500 (Average: 86.486)\n",
      "Epoch: No: [93] Batches: [250/391]\tLoss: 0.3029 (Average: 0.3982)\tPrecision: 91.406 (Average: 86.516)\n",
      "Epoch: No: [93] Batches: [300/391]\tLoss: 0.4522 (Average: 0.3954)\tPrecision: 83.594 (Average: 86.540)\n",
      "Epoch: No: [93] Batches: [350/391]\tLoss: 0.5157 (Average: 0.3995)\tPrecision: 81.250 (Average: 86.414)\n",
      "Test Accuracy\t  Top Precision: 81.290 (Error: 18.710 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [94] Batches: [0/391]\tLoss: 0.3006 (Average: 0.3006)\tPrecision: 89.844 (Average: 89.844)\n",
      "Epoch: No: [94] Batches: [50/391]\tLoss: 0.4574 (Average: 0.3914)\tPrecision: 89.062 (Average: 86.535)\n",
      "Epoch: No: [94] Batches: [100/391]\tLoss: 0.4216 (Average: 0.4042)\tPrecision: 84.375 (Average: 86.131)\n",
      "Epoch: No: [94] Batches: [150/391]\tLoss: 0.3765 (Average: 0.3990)\tPrecision: 86.719 (Average: 86.196)\n",
      "Epoch: No: [94] Batches: [200/391]\tLoss: 0.2977 (Average: 0.3997)\tPrecision: 89.844 (Average: 86.194)\n",
      "Epoch: No: [94] Batches: [250/391]\tLoss: 0.4990 (Average: 0.4037)\tPrecision: 82.031 (Average: 86.081)\n",
      "Epoch: No: [94] Batches: [300/391]\tLoss: 0.3481 (Average: 0.4039)\tPrecision: 88.281 (Average: 86.080)\n",
      "Epoch: No: [94] Batches: [350/391]\tLoss: 0.2728 (Average: 0.4002)\tPrecision: 90.625 (Average: 86.107)\n",
      "Test Accuracy\t  Top Precision: 81.720 (Error: 18.280 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [95] Batches: [0/391]\tLoss: 0.4161 (Average: 0.4161)\tPrecision: 84.375 (Average: 84.375)\n",
      "Epoch: No: [95] Batches: [50/391]\tLoss: 0.3584 (Average: 0.3929)\tPrecision: 85.938 (Average: 86.458)\n",
      "Epoch: No: [95] Batches: [100/391]\tLoss: 0.6453 (Average: 0.3845)\tPrecision: 78.125 (Average: 86.719)\n",
      "Epoch: No: [95] Batches: [150/391]\tLoss: 0.3853 (Average: 0.3895)\tPrecision: 86.719 (Average: 86.527)\n",
      "Epoch: No: [95] Batches: [200/391]\tLoss: 0.3794 (Average: 0.3987)\tPrecision: 89.062 (Average: 86.178)\n",
      "Epoch: No: [95] Batches: [250/391]\tLoss: 0.5149 (Average: 0.4065)\tPrecision: 83.594 (Average: 85.900)\n",
      "Epoch: No: [95] Batches: [300/391]\tLoss: 0.3911 (Average: 0.4059)\tPrecision: 89.062 (Average: 86.000)\n",
      "Epoch: No: [95] Batches: [350/391]\tLoss: 0.4878 (Average: 0.4029)\tPrecision: 82.812 (Average: 86.100)\n",
      "Test Accuracy\t  Top Precision: 78.950 (Error: 21.050 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [96] Batches: [0/391]\tLoss: 0.4095 (Average: 0.4095)\tPrecision: 85.938 (Average: 85.938)\n",
      "Epoch: No: [96] Batches: [50/391]\tLoss: 0.4481 (Average: 0.3833)\tPrecision: 85.156 (Average: 86.903)\n",
      "Epoch: No: [96] Batches: [100/391]\tLoss: 0.4397 (Average: 0.3852)\tPrecision: 84.375 (Average: 86.819)\n",
      "Epoch: No: [96] Batches: [150/391]\tLoss: 0.3682 (Average: 0.3846)\tPrecision: 87.500 (Average: 86.786)\n",
      "Epoch: No: [96] Batches: [200/391]\tLoss: 0.3964 (Average: 0.3883)\tPrecision: 85.938 (Average: 86.746)\n",
      "Epoch: No: [96] Batches: [250/391]\tLoss: 0.5062 (Average: 0.3895)\tPrecision: 80.469 (Average: 86.638)\n",
      "Epoch: No: [96] Batches: [300/391]\tLoss: 0.3763 (Average: 0.3931)\tPrecision: 85.938 (Average: 86.560)\n",
      "Epoch: No: [96] Batches: [350/391]\tLoss: 0.3284 (Average: 0.3933)\tPrecision: 89.062 (Average: 86.583)\n",
      "Test Accuracy\t  Top Precision: 80.540 (Error: 19.460 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [97] Batches: [0/391]\tLoss: 0.3518 (Average: 0.3518)\tPrecision: 84.375 (Average: 84.375)\n",
      "Epoch: No: [97] Batches: [50/391]\tLoss: 0.3829 (Average: 0.3609)\tPrecision: 85.156 (Average: 87.485)\n",
      "Epoch: No: [97] Batches: [100/391]\tLoss: 0.4640 (Average: 0.3846)\tPrecision: 82.031 (Average: 86.696)\n",
      "Epoch: No: [97] Batches: [150/391]\tLoss: 0.3248 (Average: 0.3842)\tPrecision: 87.500 (Average: 86.786)\n",
      "Epoch: No: [97] Batches: [200/391]\tLoss: 0.4299 (Average: 0.3878)\tPrecision: 86.719 (Average: 86.637)\n",
      "Epoch: No: [97] Batches: [250/391]\tLoss: 0.4044 (Average: 0.3906)\tPrecision: 85.938 (Average: 86.554)\n",
      "Epoch: No: [97] Batches: [300/391]\tLoss: 0.2387 (Average: 0.3903)\tPrecision: 92.969 (Average: 86.654)\n",
      "Epoch: No: [97] Batches: [350/391]\tLoss: 0.4984 (Average: 0.3906)\tPrecision: 84.375 (Average: 86.685)\n",
      "Test Accuracy\t  Top Precision: 78.380 (Error: 21.620 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [98] Batches: [0/391]\tLoss: 0.4188 (Average: 0.4188)\tPrecision: 82.812 (Average: 82.812)\n",
      "Epoch: No: [98] Batches: [50/391]\tLoss: 0.5045 (Average: 0.3797)\tPrecision: 83.594 (Average: 86.795)\n",
      "Epoch: No: [98] Batches: [100/391]\tLoss: 0.6213 (Average: 0.3774)\tPrecision: 80.469 (Average: 87.167)\n",
      "Epoch: No: [98] Batches: [150/391]\tLoss: 0.4690 (Average: 0.3901)\tPrecision: 84.375 (Average: 86.657)\n",
      "Epoch: No: [98] Batches: [200/391]\tLoss: 0.3702 (Average: 0.3919)\tPrecision: 86.719 (Average: 86.602)\n",
      "Epoch: No: [98] Batches: [250/391]\tLoss: 0.4171 (Average: 0.3938)\tPrecision: 83.594 (Average: 86.628)\n",
      "Epoch: No: [98] Batches: [300/391]\tLoss: 0.3298 (Average: 0.3889)\tPrecision: 91.406 (Average: 86.791)\n",
      "Epoch: No: [98] Batches: [350/391]\tLoss: 0.3632 (Average: 0.3918)\tPrecision: 89.062 (Average: 86.770)\n",
      "Test Accuracy\t  Top Precision: 80.830 (Error: 19.170 )\n",
      "\n",
      "Training model: Model 1\n",
      "Current Learning Rate 1.00000e-01\n",
      "Epoch: No: [99] Batches: [0/391]\tLoss: 0.4308 (Average: 0.4308)\tPrecision: 85.938 (Average: 85.938)\n",
      "Epoch: No: [99] Batches: [50/391]\tLoss: 0.3074 (Average: 0.3690)\tPrecision: 90.625 (Average: 87.592)\n",
      "Epoch: No: [99] Batches: [100/391]\tLoss: 0.3256 (Average: 0.3754)\tPrecision: 89.062 (Average: 87.330)\n",
      "Epoch: No: [99] Batches: [150/391]\tLoss: 0.3789 (Average: 0.3735)\tPrecision: 83.594 (Average: 87.288)\n",
      "Epoch: No: [99] Batches: [200/391]\tLoss: 0.2397 (Average: 0.3771)\tPrecision: 89.844 (Average: 87.146)\n",
      "Epoch: No: [99] Batches: [250/391]\tLoss: 0.4102 (Average: 0.3793)\tPrecision: 85.938 (Average: 87.102)\n",
      "Epoch: No: [99] Batches: [300/391]\tLoss: 0.2954 (Average: 0.3790)\tPrecision: 92.188 (Average: 87.168)\n",
      "Epoch: No: [99] Batches: [350/391]\tLoss: 0.4580 (Average: 0.3816)\tPrecision: 80.469 (Average: 87.097)\n",
      "Test Accuracy\t  Top Precision: 79.780 (Error: 20.220 )\n",
      "\n",
      "The lowest error from model: Model 1 after 100 epochs is 17.440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "args=ResNetParams()\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "best_precision = 0\n",
    "best_precision = run_epochs()\n",
    "print('The lowest error from model: {} after {} epochs is {error:.3f}'.format(args.arch,args.epochs, error=100-best_precision))\n",
    "model_save_name = 'project1_model.pt'\n",
    "#path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
    "\n",
    "#Saving the generated model and testing its loading\n",
    "path = model_save_name\n",
    "torch.save(model.state_dict(), path) \n",
    "model_path = path\n",
    "model.load_state_dict(torch.load(model_path, map_location=device), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e00a0c9c-eb50-475f-9682-4cd323a5ae59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Plotting the model\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.plot(val_losses,label=\"Val\")\n",
    "plt.plot(train_losses,label=\"Train\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('plot_graph.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "22cf10741174a73c16493d470940eeed718720095727e52348075ea0a9a6dd40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
